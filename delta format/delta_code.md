# 3. Delta Lake / Delta Format Code

## 1. Creating Delta Tables

### 1.1 Basic Delta Table Creation

**Using SQL (CREATE TABLE)**

```sql
-- Create managed Delta table
CREATE TABLE employees (
    id INT,
    name STRING,
    salary DECIMAL(10,2),
    hire_date DATE
) USING DELTA;

-- Create external Delta table with location
CREATE TABLE employees_external (
    id INT,
    name STRING,
    salary DECIMAL(10,2)
) USING DELTA
LOCATION '/mnt/delta/employees';

-- Create table with partitioning
CREATE TABLE events (
    event_id STRING,
    event_date DATE,
    user_id INT
) USING DELTA
PARTITIONED BY (event_date);

-- Create table with table properties
CREATE TABLE transactions (
    txn_id STRING,
    amount DECIMAL(10,2)
) USING DELTA
TBLPROPERTIES (
    'delta.autoOptimize.optimizeWrite' = 'true',
    'delta.autoOptimize.autoCompact' = 'true'
);
```

**Using PySpark DataFrame API**

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark = SparkSession.builder.getOrCreate()

# Create DataFrame
schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("name", StringType(), True),
    StructField("salary", DecimalType(10,2), True),
    StructField("hire_date", DateType(), True)
])

df = spark.createDataFrame([
    (1, "Alice", 75000.00, "2020-01-15"),
    (2, "Bob", 80000.00, "2019-06-20")
], schema)

# Write as Delta table
df.write.format("delta").mode("overwrite").saveAsTable("employees")

# Write to specific location
df.write.format("delta").mode("overwrite").save("/mnt/delta/employees")

# With partitioning
df.write.format("delta") \
    .mode("overwrite") \
    .partitionBy("hire_date") \
    .saveAsTable("employees_partitioned")

# With table properties
df.write.format("delta") \
    .mode("overwrite") \
    .option("delta.autoOptimize.optimizeWrite", "true") \
    .saveAsTable("employees_optimized")
```

**Using DeltaTableBuilder API**

```python
from delta.tables import DeltaTable

DeltaTable.create(spark) \
    .tableName("employees_builder") \
    .addColumn("id", "INT", nullable=False) \
    .addColumn("name", "STRING") \
    .addColumn("salary", "DECIMAL(10,2)") \
    .addColumn("hire_date", "DATE") \
    .property("delta.autoOptimize.optimizeWrite", "true") \
    .execute()

# With location
DeltaTable.createIfNotExists(spark) \
    .tableName("employees_external") \
    .addColumn("id", "INT") \
    .addColumn("name", "STRING") \
    .location("/mnt/delta/employees_external") \
    .partitionedBy("hire_date") \
    .execute()
```

### 1.2 Identity Columns (Auto-increment)

**Key Concepts:**

- Identity columns auto-generate unique, monotonically increasing values
- Introduced in Delta Lake 2.3.0 / DBR 11.3 LTS
- Guarantees: uniqueness, monotonicity (not gapless)
- Cannot be updated manually
- Survives MERGE, INSERT operations

**SQL Syntax**

```sql
-- Create table with identity column
CREATE TABLE orders (
    order_id BIGINT GENERATED ALWAYS AS IDENTITY,
    customer_id INT,
    order_date DATE,
    amount DECIMAL(10,2)
) USING DELTA;

-- With START and INCREMENT
CREATE TABLE orders_custom (
    order_id BIGINT GENERATED ALWAYS AS IDENTITY (START WITH 1000 INCREMENT BY 1),
    customer_id INT,
    amount DECIMAL(10,2)
) USING DELTA;

-- GENERATED BY DEFAULT (allows manual values)
CREATE TABLE orders_flexible (
    order_id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    customer_id INT,
    amount DECIMAL(10,2)
) USING DELTA;
```

**Using DeltaTableBuilder**

```python
from delta.tables import DeltaTable

DeltaTable.create(spark) \
    .tableName("orders") \
    .addColumn("order_id", "BIGINT", generatedAlwaysAs="GENERATED ALWAYS AS IDENTITY") \
    .addColumn("customer_id", "INT") \
    .addColumn("amount", "DECIMAL(10,2)") \
    .execute()

# With START and INCREMENT
DeltaTable.create(spark) \
    .tableName("orders_custom") \
    .addColumn("order_id", "BIGINT",
               generatedAlwaysAs="GENERATED ALWAYS AS IDENTITY (START WITH 1000 INCREMENT BY 1)") \
    .addColumn("customer_id", "INT") \
    .execute()
```

**Insert Examples**

```sql
-- Identity column auto-populates
INSERT INTO orders (customer_id, order_date, amount)
VALUES (101, '2024-01-15', 250.00);

-- GENERATED BY DEFAULT allows explicit values
INSERT INTO orders_flexible (order_id, customer_id, amount)
VALUES (5000, 101, 250.00);
```

```python
# PySpark - Identity column omitted
df = spark.createDataFrame([
    (101, "2024-01-15", 250.00),
    (102, "2024-01-16", 300.00)
], ["customer_id", "order_date", "amount"])

df.write.format("delta").mode("append").saveAsTable("orders")
```

**Interview Points:**

- Identity values are NOT gapless (gaps occur after failures, MERGE operations)
- Identity column cannot be part of PARTITION BY
- Cannot manually INSERT into GENERATED ALWAYS columns
- Identity state stored in Delta table metadata
- High-water mark persists across sessions

### 1.3 Generated/Computed Columns

**Key Concepts:**

- Values computed from expressions based on other columns
- Computed at write time, stored physically (not virtual)
- Immutable - cannot be updated directly
- Useful for partitioning, indexing, data quality

**SQL Syntax**

```sql
-- Create table with generated column
CREATE TABLE events (
    event_id STRING,
    event_timestamp TIMESTAMP,
    event_date DATE GENERATED ALWAYS AS (CAST(event_timestamp AS DATE)),
    user_id INT
) USING DELTA;

-- Multiple generated columns
CREATE TABLE transactions (
    txn_id STRING,
    txn_timestamp TIMESTAMP,
    amount DECIMAL(10,2),
    txn_date DATE GENERATED ALWAYS AS (CAST(txn_timestamp AS DATE)),
    txn_year INT GENERATED ALWAYS AS (YEAR(txn_timestamp)),
    txn_month INT GENERATED ALWAYS AS (MONTH(txn_timestamp))
) USING DELTA
PARTITIONED BY (txn_year, txn_month);

-- String manipulation
CREATE TABLE users (
    user_id INT,
    email STRING,
    email_domain STRING GENERATED ALWAYS AS (SUBSTRING_INDEX(email, '@', -1))
) USING DELTA;
```

**Using DeltaTableBuilder**

```python
from delta.tables import DeltaTable

DeltaTable.create(spark) \
    .tableName("events") \
    .addColumn("event_id", "STRING") \
    .addColumn("event_timestamp", "TIMESTAMP") \
    .addColumn("event_date", "DATE", generatedAlwaysAs="CAST(event_timestamp AS DATE)") \
    .addColumn("user_id", "INT") \
    .execute()

# With partitioning on generated column
DeltaTable.create(spark) \
    .tableName("transactions") \
    .addColumn("txn_id", "STRING") \
    .addColumn("txn_timestamp", "TIMESTAMP") \
    .addColumn("amount", "DECIMAL(10,2)") \
    .addColumn("txn_year", "INT", generatedAlwaysAs="YEAR(txn_timestamp)") \
    .addColumn("txn_month", "INT", generatedAlwaysAs="MONTH(txn_timestamp)") \
    .partitionedBy("txn_year", "txn_month") \
    .execute()
```

**Insert Examples**

```sql
-- Generated columns auto-populate
INSERT INTO events (event_id, event_timestamp, user_id)
VALUES ('evt_001', '2024-01-15 10:30:00', 101);

-- Cannot explicitly insert into generated column
-- This will FAIL:
-- INSERT INTO events VALUES ('evt_002', '2024-01-15 11:00:00', '2024-01-15', 102);
```

```python
# PySpark - Generated column omitted
from pyspark.sql.functions import current_timestamp

df = spark.createDataFrame([
    ("evt_001", "2024-01-15 10:30:00", 101),
    ("evt_002", "2024-01-15 11:00:00", 102)
], ["event_id", "event_timestamp", "user_id"])

df.write.format("delta").mode("append").saveAsTable("events")
```

**Interview Points:**

- Generated columns are computed at WRITE time (not read time)
- Values are physically stored in data files
- Cannot violate the generation expression
- Expression must be deterministic
- Common use case: partition pruning with derived date columns
- Schema evolution compatible - can add generated columns via ALTER TABLE

---

## 2. Reading Delta Format vs Delta Table

### 2.1 Delta Table (Metastore-Registered)

**What It Is:**

- Table registered in Hive metastore or Unity Catalog
- Metadata includes table name, schema, location, properties
- Supports SQL DDL operations (ALTER, DROP)

**SQL Read**

```sql
-- Read by table name
SELECT * FROM employees;

-- Time travel by version
SELECT * FROM employees VERSION AS OF 5;

-- Time travel by timestamp
SELECT * FROM employees TIMESTAMP AS OF '2024-01-15 10:00:00';

-- Read specific version
SELECT * FROM employees@v5;
```

**PySpark Read**

```python
# Read current version
df = spark.table("employees")

# Time travel by version
df = spark.read.format("delta").option("versionAsOf", 5).table("employees")

# Time travel by timestamp
df = spark.read.format("delta").option("timestampAsOf", "2024-01-15").table("employees")

# Using DeltaTable API
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "employees")
df = delta_table.toDF()

# Time travel
df = delta_table.history().select("version", "timestamp", "operation").show()
```

### 2.2 Delta Format (Path-Based)

**What It Is:**

- Direct read from file path (no metastore entry)
- Access underlying Delta log and data files
- Useful for external tables, ad-hoc analysis

**SQL Read**

```sql
-- Read by path
SELECT * FROM delta.`/mnt/delta/employees`;

-- Time travel by version
SELECT * FROM delta.`/mnt/delta/employees@v5`;

-- Time travel by timestamp
SELECT * FROM delta.`/mnt/delta/employees` TIMESTAMP AS OF '2024-01-15';
```

**PySpark Read**

```python
# Read by path
df = spark.read.format("delta").load("/mnt/delta/employees")

# Time travel by version
df = spark.read.format("delta").option("versionAsOf", 5).load("/mnt/delta/employees")

# Time travel by timestamp
df = spark.read.format("delta").option("timestampAsOf", "2024-01-15").load("/mnt/delta/employees")

# Using DeltaTable API
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/mnt/delta/employees")
df = delta_table.toDF()
```

### 2.3 Key Differences

| Aspect           | Delta Table              | Delta Format                              |
| ---------------- | ------------------------ | ----------------------------------------- |
| Registration     | Metastore entry required | No registration needed                    |
| Access           | `spark.table("name")`    | `spark.read.format("delta").load("path")` |
| DDL Operations   | ALTER, DROP supported    | Not supported                             |
| Table Properties | Stored in metastore      | Only in `_delta_log`                      |
| Permissions      | Metastore-level ACLs     | File system permissions only              |
| Discoverability  | SHOW TABLES lists it     | Must know path                            |
| Unity Catalog    | Full governance support  | Limited governance                        |

### 2.4 Reading Delta Transaction Log

**Structure:**

```
/mnt/delta/employees/
├── _delta_log/
│   ├── 00000000000000000000.json       # Version 0
│   ├── 00000000000000000001.json       # Version 1
│   ├── 00000000000000000002.json       # Version 2
│   └── 00000000000000000010.checkpoint.parquet
├── part-00000-xxx.snappy.parquet
└── part-00001-xxx.snappy.parquet
```

**Reading Transaction Log Directly**

```python
# Read a specific version's commit file
log_df = spark.read.json("/mnt/delta/employees/_delta_log/00000000000000000001.json")
log_df.show(truncate=False)

# Common fields in transaction log:
# - add: files added
# - remove: files removed
# - metaData: table metadata
# - protocol: Delta protocol version
# - commitInfo: commit metadata
```

**Interview Points:**

- Delta Format = raw files + `_delta_log`
- Delta Table = Delta Format + metastore metadata
- Both use the same underlying format
- Path-based reads bypass metastore, useful for data recovery
- Delta Format can be read by non-Databricks Spark with Delta Lake library

---

## 3. Upsert/Merge Operations (SCD Type 1 & 2)

### 3.1 MERGE Syntax Basics

**SQL MERGE**

```sql
MERGE INTO target_table AS target
USING source_table AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

**PySpark MERGE**

```python
from delta.tables import DeltaTable

target = DeltaTable.forName(spark, "target_table")
source = spark.table("source_table")

target.alias("target").merge(
    source.alias("source"),
    "target.id = source.id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

### 3.2 SCD Type 1 (Overwrite Changes)

**Scenario:** Update customer records, overwriting old values

**SQL**

```sql
-- Source data
CREATE OR REPLACE TEMP VIEW customer_updates AS
SELECT 1 AS customer_id, 'Alice' AS name, 'alice@new.com' AS email, 30 AS age
UNION ALL
SELECT 3 AS customer_id, 'Charlie' AS name, 'charlie@example.com' AS email, 28 AS age;

-- SCD Type 1 Merge
MERGE INTO customers AS target
USING customer_updates AS source
ON target.customer_id = source.customer_id
WHEN MATCHED THEN
    UPDATE SET
        target.name = source.name,
        target.email = source.email,
        target.age = source.age
WHEN NOT MATCHED THEN
    INSERT (customer_id, name, email, age)
    VALUES (source.customer_id, source.name, source.email, source.age);
```

**PySpark**

```python
from delta.tables import DeltaTable

# Source DataFrame
source = spark.createDataFrame([
    (1, "Alice", "alice@new.com", 30),
    (3, "Charlie", "charlie@example.com", 28)
], ["customer_id", "name", "email", "age"])

# Target Delta Table
target = DeltaTable.forName(spark, "customers")

# SCD Type 1 Merge
target.alias("target").merge(
    source.alias("source"),
    "target.customer_id = source.customer_id"
).whenMatchedUpdate(set={
    "name": "source.name",
    "email": "source.email",
    "age": "source.age"
}).whenNotMatchedInsert(values={
    "customer_id": "source.customer_id",
    "name": "source.name",
    "email": "source.email",
    "age": "source.age"
}).execute()

# Shorthand using updateAll/insertAll
target.alias("target").merge(
    source.alias("source"),
    "target.customer_id = source.customer_id"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

**Result:**

```
Before:
customer_id | name    | email              | age
1           | Alice   | alice@old.com      | 25
2           | Bob     | bob@example.com    | 35

After:
customer_id | name    | email              | age
1           | Alice   | alice@new.com      | 30  (updated)
2           | Bob     | bob@example.com    | 35  (unchanged)
3           | Charlie | charlie@example.com| 28  (inserted)
```

### 3.3 SCD Type 2 (Maintain History)

**Scenario:** Track historical changes with effective dates

**SQL**

```sql
-- Target table with SCD Type 2 columns
CREATE TABLE customers_scd2 (
    customer_id INT,
    name STRING,
    email STRING,
    age INT,
    effective_start_date DATE,
    effective_end_date DATE,
    is_current BOOLEAN
) USING DELTA;

-- Insert initial records
INSERT INTO customers_scd2 VALUES
(1, 'Alice', 'alice@old.com', 25, '2020-01-01', '9999-12-31', true),
(2, 'Bob', 'bob@example.com', 35, '2020-01-01', '9999-12-31', true);

-- SCD Type 2 Merge
MERGE INTO customers_scd2 AS target
USING (
    SELECT
        customer_id,
        name,
        email,
        age,
        CURRENT_DATE() AS effective_start_date
    FROM customer_updates
) AS source
ON target.customer_id = source.customer_id
   AND target.is_current = true
WHEN MATCHED AND (
    target.name != source.name OR
    target.email != source.email OR
    target.age != source.age
) THEN UPDATE SET
    target.effective_end_date = CURRENT_DATE(),
    target.is_current = false
WHEN NOT MATCHED THEN
    INSERT (customer_id, name, email, age, effective_start_date, effective_end_date, is_current)
    VALUES (source.customer_id, source.name, source.email, source.age,
            source.effective_start_date, CAST('9999-12-31' AS DATE), true);

-- Insert new versions (separate INSERT after MERGE)
INSERT INTO customers_scd2
SELECT
    source.customer_id,
    source.name,
    source.email,
    source.age,
    CURRENT_DATE() AS effective_start_date,
    CAST('9999-12-31' AS DATE) AS effective_end_date,
    true AS is_current
FROM customer_updates AS source
JOIN customers_scd2 AS target
    ON source.customer_id = target.customer_id
    AND target.effective_end_date = CURRENT_DATE();
```

**PySpark (Better Approach)**

```python
from delta.tables import DeltaTable
from pyspark.sql.functions import current_date, lit, col

# Source data
source = spark.createDataFrame([
    (1, "Alice", "alice@new.com", 30),
    (3, "Charlie", "charlie@example.com", 28)
], ["customer_id", "name", "email", "age"])

target = DeltaTable.forName(spark, "customers_scd2")

# Step 1: Expire old records (set is_current = false)
target.alias("target").merge(
    source.alias("source"),
    "target.customer_id = source.customer_id AND target.is_current = true"
).whenMatchedUpdate(
    condition="target.name != source.name OR target.email != source.email OR target.age != source.age",
    set={
        "effective_end_date": "current_date()",
        "is_current": "false"
    }
).execute()

# Step 2: Insert new versions
new_records = source.alias("source").join(
    target.toDF().alias("target"),
    (col("source.customer_id") == col("target.customer_id")) &
    (col("target.effective_end_date") == current_date()),
    "inner"
).select(
    col("source.customer_id"),
    col("source.name"),
    col("source.email"),
    col("source.age"),
    current_date().alias("effective_start_date"),
    lit("9999-12-31").cast("date").alias("effective_end_date"),
    lit(True).alias("is_current")
)

new_records.write.format("delta").mode("append").saveAsTable("customers_scd2")

# Step 3: Insert completely new customers
target.alias("target").merge(
    source.alias("source"),
    "target.customer_id = source.customer_id"
).whenNotMatchedInsert(values={
    "customer_id": "source.customer_id",
    "name": "source.name",
    "email": "source.email",
    "age": "source.age",
    "effective_start_date": "current_date()",
    "effective_end_date": "cast('9999-12-31' as date)",
    "is_current": "true"
}).execute()
```

**Result:**

```
Before:
customer_id | name  | email           | age | start_date | end_date   | is_current
1           | Alice | alice@old.com   | 25  | 2020-01-01 | 9999-12-31 | true
2           | Bob   | bob@example.com | 35  | 2020-01-01 | 9999-12-31 | true

After:
customer_id | name    | email              | age | start_date | end_date   | is_current
1           | Alice   | alice@old.com      | 25  | 2020-01-01 | 2024-01-15 | false (expired)
1           | Alice   | alice@new.com      | 30  | 2024-01-15 | 9999-12-31 | true  (new version)
2           | Bob     | bob@example.com    | 35  | 2020-01-01 | 9999-12-31 | true  (unchanged)
3           | Charlie | charlie@example.com| 28  | 2024-01-15 | 9999-12-31 | true  (new customer)
```

### 3.4 Advanced MERGE Patterns

**Conditional MERGE**

```sql
-- Update only if value changed
MERGE INTO target
USING source
ON target.id = source.id
WHEN MATCHED AND target.value != source.value THEN
    UPDATE SET target.value = source.value, target.updated_at = current_timestamp()
WHEN NOT MATCHED THEN INSERT *;
```

**Delete on MERGE**

```sql
-- Soft delete pattern
MERGE INTO target
USING source
ON target.id = source.id
WHEN MATCHED AND source.is_deleted = true THEN
    UPDATE SET target.is_active = false
WHEN MATCHED AND source.is_deleted = false THEN
    UPDATE SET target.value = source.value
WHEN NOT MATCHED THEN INSERT *;
```

```python
# PySpark with DELETE
target.alias("target").merge(
    source.alias("source"),
    "target.id = source.id"
).whenMatchedDelete(
    condition="source.is_deleted = true"
).whenMatchedUpdate(
    condition="source.is_deleted = false",
    set={"value": "source.value"}
).whenNotMatchedInsert(values={
    "id": "source.id",
    "value": "source.value"
}).execute()
```

**Interview Points:**

- MERGE is atomic (all-or-nothing)
- MERGE creates a single transaction version
- Each record in target matched at most once (no duplicates)
- Order of clauses matters: WHEN MATCHED before WHEN NOT MATCHED
- Can have multiple WHEN MATCHED clauses with different conditions
- MERGE INTO requires target to be Delta table
- Source can be any DataFrame/Table/View

---

## 4. Table Utility Commands

### 4.1 DESCRIBE Commands

**DESCRIBE (Basic Schema)**

```sql
DESCRIBE employees;
```

**Output:**

```
col_name    | data_type | comment
id          | int       | null
name        | string    | null
salary      | decimal   | null
hire_date   | date      | null
```

**DESCRIBE EXTENDED (Full Metadata)**

```sql
DESCRIBE EXTENDED employees;
```

**Output:**

```
col_name                | data_type           | comment
id                      | int                 | null
name                    | string              | null
...
# Detailed Table Information
Database                | default
Table                   | employees
Owner                   | user@databricks.com
Created Time            | 2024-01-15 10:00:00
Last Access             | 2024-01-15 15:30:00
Type                    | MANAGED
Provider                | delta
Location                | dbfs:/user/hive/warehouse/employees
Table Properties        | [delta.minReaderVersion=1,delta.minWriterVersion=2]
```

**DESCRIBE DETAIL (Delta-Specific Metadata)**

```sql
DESCRIBE DETAIL employees;
```

**PySpark:**

```python
# DESCRIBE
spark.sql("DESCRIBE employees").show()

# DESCRIBE EXTENDED
spark.sql("DESCRIBE EXTENDED employees").show(truncate=False)

# DESCRIBE DETAIL
spark.sql("DESCRIBE DETAIL employees").show(truncate=False)

# Using DeltaTable API
from delta.tables import DeltaTable
delta_table = DeltaTable.forName(spark, "employees")
delta_table.detail().show(truncate=False)
```

**DESCRIBE DETAIL Output:**

```
format | id        | name      | location | createdAt           | numFiles | sizeInBytes | partitionColumns
delta  | uuid-1234 | employees | /path    | 2024-01-15 10:00:00 | 10       | 1024000     | [hire_date]
```

### 4.2 HISTORY

**SQL:**

```sql
-- Full history
DESCRIBE HISTORY employees;

-- Limit results
DESCRIBE HISTORY employees LIMIT 10;
```

**PySpark:**

```python
# Using SQL
spark.sql("DESCRIBE HISTORY employees").show(truncate=False)

# Using DeltaTable API
from delta.tables import DeltaTable
delta_table = DeltaTable.forName(spark, "employees")
delta_table.history().show(truncate=False)

# Filter by specific versions
delta_table.history(10).show()  # Last 10 versions
```

**Output:**

```
version | timestamp           | operation | operationParameters      | readVersion | isolationLevel
5       | 2024-01-15 10:30:00 | MERGE     | {predicate: "id = ..."}  | 4           | Serializable
4       | 2024-01-15 10:00:00 | UPDATE    | {predicate: "..."}       | 3           | Serializable
3       | 2024-01-14 15:00:00 | DELETE    | {predicate: "..."}       | 2           | Serializable
2       | 2024-01-14 10:00:00 | WRITE     | {mode: Append}           | 1           | Serializable
1       | 2024-01-13 10:00:00 | WRITE     | {mode: Overwrite}        | 0           | Serializable
0       | 2024-01-12 10:00:00 | CREATE    | {}                       | null        | Serializable
```

**Interview Points:**

- History stored in `_delta_log` transaction log
- Each version is immutable
- History enables time travel
- History does NOT show data-level changes (use CDF for that)

### 4.3 RESTORE

**Restore to Specific Version**

```sql
-- Restore by version number
RESTORE TABLE employees TO VERSION AS OF 5;

-- Restore by timestamp
RESTORE TABLE employees TO TIMESTAMP AS OF '2024-01-15 10:00:00';
```

**PySpark:**

```python
# SQL approach
spark.sql("RESTORE TABLE employees TO VERSION AS OF 5")

# Using DeltaTable API
from delta.tables import DeltaTable
delta_table = DeltaTable.forName(spark, "employees")

# Restore to version
delta_table.restoreToVersion(5)

# Restore to timestamp
delta_table.restoreToTimestamp("2024-01-15 10:00:00")
```

**What Happens:**

- Creates new version in transaction log
- Does NOT delete data files (VACUUM needed for cleanup)
- Metadata points to files from target version
- Restores schema, partitioning, table properties

**Example:**

```sql
-- Current version: 10
-- Restore to version 5
RESTORE TABLE employees TO VERSION AS OF 5;
-- New version created: 11 (points to files from version 5)

-- Check history
DESCRIBE HISTORY employees;
-- Version 11: RESTORE operation
```

**Interview Points:**

- RESTORE is metadata operation (fast)
- No data duplication
- Old versions remain accessible until VACUUM
- Cannot restore if target version VACUUMed
- RESTORE creates a new version

### 4.4 Table Properties (TBLPROPERTIES)

**View Properties**

```sql
SHOW TBLPROPERTIES employees;
```

**Set Properties**

```sql
-- Using ALTER TABLE
ALTER TABLE employees SET TBLPROPERTIES (
    'delta.autoOptimize.optimizeWrite' = 'true',
    'delta.autoOptimize.autoCompact' = 'true',
    'delta.deletedFileRetentionDuration' = 'interval 7 days',
    'delta.logRetentionDuration' = 'interval 30 days'
);
```

**PySpark:**

```python
# View properties
spark.sql("SHOW TBLPROPERTIES employees").show(truncate=False)

# Set properties
spark.sql("""
    ALTER TABLE employees SET TBLPROPERTIES (
        'delta.autoOptimize.optimizeWrite' = 'true',
        'delta.autoOptimize.autoCompact' = 'true'
    )
""")
```

**Common Table Properties:**

```sql
-- Retention settings
'delta.deletedFileRetentionDuration' = 'interval 7 days'
'delta.logRetentionDuration' = 'interval 30 days'

-- Auto-optimization
'delta.autoOptimize.optimizeWrite' = 'true'  -- Small file compaction on write
'delta.autoOptimize.autoCompact' = 'true'    -- Auto OPTIMIZE

-- Change Data Feed
'delta.enableChangeDataFeed' = 'true'

-- Column Mapping
'delta.columnMapping.mode' = 'name'

-- Constraints
'delta.constraints.check_salary' = 'salary > 0'

-- Data Skipping
'delta.dataSkippingNumIndexedCols' = '32'
```

### 4.5 VACUUM

**Purpose:**

- Permanently delete data files not required by recent versions
- Reclaim storage space
- Remove uncommitted files

**SQL:**

```sql
-- Vacuum with default retention (7 days)
VACUUM employees;

-- Vacuum with custom retention
VACUUM employees RETAIN 168 HOURS;  -- 7 days

-- Dry run (see what would be deleted)
VACUUM employees DRY RUN;

-- Override retention check (DANGEROUS)
SET spark.databricks.delta.retentionDurationCheck.enabled = false;
VACUUM employees RETAIN 0 HOURS;
SET spark.databricks.delta.retentionDurationCheck.enabled = true;
```

**PySpark:**

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "employees")

# Vacuum with default retention
delta_table.vacuum()

# Vacuum with custom retention
delta_table.vacuum(168)  # hours

# Dry run
spark.conf.set("spark.databricks.delta.vacuum.logging.enabled", "true")
delta_table.vacuum()  # Check logs

# Override retention (DANGEROUS)
spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", "false")
delta_table.vacuum(0)
spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", "true")
```

**Retention Periods:**

```sql
-- Default: 7 days
VACUUM employees;

-- Custom retention
VACUUM employees RETAIN 336 HOURS;  -- 14 days
VACUUM employees RETAIN 24 HOURS;   -- 1 day

-- Table property (permanent setting)
ALTER TABLE employees SET TBLPROPERTIES (
    'delta.deletedFileRetentionDuration' = 'interval 14 days'
);
```

**What Gets Deleted:**

- Data files not referenced by any version within retention
- Uncommitted files (from failed writes)
- Checkpoint files older than retention

**What Survives:**

- Files referenced by versions within retention
- Transaction log JSON files
- Latest checkpoint

**Interview Points:**

- VACUUM is irreversible
- Default 7-day retention protects time travel
- Cannot time travel beyond VACUUMed versions
- VACUUM runs in two phases: identify files, delete files
- Long-running queries reading old versions can fail during VACUUM
- Set `spark.databricks.delta.retentionDurationCheck.enabled = false` to override (risky)

### 4.6 CLONE (Shallow vs Deep)

**Shallow Clone (Metadata Only)**

```sql
-- Create shallow clone
CREATE TABLE employees_clone
SHALLOW CLONE employees;

-- Shallow clone from specific version
CREATE TABLE employees_clone
SHALLOW CLONE employees VERSION AS OF 5;

-- Shallow clone to specific location
CREATE TABLE employees_clone
SHALLOW CLONE employees
LOCATION '/mnt/delta/employees_clone';
```

**What Happens:**

- Copies transaction log only
- References same data files as source
- Fast and space-efficient
- Independent table metadata

**Deep Clone (Full Copy)**

```sql
-- Create deep clone
CREATE TABLE employees_backup
DEEP CLONE employees;

-- Deep clone from specific version
CREATE TABLE employees_backup
DEEP CLONE employees VERSION AS OF 5;
```

**What Happens:**

- Copies transaction log AND data files
- Full independent copy
- Slower and space-consuming
- Complete isolation from source

**PySpark:**

```python
from delta.tables import DeltaTable

# Shallow clone
spark.sql("CREATE TABLE employees_clone SHALLOW CLONE employees")

# Deep clone
spark.sql("CREATE TABLE employees_backup DEEP CLONE employees")

# Using DeltaTable API
source = DeltaTable.forName(spark, "employees")

# Clone not directly supported in API - use SQL
spark.sql("CREATE TABLE employees_clone SHALLOW CLONE employees")
```

**Key Differences:**

| Aspect        | Shallow Clone            | Deep Clone                 |
| ------------- | ------------------------ | -------------------------- |
| Data Files    | Referenced (shared)      | Copied (independent)       |
| Speed         | Fast (metadata only)     | Slow (copies data)         |
| Storage       | Minimal overhead         | Full copy                  |
| Independence  | Shares files with source | Fully independent          |
| Source VACUUM | Affects clone            | No impact                  |
| Use Cases     | Testing, dev/staging     | Backups, disaster recovery |

**Use Cases:**

**Shallow Clone:**

- Quick environment setup (dev, staging, test)
- Data exploration without duplication
- Creating table variants with different properties
- Testing schema changes

**Deep Clone:**

- Production backups
- Disaster recovery
- Data archival
- Migrating tables across workspaces/regions
- Complete isolation from source changes

**Interview Points:**

- Shallow clones become "dangling" if source VACUUMed
- Deep clones survive source VACUUM/deletion
- Clones are independent tables (separate history)
- Clones can have different properties, schema evolution
- REPLACE TABLE with CLONE syntax overwrites target

### 4.7 Change Data Feed (CDF)

**Enable CDF**

```sql
-- Enable on existing table
ALTER TABLE employees SET TBLPROPERTIES (
    'delta.enableChangeDataFeed' = 'true'
);

-- Enable during table creation
CREATE TABLE employees (
    id INT,
    name STRING,
    salary DECIMAL(10,2)
) USING DELTA
TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true');
```

**PySpark:**

```python
# Enable CDF
spark.sql("""
    ALTER TABLE employees SET TBLPROPERTIES (
        'delta.enableChangeDataFeed' = 'true'
    )
""")
```

**Reading CDF**

```sql
-- Read changes between versions
SELECT * FROM table_changes('employees', 2, 5);

-- Read changes by timestamp
SELECT * FROM table_changes('employees',
    '2024-01-15 10:00:00',
    '2024-01-15 15:00:00'
);
```

**PySpark:**

```python
# Read changes by version
df = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 2) \
    .option("endingVersion", 5) \
    .table("employees")

# Read changes by timestamp
df = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingTimestamp", "2024-01-15 10:00:00") \
    .option("endingTimestamp", "2024-01-15 15:00:00") \
    .table("employees")

# Latest changes since version
df = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 5) \
    .table("employees")

# Show changes
df.show()
```

**CDF Output Schema:**

```python
# Original columns + CDF metadata columns
id | name  | salary | _change_type | _commit_version | _commit_timestamp
1  | Alice | 75000  | insert       | 3               | 2024-01-15 10:00:00
2  | Bob   | 80000  | update_preimage  | 4           | 2024-01-15 11:00:00
2  | Bob   | 85000  | update_postimage | 4           | 2024-01-15 11:00:00
3  | Carol | 70000  | delete       | 5               | 2024-01-15 12:00:00
```

**Change Types:**

- `insert`: New row added
- `update_preimage`: Old value before update
- `update_postimage`: New value after update
- `delete`: Row deleted

**CDF Use Cases:**

**1. Incremental ETL**

```python
# Read only new changes since last processed version
last_processed_version = 10

df_changes = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", last_processed_version + 1) \
    .table("source_table")

# Process only changes
df_changes.write.format("delta").mode("append").saveAsTable("target_table")
```

**2. Audit/Compliance**

```sql
-- Track who changed what and when
SELECT
    id,
    name,
    _change_type,
    _commit_version,
    _commit_timestamp
FROM table_changes('employees', 0, 100)
WHERE _change_type IN ('update_preimage', 'update_postimage', 'delete')
ORDER BY _commit_timestamp;
```

**3. Replication to External Systems**

```python
# Stream changes to Kafka/downstream systems
df_stream = spark.readStream.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 0) \
    .table("employees")

df_stream.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "employee_changes") \
    .start()
```

**4. Slowly Changing Dimensions**

```python
# Extract only updates for SCD processing
updates = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 5) \
    .table("employees") \
    .filter("_change_type IN ('update_preimage', 'update_postimage')")
```

**Interview Points:**

- CDF adds ~5-10% storage overhead
- CDF requires Delta Lake 2.0+
- Changes tracked at row level
- Cannot enable CDF retroactively for past versions
- CDF data retained per `delta.deletedFileRetentionDuration`
- CDF useful for CDC, incremental processing, audit logs

### 4.8 OPTIMIZE

**Purpose:**

- Compact small files into larger files
- Improve read performance
- Reduce metadata overhead

**SQL:**

```sql
-- Optimize entire table
OPTIMIZE employees;

-- Optimize specific partition
OPTIMIZE employees WHERE hire_date = '2024-01-15';

-- Optimize with Z-ORDER
OPTIMIZE employees ZORDER BY (department, salary);
```

**PySpark:**

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "employees")

# Optimize entire table
delta_table.optimize().executeCompaction()

# Optimize specific partition
delta_table.optimize().where("hire_date = '2024-01-15'").executeCompaction()

# Optimize with Z-ORDER
delta_table.optimize().executeZOrderBy("department", "salary")
```

**How OPTIMIZE Works (Internals):**

**1. File Selection**

- Identifies small files (< `targetFileSize`)
- Default `targetFileSize` = 128 MB
- Groups files by partition

**2. Coalesce/Repartition**

- Reads small files into DataFrame
- Coalesces into fewer, larger files
- Writes new files

**3. Transaction Commit**

- Adds new large files to transaction log
- Marks old small files as removed
- Atomic operation

**Example:**

```
Before OPTIMIZE:
part-00001.parquet (10 MB)
part-00002.parquet (5 MB)
part-00003.parquet (8 MB)
part-00004.parquet (12 MB)
Total: 4 files, 35 MB

After OPTIMIZE:
part-00005.parquet (35 MB)
Total: 1 file, 35 MB
```

**OPTIMIZE Configuration:**

```python
# Set target file size
spark.conf.set("spark.databricks.delta.optimize.maxFileSize", 134217728)  # 128 MB
spark.conf.set("spark.databricks.delta.optimize.minFileSize", 10485760)   # 10 MB

# Auto-optimize on write
spark.sql("""
    ALTER TABLE employees SET TBLPROPERTIES (
        'delta.autoOptimize.optimizeWrite' = 'true',
        'delta.autoOptimize.autoCompact' = 'true'
    )
""")
```

### 4.9 ZORDER

**Purpose:**

- Co-locate related data in files
- Improve data skipping
- Reduce data scanning for filtered queries

**SQL:**

```sql
-- Z-ORDER by single column
OPTIMIZE employees ZORDER BY (department);

-- Z-ORDER by multiple columns
OPTIMIZE employees ZORDER BY (department, hire_date, salary);
```

**PySpark:**

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "employees")

# Z-ORDER by single column
delta_table.optimize().executeZOrderBy("department")

# Z-ORDER by multiple columns
delta_table.optimize().executeZOrderBy("department", "hire_date", "salary")
```

**How Z-ORDER Works:**

**1. Data Arrangement**

- Uses space-filling Z-curve algorithm
- Co-locates rows with similar values
- Multi-dimensional clustering

**2. Column Statistics**

- Collects min/max stats per file per column
- Stores stats in transaction log (`add` action)
- Enables data skipping during reads

**Example:**

```
Employees Table:
department | hire_date  | salary
Sales      | 2024-01-15 | 50000
Sales      | 2024-02-20 | 55000
Eng        | 2024-01-10 | 80000
Eng        | 2024-03-05 | 90000

Without Z-ORDER:
File 1: Sales(2024-01-15), Eng(2024-01-10)
File 2: Sales(2024-02-20), Eng(2024-03-05)

With Z-ORDER BY (department):
File 1: Sales(2024-01-15), Sales(2024-02-20)
File 2: Eng(2024-01-10), Eng(2024-03-05)

Query: WHERE department = 'Sales'
Without Z-ORDER: Reads 2 files
With Z-ORDER: Reads 1 file (50% reduction)
```

**Column Stats Example:**

```json
{
  "add": {
    "path": "part-00001.parquet",
    "stats": {
      "numRecords": 1000,
      "minValues": { "department": "Eng", "salary": 50000 },
      "maxValues": { "department": "Sales", "salary": 150000 }
    }
  }
}
```

**Z-ORDER vs Default Behavior:**

| Aspect            | Default (No Z-ORDER)      | With Z-ORDER                  |
| ----------------- | ------------------------- | ----------------------------- |
| File Organization | Arbitrary/insertion order | Clustered by Z-ORDER columns  |
| Data Skipping     | Basic (partition-level)   | Advanced (file-level)         |
| Column Stats      | Collected                 | Enhanced for Z-ORDER columns  |
| Query Performance | Standard                  | Improved for filtered queries |
| Write Performance | Fast                      | Slower (Z-ORDER computation)  |

**Choosing Z-ORDER Columns:**

- High-cardinality columns (department, user_id)
- Frequently filtered columns (WHERE clauses)
- Low-to-medium cardinality better than very high
- Order matters: most selective column first
- Limit to 3-4 columns (diminishing returns)

**Interview Points:**

- Z-ORDER is a one-time operation (must re-run after new writes)
- Z-ORDER requires full table rewrite
- Z-ORDER incompatible with partitioning on same columns
- Z-ORDER leverages Delta's data skipping
- Stats stored in `_delta_log` JSON files
- Z-ORDER improves read, slows write

### 4.10 Liquid Clustering

**Purpose:**

- Automatic incremental clustering (no manual OPTIMIZE needed)
- Replaces manual Z-ORDER for clustering
- Optimizes both read and write performance

**Enable Liquid Clustering**

```sql
-- Create table with Liquid Clustering
CREATE TABLE employees_clustered (
    id INT,
    name STRING,
    department STRING,
    hire_date DATE,
    salary DECIMAL(10,2)
) USING DELTA
CLUSTER BY (department, hire_date);

-- Convert existing table
ALTER TABLE employees CLUSTER BY (department, hire_date);
```

**PySpark:**

```python
# Create table with Liquid Clustering
spark.sql("""
    CREATE TABLE employees_clustered (
        id INT,
        name STRING,
        department STRING,
        hire_date DATE,
        salary DECIMAL(10,2)
    ) USING DELTA
    CLUSTER BY (department, hire_date)
""")

# Using DeltaTableBuilder
from delta.tables import DeltaTable

DeltaTable.create(spark) \
    .tableName("employees_clustered") \
    .addColumn("id", "INT") \
    .addColumn("name", "STRING") \
    .addColumn("department", "STRING") \
    .addColumn("hire_date", "DATE") \
    .addColumn("salary", "DECIMAL(10,2)") \
    .clusterBy("department", "hire_date") \
    .execute()
```

**How Liquid Clustering Works:**

**1. Automatic Clustering on Write**

- Clusters data during INSERT/MERGE/UPDATE
- No manual OPTIMIZE needed
- Incremental clustering

**2. Adaptive Layout**

- Dynamically adjusts clustering layout
- Learns from query patterns
- Self-tunes over time

**3. Z-ORDER Replacement**

- More efficient than manual Z-ORDER
- Lower maintenance overhead
- Better for high-volume writes

**Key Differences: Z-ORDER vs Liquid Clustering**

| Aspect            | Z-ORDER                | Liquid Clustering     |
| ----------------- | ---------------------- | --------------------- |
| Trigger           | Manual OPTIMIZE        | Automatic on write    |
| Incremental       | No (full rewrite)      | Yes (incremental)     |
| Maintenance       | Manual re-run needed   | Self-maintaining      |
| Write Performance | Slows writes           | Minimal impact        |
| Use Case          | Batch-optimized tables | High-volume streaming |
