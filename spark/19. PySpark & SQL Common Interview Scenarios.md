# 19. PySpark & SQL Common Interview Scenarios ðŸ§ª

## Scenario 1: ETL Pipeline in Spark

**Problem**: Build an ETL pipeline that:

1. Reads raw sales data from CSV
2. Cleans and transforms data
3. Joins with product information
4. Aggregates by category
5. Writes to Parquet

**Solution**:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("ETL_Pipeline") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Read raw data
sales = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("s3://raw-data/sales.csv")

products = spark.read.parquet("s3://reference/products.parquet")

# Clean: Remove nulls, invalid amounts
sales_clean = sales \
    .dropna(subset=["order_id", "product_id", "amount"]) \
    .filter(col("amount") > 0) \
    .withColumn("amount", col("amount").cast("double"))

# Transform: Add calculated columns
sales_transform = sales_clean \
    .withColumn("tax", col("amount") * 0.1) \
    .withColumn("net_amount", col("amount") - col("tax")) \
    .withColumn("date_key", date_format(col("order_date"), "yyyyMMdd"))

# Join with products
joined = sales_transform.join(
    products,
    sales_transform.product_id == products.id,
    "left"
)

# Aggregate
result = joined.groupBy("category", "date_key") \
    .agg(
        count("*").alias("order_count"),
        sum("net_amount").alias("total_sales"),
        avg("net_amount").alias("avg_order_value"),
        max("amount").alias("max_order")
    )

# Write to Parquet (partitioned by date)
result.write \
    .mode("overwrite") \
    .partitionBy("date_key") \
    .parquet("s3://processed-data/sales_summary/")

# Verify
spark.read.parquet("s3://processed-data/sales_summary/").show()
```

## Scenario 2: Incremental Load (CDC)

**Problem**: Load only new/changed records since last load.

**Solution**:

```python
# Last load timestamp
last_load_time = spark.sql("SELECT MAX(modified_at) FROM delta_table").collect()[0][0]

# Read new data
new_data = spark.read.csv("s3://source/") \
    .filter(col("modified_at") > last_load_time)

# Merge into target
from delta.tables import DeltaTable

target = DeltaTable.forPath(spark, "s3://target/delta_table/")

target.alias("t").merge(
    new_data.alias("s"),
    "t.id = s.id"
).whenMatchedUpdateAll() \
.whenNotMatchedInsertAll() \
.execute()

# Track load time
spark.sql("""
    INSERT INTO load_metadata (last_load_time, record_count)
    VALUES (current_timestamp(), {})
""".format(new_data.count()))
```

## Scenario 3: SCD Type 2 Implementation

**Problem**: Maintain historical changes in a dimension table.

**Solution**:

```python
# New dimension data
new_data = spark.createDataFrame([
    (1, "Alice", "NYC", "2024-06-01"),
    (2, "Bob", "Seattle", "2024-06-01"),
    (3, "Charlie", "LA", "2024-06-01")
], ["cust_id", "name", "city", "effective_date"])

# Load current dimension
current = spark.read.parquet("s3://dimension/customer/")

# Find changed records
changed = new_data.alias("new") \
    .join(
        current.alias("current"),
        new_data.cust_id == current.cust_id,
        "left"
    ) \
    .filter(
        (col("current.cust_id").isNull()) | \
        (col("new.city") != col("current.city"))
    )

# Update old records (set end_date)
to_close = current \
    .join(
        changed.select("cust_id"),
        "cust_id",
        "inner"
    ) \
    .withColumn("end_date", lit(date_sub(current_date(), 1))) \
    .withColumn("is_current", lit(False))

# New records (no end_date)
to_add = new_data.select(
    "cust_id", "name", "city",
    col("effective_date").alias("start_date"),
    lit(None).alias("end_date"),
    lit(True).alias("is_current")
)

# Keep unchanged current records
unchanged = current \
    .join(
        changed.select("cust_id"),
        "cust_id",
        "anti"
    )

# Union all
result = to_close.union(to_add).union(unchanged)

# Write
result.write.mode("overwrite").parquet("s3://dimension/customer_scd2/")
```

## Scenario 4: Handling Skew

**Problem**: Some keys have disproportionately more data (skew).

**Solution**:

```python
# Detect skew
skew_analysis = df.groupBy("customer_id").count() \
    .orderBy(desc("count"))

skew_analysis.show()  # See data distribution

# Mitigation 1: Broadcast join (if small table)
df_large.join(broadcast(df_small), "id")

# Mitigation 2: Salting (add random suffix to high-volume keys)
df_salted = df.withColumn(
    "salt",
    when(
        col("customer_id").isin([highly_skewed_ids]),
        (rand() * 10).cast("int")
    ).otherwise(0)
)

df_salted = df_salted.withColumn(
    "salted_key",
    concat(col("customer_id"), col("salt"))
)

# Join on salted key
result = df_salted.join(other, "salted_key")

# Remove salt from result
result = result.withColumn("customer_id", regexp_replace("customer_id", "_\\d+", ""))

# Mitigation 3: Repartition to spread skew
df.repartition(500, "customer_id")
```

## Scenario 5: Large File Processing

**Problem**: Process 100GB+ file efficiently.

**Solution**:

```python
# Read with optimal settings
df = spark.read \
    .parquet("s3://large_file/") \
    .repartition(500)  # Ensure enough parallelism

# Filter early (predicate pushdown)
df_filtered = df.filter(col("amount") > 0)

# Use columnar operations (no UDFs)
result = df_filtered.groupBy("category") \
    .agg(
        sum("amount").alias("total"),
        count("*").alias("count"),
        avg("amount").alias("avg_amount")
    )

# Repartition for output (optimal for writing)
result.coalesce(100).write \
    .mode("overwrite") \
    .partitionBy("category") \
    .parquet("s3://output/")

# Monitor via Spark UI
# - Check shuffle read/write sizes
# - Ensure no severe skew
# - Monitor executor memory
```

## Scenario 6: Complex Window Function

**Problem**: Rank products by sales within category, keep top 3, calculate rank% of category total.

**Solution**:

```python
from pyspark.sql.window import Window

sales = spark.read.parquet("sales.parquet")

window_rank = Window.partitionBy("category") \
    .orderBy(col("total_sales").desc())

window_partition = Window.partitionBy("category")

result = sales.withColumn(
    "sales_rank",
    rank().over(window_rank)
).withColumn(
    "category_total",
    sum("total_sales").over(window_partition)
).withColumn(
    "sales_pct",
    (col("total_sales") / col("category_total") * 100).cast("decimal(5,2)")
).filter(
    col("sales_rank") <= 3
).select(
    "category", "product", "total_sales", "sales_rank", "sales_pct"
)

result.show()
```

## Scenario 7: Data Quality Checks

**Problem**: Validate data before processing.

**Solution**:

```python
# Quality checks
checks = {
    "null_orders": df.filter(col("order_id").isNull()).count(),
    "null_amounts": df.filter(col("amount").isNull()).count(),
    "negative_amounts": df.filter(col("amount") < 0).count(),
    "duplicate_orders": df.groupBy("order_id").count().filter(col("count") > 1).count(),
    "future_dates": df.filter(col("order_date") > current_date()).count()
}

# Report
for check, count in checks.items():
    status = "âœ“ PASS" if count == 0 else f"âœ— FAIL ({count})"
    print(f"{check}: {status}")

# Fail pipeline if critical checks fail
assert checks["null_orders"] == 0, "Found null order IDs"
assert checks["negative_amounts"] == 0, "Found negative amounts"

# Log to table
quality_log = spark.createDataFrame([
    (current_timestamp(), check, count)
    for check, count in checks.items()
], ["timestamp", "check_name", "failed_count"])

quality_log.write.mode("append").parquet("s3://quality_logs/")
```

## Scenario 8: Optimized Join

**Problem**: Join 10GB table with 5GB table efficiently.

**Solution**:

```python
# Read both tables
large = spark.read.parquet("large.parquet")
medium = spark.read.parquet("medium.parquet")

# Pre-filter to reduce data
large_filtered = large.filter(col("status") == "active")
medium_filtered = medium.filter(col("valid") == True)

# Pre-repartition both by join key
large_repartitioned = large_filtered.repartition(100, "id")
medium_repartitioned = medium_filtered.repartition(100, "id")

# Sort both (prepares for sort-merge join)
large_sorted = large_repartitioned.sortByKey()
medium_sorted = medium_repartitioned.sortByKey()

# Join (no shuffle, leverages pre-sorting)
result = large_sorted.join(medium_sorted, "id")

# Aggregate
final = result.groupBy("category") \
    .agg(sum("amount").alias("total"))

# Write
final.write.parquet("s3://output/")
```

---

## Conclusion

This comprehensive guide covers:

1. **Architecture**: Driver, executors, cluster managers, memory, schedulers
2. **Core Abstractions**: RDD, DataFrame, Dataset differences
3. **Optimization**: Catalyst, Tungsten, code generation
4. **Operations**: Transformations, actions, lazy evaluation
5. **Performance**: Partitioning, shuffling, joins, broadcasting
6. **Analytics**: Window functions, aggregations, deduplication
7. **Advanced**: UDF, Pandas UDF, caching, checkpointing
8. **Streaming**: Structured Streaming, watermarking, joins
9. **Storage**: Parquet, ORC, Delta, CSV, JSON
10. **Real-World Scenarios**: ETL, CDC, SCD Type 2, skew handling, data quality

Master these concepts to become a strong Spark engineer capable of building scalable, performant data pipelines.
