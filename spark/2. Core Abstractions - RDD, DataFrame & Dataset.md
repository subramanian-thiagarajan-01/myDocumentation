# 2. Core Abstractions: RDD, DataFrame & Dataset üìö

## RDD (Resilient Distributed Dataset)

RDDs are the **lowest-level abstraction** in Spark. They represent an immutable, distributed collection of objects that can be processed in parallel.

### Characteristics of RDD

**Resilient (Fault-tolerant)**: If a partition is lost, Spark can recompute it using the original transformation lineage.

```python
# Example: RDD fault tolerance
rdd = sc.parallelize([1, 2, 3, 4, 5])  # Create RDD from local data
rdd2 = rdd.map(lambda x: x * 2)  # Transformation

# If executor crashes after this point, Spark remembers:
# "rdd2 = rdd.map(lambda x: x * 2)"
# It can recompute rdd2 from the original rdd using the same transformation
```

**Immutable**: Once created, RDDs cannot be changed. Transformations create new RDDs.

```python
rdd = sc.parallelize([1, 2, 3])
rdd2 = rdd.map(lambda x: x * 2)  # Creates NEW RDD, doesn't modify rdd

# rdd still contains [1, 2, 3]
# rdd2 contains [2, 4, 6]
```

**Distributed**: Data is partitioned across multiple nodes and processed in parallel.

```python
# RDD partitions
rdd = sc.parallelize(range(100), numPartitions=4)
# This creates 4 partitions, each with 25 elements
# Partition 0: [0-24]
# Partition 1: [25-49]
# Partition 2: [50-74]
# Partition 3: [75-99]

# Each partition can be processed independently on different executors
```

### RDD Creation Methods

```python
# Method 1: From Hadoop storage (HDFS, S3, etc.)
rdd = sc.textFile("hdfs://path/to/file.txt")

# Method 2: Parallelizing existing collection
rdd = sc.parallelize([1, 2, 3, 4, 5], numPartitions=2)

# Method 3: Transforming existing RDD
rdd2 = rdd.map(lambda x: x * 2)

# Method 4: From another RDD
rdd3 = rdd.filter(lambda x: x > 10)
```

### RDD Transformations vs Actions

**Transformations**: Create new RDD from existing RDD (lazy)

```python
rdd1 = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = rdd1.map(lambda x: x * 2)         # Lazy - not executed yet
rdd3 = rdd2.filter(lambda x: x > 4)      # Lazy - not executed yet
```

**Actions**: Trigger actual computation and return results to driver

```python
result = rdd3.collect()  # Now actually computes map + filter
print(result)  # [6, 8, 10]
```

### RDD Internal Structure: Lineage

Each RDD maintains a **lineage** (DAG) showing how it was created:

```python
rdd1 = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = rdd1.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 4)

# Lineage for rdd3:
# rdd3 ‚Üí filter(x > 4) ‚Üí rdd2 ‚Üí map(x * 2) ‚Üí rdd1 ‚Üí parallelize()

# If partition of rdd3 is lost:
# 1. Recompute corresponding partition of rdd2
# 2. Recompute corresponding partition of rdd1 from parallelize()
# 3. Apply map transformation
# 4. Apply filter transformation
```

You can view the lineage:

```python
print(rdd3.toDebugString())  # Prints the entire DAG
```

### Why RDD is Less Used in Modern Data Engineering

1. **Untyped**: No schema information, requires manual parsing
2. **Less optimizable**: Catalyst optimizer can't optimize RDD operations
3. **Lower performance**: Slower than DataFrames for similar operations
4. **More verbose**: Requires more boilerplate code

```python
# RDD approach (verbose, untyped)
rdd = sc.textFile("data.csv")
rdd2 = rdd.map(lambda line: line.split(",")) \
    .filter(lambda fields: int(fields[1]) > 100) \
    .map(lambda fields: (fields[0], int(fields[2])))

# DataFrame approach (concise, typed)
df = spark.read.csv("data.csv", header=True, inferSchema=True)
result = df.filter(df.amount > 100).select("name", "price")
```

## DataFrame

**DataFrames** are distributed collections of data organized into **named columns**. They're similar to relational tables or Pandas DataFrames, but distributed across a cluster.

### DataFrame vs RDD

| Aspect            | RDD                       | DataFrame                |
| ----------------- | ------------------------- | ------------------------ |
| **Schema**        | None (untyped)            | Has schema (typed)       |
| **Optimization**  | No                        | Catalyst optimizer       |
| **Performance**   | Slower                    | Faster                   |
| **API**           | Functional (map, filter)  | SQL-like and functional  |
| **Serialization** | Full object serialization | Binary format (Tungsten) |

### DataFrame Schema

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Implicit schema inference
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.printSchema()

# Output:
# root
#  |-- name: string (nullable = true)
#  |-- age: integer (nullable = true)
#  |-- salary: double (nullable = true)

# Explicit schema definition
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("salary", DoubleType(), True)
])

df = spark.read.csv("data.csv", header=True, schema=schema)
```

### Creating DataFrames

```python
# From CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# From Parquet (recommended format)
df = spark.read.parquet("data.parquet")

# From JSON
df = spark.read.json("data.json")

# From Python list/dict
data = [
    ("Alice", 25, 70000),
    ("Bob", 30, 80000),
    ("Charlie", 35, 90000)
]
columns = ["name", "age", "salary"]
df = spark.createDataFrame(data, schema=columns)

# From Pandas DataFrame
import pandas as pd
pandas_df = pd.read_csv("data.csv")
df = spark.createDataFrame(pandas_df)
```

### DataFrame Properties

```python
df.count()              # Number of rows
df.columns              # Column names
df.dtypes               # Column data types
df.describe().show()    # Statistical summary
df.printSchema()        # Print schema
df.rdd                  # Convert to RDD (loses schema)
```

## Dataset

**Datasets** are **type-safe** versions of DataFrames (strongly-typed). They combine the benefits of RDDs (type safety) and DataFrames (optimization).

### Important: Dataset is Scala/Java Only

**Datasets are NOT available in PySpark**. In PySpark, you work with DataFrames exclusively.

```python
# ‚ùå This does NOT exist in PySpark
# dataset = df.as[MyClass]  # Not available

# ‚úÖ PySpark uses only DataFrames
df = spark.read.parquet("data.parquet")
```

### Why Not in PySpark?

1. Python is dynamically typed
2. Type safety is a Scala/Java feature
3. PySpark DataFrames handle most use cases

### Scala/Java Datasets (for reference)

In Scala:

```scala
// Scala example (not Python)
case class Person(name: String, age: Int, salary: Double)

val ds: Dataset[Person] = df.as[Person]
// Now type-safe: compile-time guarantees
// ds.map(_.age).show()  // Compiler knows _.age is Int
```

---
