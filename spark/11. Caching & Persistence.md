# 11. Caching & Persistence ğŸ’¾

**Caching** stores DataFrame/RDD in memory to speed up repeated access.

## Cache vs Persist

```python
from pyspark.sql.functions import *

df = spark.read.parquet("large_data.parquet")

# cache() is shorthand for persist(MEMORY_ONLY)
df.cache()

# persist() allows specifying storage level
df.persist(StorageLevel.MEMORY_ONLY)

# They trigger caching on first action
df.count()  # Caches data into memory

# Now, subsequent actions use cached data
df.count()  # Fast (reads from cache)
df.filter(col("amount") > 100).count()  # Fast (reads from cache)

# Unpersist (remove from cache)
df.unpersist()
```

## Storage Levels

```python
from pyspark.storage import StorageLevel

# MEMORY_ONLY (default for cache())
# Store in memory only
# If data doesn't fit, not stored
df.persist(StorageLevel.MEMORY_ONLY)

# MEMORY_AND_DISK
# Store in memory, spill excess to disk
# Guaranteed to be stored
df.persist(StorageLevel.MEMORY_AND_DISK)

# DISK_ONLY
# Store only on disk
# Useful for very large DataFrames
df.persist(StorageLevel.DISK_ONLY)

# MEMORY_ONLY_2
# Store in memory, 2 replicas across executors
# Higher availability but 2x memory usage
df.persist(StorageLevel.MEMORY_ONLY_2)

# OFF_HEAP
# Store off-heap memory (requires off-heap enabled)
df.persist(StorageLevel.OFF_HEAP)
```

## When to Cache

```python
# DO CACHE:

# 1. DataFrame used multiple times
df = spark.read.parquet("data.parquet")
df.cache()

result1 = df.filter(col("amount") > 100).count()
result2 = df.filter(col("amount") < 50).count()  # Uses cache
result3 = df.groupBy("category").count()  # Uses cache

# 2. Expensive computation (slow transformation)
df = spark.read.csv("large_csv.csv", inferSchema=True)
df.cache()  # CSV inference is slow

result = df.select("id", "name").show()

# 3. Reference table (used in joins)
dimension = spark.read.parquet("dimension.parquet")
dimension.cache()

fact1 = spark.read.parquet("fact1.parquet").join(dimension, "id")
fact2 = spark.read.parquet("fact2.parquet").join(dimension, "id")
# Both joins use cached dimension


# DON'T CACHE:

# 1. Large dataset that doesn't fit in memory
huge_df = spark.read.parquet("1tb_dataset.parquet")
# huge_df.cache()  # âŒ Will spill to disk, defeating purpose

# 2. One-time use
df = spark.read.parquet("data.parquet")
result = df.filter(col("status") == "active").show()
# No need to cache if only used once

# 3. Rapidly changing data
stream_df = spark.readStream.kafka(...)
# stream_df.cache()  # âŒ Cache is static, streaming is dynamic
```

## Monitoring Cache

```python
# Check cached DataFrames via Spark UI
# - localhost:4040
# - Storage tab: Shows cached RDD/DataFrame sizes

# Programmatic check
storage_status = spark.sparkContext.getRDDStorageInfo()
for rdd in storage_status:
    print(f"RDD: {rdd.id}, Size: {rdd.memSize}")

# Clear all cache
spark.catalog.clearCache()

# Clear specific DataFrame
df.unpersist()

# Check cache statistics
spark.sparkContext._sc.getRDDStorageInfo()
```
