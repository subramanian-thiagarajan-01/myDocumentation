# 10. UDF & Pandas UDF ðŸ

## UDF (User-Defined Functions)

**UDFs** allow you to create custom functions that execute on every row.

### Row-by-Row UDF

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType, DoubleType

# Define Python function
def add_ten(value):
    return value + 10

# Register as UDF
add_ten_udf = udf(add_ten, IntegerType())

# Use in DataFrame
df = spark.createDataFrame([(1,), (2,), (3,)], ["num"])
result = df.select(add_ten_udf(col("num")).alias("result"))

# Output:
# num | result
# 1   | 11
# 2   | 12
# 3   | 13
```

### UDF Performance Issues

UDFs are **slow** because:

1. **Serialization overhead**: Data serialized to Python
2. **Row-by-row execution**: No vectorization
3. **Python GIL**: Single-threaded Python execution

```python
# Performance comparison: UDF vs Built-in

# Using UDF (SLOW)
def multiply_by_two(x):
    return x * 2

multiply_udf = udf(multiply_by_two, IntegerType())
df_udf = df.select(multiply_udf(col("amount")))  # Processes row-by-row

# Using built-in function (FAST)
df_builtin = df.select(col("amount") * 2)  # Vectorized, codegen
```

### UDF Use Cases

Only use UDFs when:

- Complex business logic not available in SQL
- External API calls (rate conversion API, ML model)
- Legacy code integration

```python
# Example: External API call (justified UDF)
import requests

def convert_currency(amount, target_currency):
    # Call external API
    response = requests.get(
        f"https://api.rates.com/convert?amount={amount}&to={target_currency}"
    )
    return response.json()["converted_amount"]

convert_udf = udf(convert_currency, DoubleType())

df = spark.createDataFrame([
    (100, "EUR"),
    (200, "GBP")
], ["amount", "currency"])

result = df.select(
    col("amount"),
    col("currency"),
    convert_udf(col("amount"), col("currency")).alias("converted")
)
```

### UDF With Complex Return Type

```python
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

# Return multiple values
def parse_data(raw_string):
    parts = raw_string.split("|")
    return (parts[0], float(parts[1]))

schema = StructType([
    StructField("name", StringType()),
    StructField("value", DoubleType())
])

parse_udf = udf(parse_data, schema)

df = spark.createDataFrame([
    ("Alice|100.5",),
    ("Bob|200.75",)
], ["raw"])

result = df.select(
    parse_udf("raw").alias("parsed")
).select(col("parsed.name"), col("parsed.value"))
```

## Pandas UDF (Vectorized UDF)

**Pandas UDF** is a high-performance alternative that processes batches using Apache Arrow.

### Benefits of Pandas UDF

1. **Vectorized execution**: Process entire batch at once
2. **Apache Arrow**: Efficient data transfer between JVM and Python
3. **10-100x faster** than row-by-row UDF

### Creating Pandas UDF

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

# Define function that takes and returns pandas Series
@pandas_udf(DoubleType())
def multiply_by_two(s: pd.Series) -> pd.Series:
    return s * 2

# Use in DataFrame
df = spark.createDataFrame([(1.0,), (2.0,), (3.0,)], ["value"])
result = df.select(multiply_by_two(col("value")).alias("result"))

# Internally:
# 1. Converts Arrow column to pandas Series (batch of rows)
# 2. Calls function on entire Series (vectorized)
# 3. Converts result back to Arrow
# 4. Transfers to JVM
```

### Pandas UDF for Aggregation

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

# Grouped aggregation
@pandas_udf(DoubleType())
def mean_udf(s: pd.Series) -> float:
    return s.mean()

df = spark.createDataFrame([
    ("A", 10),
    ("A", 20),
    ("B", 30),
    ("B", 40)
], ["group", "value"])

result = df.groupBy("group").agg(mean_udf("value"))

# Output:
# group | mean_udf(value)
# A     | 15.0
# B     | 35.0
```

### Pandas UDF for Grouped Transform

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType

# Transform: output same number of rows as input
@pandas_udf(DoubleType())
def normalize(s: pd.Series) -> pd.Series:
    return (s - s.mean()) / s.std()

result = df.groupby("group").apply(normalize)

# Normalizes within each group
```

### Pandas UDF with ML Models

```python
import pandas as pd
import numpy as np
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType
import joblib

# Load pre-trained model
model = joblib.load("model.pkl")

@pandas_udf(DoubleType())
def predict(features: pd.Series) -> pd.Series:
    # Reshape for model input
    X = features.values.reshape(-1, 1)
    predictions = model.predict(X)
    return pd.Series(predictions)

df = spark.createDataFrame([(1.0,), (2.0,), (3.0,)], ["feature"])
result = df.select(predict(col("feature")).alias("prediction"))
```

### Performance Comparison

```python
import time

df = spark.createDataFrame([(i,) for i in range(1000000)], ["value"])

# Row-by-row UDF
@udf(IntegerType())
def row_udf(x):
    return x * 2

start = time.time()
df.select(row_udf(col("value"))).count()
row_time = time.time() - start

# Pandas UDF
@pandas_udf(IntegerType())
def pandas_udf_func(s: pd.Series):
    return s * 2

start = time.time()
df.select(pandas_udf_func(col("value"))).count()
pandas_time = time.time() - start

print(f"Row UDF: {row_time}s")
print(f"Pandas UDF: {pandas_time}s")
print(f"Speedup: {row_time / pandas_time:.1f}x")

# Typical output:
# Row UDF: 45.2s
# Pandas UDF: 2.1s
# Speedup: 21.5x
```
