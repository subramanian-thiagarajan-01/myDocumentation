# 4. Transformations & Actions ðŸ”

# Transformations & Actions

## Transformations: Creating New DataFrames

Transformations are **lazy operations** that don't execute immediately. They build a computation plan.

### Common Transformations

#### 1. Select & SelectExpr

```python
df = spark.read.parquet("sales.parquet")

# Select specific columns
df.select("id", "name", "amount").show()

# Select using expressions
df.select(
    "id",
    "name",
    (col("amount") * 1.1).alias("amount_with_tax"),
    when(col("amount") > 1000, "Premium").otherwise("Regular").alias("tier")
).show()

# SelectExpr allows raw SQL expressions
df.selectExpr(
    "id",
    "name",
    "amount * 1.1 as amount_with_tax"
).show()
```

#### 2. Filter/Where

```python
# Filter rows based on conditions
df.filter(col("amount") > 100).show()

# Multiple conditions
df.filter((col("amount") > 100) & (col("status") == "active")).show()

# Using SQL expression
df.filter("amount > 100 AND status = 'active'").show()

# Where is alias for filter
df.where(col("amount") > 100).show()
```

#### 3. WithColumn & WithColumnRenamed

```python
# Add new column
df = df.withColumn("tax", col("amount") * 0.1)

# Modify existing column
df = df.withColumn("amount", col("amount") * 1.1)

# Rename column
df = df.withColumnRenamed("amount", "total_amount")

# Multiple transformations chained
df = df \
    .withColumn("tax", col("amount") * 0.1) \
    .withColumn("net_amount", col("amount") - col("tax")) \
    .withColumnRenamed("id", "customer_id")
```

#### 4. Join

```python
customers = spark.read.parquet("customers.parquet")
orders = spark.read.parquet("orders.parquet")

# Inner join (default)
result = customers.join(orders, "customer_id", "inner")

# Left outer join
result = customers.join(orders, "customer_id", "left")

# Right join
result = customers.join(orders, "customer_id", "right")

# Full outer join
result = customers.join(orders, "customer_id", "outer")

# Anti join (left rows with no match in right)
result = customers.join(orders, "customer_id", "anti")

# Semi join (left rows with match in right, no right columns)
result = customers.join(orders, "customer_id", "semi")

# Join on different column names
result = customers.join(
    orders,
    customers.id == orders.customer_id,
    "inner"
)

# Multiple join conditions
result = customers.join(
    orders,
    (customers.id == orders.customer_id) &
    (customers.region == orders.region),
    "inner"
)
```

#### 5. GroupBy & Aggregations

```python
# Basic groupBy
result = df.groupBy("category").agg(sum("amount"))

# Multiple group columns
result = df.groupBy("category", "region").agg(
    sum("amount").alias("total_amount"),
    count("id").alias("order_count"),
    avg("amount").alias("avg_amount")
)

# Multiple aggregations
from pyspark.sql.functions import *

result = df.groupBy("category").agg(
    sum("amount").alias("total"),
    count("*").alias("count"),
    avg("amount").alias("avg"),
    min("amount").alias("min"),
    max("amount").alias("max"),
    stddev("amount").alias("stddev")
)

# Having clause (filter after groupBy)
result = df.groupBy("category").agg(
    sum("amount").alias("total")
).filter(col("total") > 10000)
```

#### 6. Distinct & DropDuplicates

```python
# Remove duplicate rows (all columns must match)
df_unique = df.distinct()

# Remove duplicates based on specific columns
df_unique = df.dropDuplicates(["customer_id", "email"])

# Drop duplicates and keep first occurrence
df_unique = df.dropDuplicates(["email"]).orderBy("created_at")
```

#### 7. Union & UnionByName

```python
df1 = spark.read.parquet("sales_2023.parquet")
df2 = spark.read.parquet("sales_2024.parquet")

# Union (combines rows, order matters)
result = df1.union(df2)

# UnionByName (combines by column name, not order)
result = df1.unionByName(df2)

# Handling different schemas
df1 = spark.createDataFrame([(1, "Alice")], ["id", "name"])
df2 = spark.createDataFrame([(2, "Bob", "Engineer")], ["id", "name", "title"])

# Allow missing columns (fills with nulls)
result = df1.unionByName(df2, allowMissingColumns=True)
```

#### 8. Explode

```python
# Explode array column into separate rows
df = spark.createDataFrame([
    (1, ["a", "b", "c"]),
    (2, ["d", "e"])
], ["id", "items"])

result = df.select("id", explode("items").alias("item"))

# Output:
# id | item
# 1  | a
# 1  | b
# 1  | c
# 2  | d
# 2  | e

# Explode map into key-value pairs
df = spark.createDataFrame([
    (1, {"key1": "value1", "key2": "value2"}),
], ["id", "data"])

result = df.select("id", explode("data")).alias("key", "value"))
```

#### 9. Pivot

```python
# Convert rows to columns
sales = spark.createDataFrame([
    ("2024-01", "A", 1000),
    ("2024-01", "B", 2000),
    ("2024-02", "A", 1500),
    ("2024-02", "B", 2500)
], ["month", "category", "amount"])

# Pivot category into columns
result = sales.groupBy("month") \
    .pivot("category") \
    .sum("amount")

# Output:
# month | A    | B
# 2024-01| 1000| 2000
# 2024-02| 1500| 2500
```

#### 10. Melt/Unpivot

```python
# There's no built-in unpivot, so we use stack + explode
df = spark.createDataFrame([
    ("2024-01", 1000, 2000)
], ["month", "cat_A", "cat_B"])

# Unpivot using stack
result = df.select(
    "month",
    explode(array(
        struct(lit("A").alias("category"), col("cat_A").alias("amount")),
        struct(lit("B").alias("category"), col("cat_B").alias("amount"))
    )).alias("data")
).select("month", col("data.category"), col("data.amount"))
```

#### 11. OrderBy & Sort

```python
# Sort ascending (default)
df.orderBy("amount").show()

# Sort descending
df.orderBy(col("amount").desc()).show()

# Multiple column sort
df.orderBy(col("category").asc(), col("amount").desc()).show()

# Sort by SQL expression
df.sort(col("amount") * 1.1).show()

# Using null handling
df.orderBy(col("amount").asc_nulls_last()).show()
df.orderBy(col("amount").desc_nulls_first()).show()
```

#### 12. Limit & Offset

```python
# First N rows
df.limit(10).show()

# Offset not directly available, use with row_number()
from pyspark.sql.window import Window

# Skip first 100, take next 50
result = df.withColumn(
    "rn", row_number().over(Window.orderBy("id"))
).filter((col("rn") > 100) & (col("rn") <= 150)) \
.drop("rn")
```

### Complex Transformation Example

```python
# Complete transformation pipeline
result = (
    spark.read.parquet("sales.parquet")
    .filter(col("status") == "completed")
    .withColumn("revenue", col("quantity") * col("price"))
    .withColumn("region", when(col("country") == "US", "North America")
                           .when(col("country") == "CA", "North America")
                           .otherwise("Other"))
    .groupBy("region", "product_category")
    .agg(
        sum("revenue").alias("total_revenue"),
        count("*").alias("total_orders"),
        avg("revenue").alias("avg_order_value")
    )
    .filter(col("total_revenue") > 100000)
    .orderBy(col("total_revenue").desc())
)

result.show()
```

## Actions: Triggering Computation

Actions execute the computation and return results to the driver or write to storage.

### Common Actions

#### 1. Show

```python
# Display first 20 rows
df.show()

# Display first N rows
df.show(50)

# Display without truncating column values
df.show(truncate=False)

# Display all rows (warning: can be huge!)
df.show(df.count())
```

#### 2. Count

```python
# Count rows (triggers full scan)
num_rows = df.count()

# This is expensive for large datasets
# Avoid calling multiple times (no caching)
```

#### 3. Collect

```python
# Bring entire DataFrame to driver memory
all_data = df.collect()

# Returns list of Row objects
for row in all_data:
    print(row.id, row.name, row.amount)
```
