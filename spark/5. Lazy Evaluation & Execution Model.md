# 5. Lazy Evaluation & Execution Model ðŸ§ 

## Understanding Lazy Evaluation

Spark uses **lazy evaluation**, meaning transformations are not executed immediately. Instead, Spark builds a **Directed Acyclic Graph (DAG)** of transformations and only executes when an action is called.

### Why Lazy Evaluation?

1. **Optimization**: Spark can see the full computation plan and optimize globally
2. **Efficient Resource Use**: Unnecessary computations are avoided
3. **Chain Operations**: Combine multiple transformations efficiently

### Example: Lazy Evaluation in Action

```python
# None of these execute immediately
df = spark.read.parquet("large_file.parquet")
filtered = df.filter(col("amount") > 100)
grouped = filtered.groupBy("category").agg(sum("amount"))
selected = grouped.select("category", "sum(amount)")

# NOW it executes (action triggered)
result = selected.collect()

# Execution flow:
# 1. Read parquet file (only necessary columns)
# 2. Filter rows where amount > 100
# 3. Group by category
# 4. Sum amounts per category
# 5. Collect results to driver
```

### Viewing the DAG

```python
df = spark.read.parquet("data.parquet")
result = df.filter(col("amount") > 100) \
    .groupBy("category") \
    .agg(sum("amount"))

# View the logical plan (before optimization)
result.explain(mode="simple")

# View optimized logical plan
result.explain(mode="extended")

# View physical plan (actual execution strategy)
result.explain(mode="codegen")

# Example output for explain():
# == Physical Plan ==
# *(2) HashAggregate(keys=[category#0], functions=[sum(amount#1)],
#      output=[category#0, sum(amount)#3L])
# +- Exchange hashpartitioning(category#0, 200)
#    +- *(1) HashAggregate(keys=[category#0], functions=[partial_sum(amount#1)],
#         output=[category#0, sum(amount)#3L])
#       +- *(1) Filter (amount#1 > 100)
#          +- *(1) FileScan parquet [category#0,amount#1]
```

### Breakdown of Explain Output

The physical plan shows:

1. **FileScan**: Read the parquet file
2. **Filter**: Apply WHERE clause
3. **HashAggregate (partial)**: Per-partition aggregation
4. **Exchange**: Shuffle data (brings together same keys)
5. **HashAggregate (final)**: Final aggregation after shuffle

The `(1)`, `(2)` indicate stage boundaries.

## Execution Model: Stages & Tasks

### Stage Creation

Spark divides execution into **stages** at shuffle boundaries. Each stage contains tasks that can run in parallel without shuffling.

```
Example: df.filter(...).groupBy(...).sum()

Stage 1 (no shuffle):
  - Scan parquet file
  - Apply filter
  â†’ Creates one task per partition
  â†’ Tasks run in parallel on different executors

[SHUFFLE BOUNDARY]

Stage 2 (with shuffle):
  - Shuffle: Group data by key across all partitions
  â†’ Each key's data moves to same executor/partition
  â†’ Perform aggregation locally
  â†’ Tasks run in parallel, each processing different keys
```

### Task Distribution

```python
# Example: 100GB file partitioned into 1000 partitions
df = spark.read.parquet("large_file.parquet")

# Stage 1: 1000 tasks created (one per partition)
# If we have 10 executors with 4 cores each:
# - 40 tasks can run in parallel
# - Total time = 1000 / 40 = 25 task batches

# Each task processes one partition:
# Task 1: Reads partition 0, filters rows, produces output partition
# Task 2: Reads partition 1, filters rows, produces output partition
# ...
```

### Task Scheduling & Locality

The scheduler attempts **data locality**:

```python
# Parquet file stored on HDFS
df = spark.read.parquet("hdfs://namenode/data.parquet")

# HDFS stores data in blocks across nodes
# Block 0 on Node A
# Block 1 on Node B
# Block 2 on Node C

# Scheduler tries to:
# - Schedule Block 0's task to an executor on Node A
# - Schedule Block 1's task to an executor on Node B
# - Schedule Block 2's task to an executor on Node C

# If an executor on Node A isn't available:
# - Falls back to NODE_LOCAL (different executor on Node A)
# - Then RACK_LOCAL (different rack)
# - Finally ANY (any available executor)
```

### Task Retries

Spark automatically retries failed tasks:

```python
spark = SparkSession.builder \
    .config("spark.task.maxFailures", "4") \
    .getOrCreate()

# If a task fails:
# Retry 1: Same executor
# Retry 2: Same executor
# Retry 3: Same executor
# Retry 4: Different executor
# If all 4 retries fail: Stage fails, job fails

# Configurable per application
```

## Complete Execution Flow Example

```python
# User code
df = spark.read.parquet("sales.parquet")  # 1000 partitions
result = df.filter(col("amount") > 100) \
    .groupBy("category") \
    .agg(sum("amount")) \
    .filter(col("sum(amount)") > 10000)

data = result.collect()

# EXECUTION FLOW:
print("STEP 1: DAG Construction (Driver)")
print("  - filter(amount > 100)")
print("  - groupBy(category) â†’ Shuffle barrier")
print("  - agg(sum(amount))")
print("  - filter(sum(amount) > 10000)")
print()

print("STEP 2: Catalyst Optimization")
print("  - Push filter(amount > 100) before groupBy")
print("  - Only read 'category' and 'amount' columns")
print()

print("STEP 3: Stage Division")
print("  - Stage 1: Scan + Filter (no shuffle)")
print("  - Stage 2: GroupBy + Aggregation (with shuffle)")
print()

print("STEP 4: Stage 1 Execution (1000 tasks)")
print("  - Create 1000 tasks, one per partition")
print("  - Schedule to executors considering data locality")
print("  - Each task: Read â†’ Filter â†’ Partial aggregation")
print()

print("STEP 5: Shuffle")
print("  - All tasks in Stage 1 complete")
print("  - Shuffle: Move data to group by 'category' key")
print("  - Writer: Tasks write shuffled data to disk/memory")
print("  - Reader: Tasks in Stage 2 read shuffled data")
print()

print("STEP 6: Stage 2 Execution (varies based on shuffle partitions)")
print("  - Default 200 partitions after shuffle")
print("  - Create 200 tasks for Stage 2")
print("  - Each task: Read shuffled data â†’ Final aggregation")
print("  - Apply post-aggregation filter")
print()

print("STEP 7: Collect (Action)")
print("  - Bring results to driver memory")
print("  - Return to user")
```
