# 3. Spark SQL & Catalyst Optimizer ⚡

# Spark SQL & Catalyst Optimizer

## Spark SQL Architecture

**Spark SQL** is a module that allows you to:

1. Query structured data using SQL
2. Create temporary views and databases
3. Leverage the Catalyst optimizer for performance

### Unified API: SQL vs DataFrame API

Both SQL and DataFrame API access the same Catalyst optimizer:

```python
# SQL approach
spark.sql("SELECT category, SUM(amount) FROM sales WHERE amount > 100 GROUP BY category")

# DataFrame approach (same optimization)
df.filter(df.amount > 100) \
    .groupBy("category") \
    .agg(sum("amount"))

# Both execute identically after Catalyst optimization
```

### Registering Temporary Views

```python
df = spark.read.parquet("sales.parquet")

# Register temporary view (session-scoped)
df.createOrReplaceTempView("sales")

# Register global temporary view (cluster-scoped)
df.createGlobalTempView("global_sales")

# Now query using SQL
result = spark.sql("SELECT * FROM sales WHERE amount > 100")

# Access global view
result2 = spark.sql("SELECT * FROM global_temp.global_sales")
```

### Working with Databases

```python
# Create database
spark.sql("CREATE DATABASE IF NOT EXISTS my_db")

# Create managed table
spark.sql("""
    CREATE TABLE IF NOT EXISTS my_db.customers (
        id INT,
        name STRING,
        email STRING
    ) USING PARQUET
""")

# Create external table
spark.sql("""
    CREATE EXTERNAL TABLE IF NOT EXISTS my_db.orders (
        id INT,
        customer_id INT,
        amount DOUBLE
    ) USING PARQUET
    LOCATION 's3://bucket/orders/'
""")

# Query
spark.sql("SELECT * FROM my_db.customers WHERE id > 100")
```

## Catalyst Optimizer: Deep Dive

The **Catalyst Optimizer** is the heart of Spark's performance. It transforms logical plans into optimized physical plans.

### Optimization Pipeline

```
User Code (DataFrame API or SQL)
    ↓
PARSER: Convert to Abstract Syntax Tree (AST)
    ↓
ANALYZER: Validate against schema, resolve column references
    ↓
LOGICAL PLAN: Unoptimized logical operations
    ↓
OPTIMIZER: Apply optimization rules
    ↓
PHYSICAL PLANNER: Convert to physical plan with strategies
    ↓
CODE GENERATION: Generate Java bytecode
    ↓
EXECUTION: Run on executors
```

### Optimization Rules Applied By Catalyst

#### 1. Predicate Pushdown

Move filter operations as early as possible (before joins):

```python
# Original (inefficient)
df1 = spark.read.parquet("large_table.parquet")
df2 = spark.read.parquet("small_table.parquet")
joined = df1.join(df2, "id")
result = joined.filter(joined.amount > 100)

# Catalyst optimizes to:
# 1. Filter df1 first (before join)
# 2. Join smaller datasets
# 3. This reduces shuffled data significantly
```

#### 2. Column Pruning

Only read necessary columns:

```python
# Original
df = spark.read.parquet("sales.parquet")  # All columns read
result = df.select("customer_id", "amount")

# Catalyst optimizes to:
# Only read "customer_id" and "amount" columns from storage
# Skips reading unnecessary columns entirely
```

#### 3. Constant Folding

Evaluate constant expressions at compile time:

```python
# Original
df.filter((1 + 2) * 5 > df.amount)

# Catalyst optimizes to:
# df.filter(15 > df.amount)  # Evaluates (1+2)*5 = 15 once
```

#### 4. Dead Code Elimination

Remove unused expressions:

```python
# Original
df.select("id", "name", "email").select("id", "name")

# Catalyst optimizes to:
# df.select("id", "name")  # Removes intermediate select
```

#### 5. Boolean Simplification

Simplify boolean expressions:

```python
# Original
df.filter((df.amount > 100) AND (df.amount > 100))

# Catalyst optimizes to:
# df.filter(df.amount > 100)  # Removes duplicate condition
```

#### 6. Join Reordering

Reorder joins for optimal execution:

```python
# Original (if small_table is small)
large_table.join(small_table)

# Catalyst recognizes small_table is tiny
# Converts to broadcast join automatically
# Avoids expensive shuffle
```

### Viewing the Execution Plan

```python
df = spark.read.parquet("sales.parquet")
result = df.filter(df.amount > 100) \
    .groupBy("category") \
    .agg(sum("amount"))

# View logical plan (unoptimized)
result.explain(mode="simple")

# View optimized logical plan
result.explain(mode="extended")

# View physical plan (actual execution)
result.explain(mode="codegen")

# Output example:
# == Physical Plan ==
# *(2) HashAggregate(keys=[category#23], functions=[sum(amount#24)])
# +- Exchange hashpartitioning(category#23, 200)
#    +- *(1) HashAggregate(keys=[category#23], functions=[partial_sum(amount#24)])
#       +- *(1) Filter (amount#24 > 100)
#          +- *(1) FileScan parquet [category#23,amount#24]
```

### Custom Optimization Rules

```python
# Enabling specific optimizations
spark = SparkSession.builder \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()
```

---
