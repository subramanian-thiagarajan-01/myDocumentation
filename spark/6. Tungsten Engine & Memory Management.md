# 6. Tungsten Engine & Memory Management ⚙️

## Tungsten: Project Tungsten

**Project Tungsten** is Spark's initiative for optimizing physical execution. It focuses on memory management, CPU efficiency, and code generation.

### Key Components of Tungsten

#### 1. Memory Management

**Tungsten** introduced **off-heap memory management** using **sun.misc.Unsafe** (or equivalent):

- **Problem with Java Objects**: Each Java object has overhead (header, pointers, alignment)

  - A simple (id: Long, name: String) takes 50+ bytes

- **Tungsten Solution**: Store data in **binary format** with minimal overhead
  - Same (id: Long, name: String) takes ~25-30 bytes
  - 40-50% memory savings

```python
# Memory layout with Tungsten (binary format):
# 8 bytes: Long (id)
# Variable: UTF-8 string (name)
# vs.
# Java Object: 16 bytes header + 8 bytes reference + variable string + padding
```

#### 2. Unsafe Memory Operations

Tungsten uses direct memory access:

```java
// Internal Spark code (Java/Scala)
// Unsafe directly manipulates memory without JVM bounds checking
// This is fast but requires careful management

// Example: Writing to off-heap memory
unsafe.putLong(address, value);  // Write 8 bytes of data directly
```

#### 3. Code Generation

Tungsten generates **custom Java bytecode** for specific operations:

```python
# Example: filter operation

# Without code generation (interpreted):
for row in rows:
    if evaluate_expression(row):  # Dynamic interpretation
        yield row

# With code generation (compiled):
# Spark generates this Java method:
public boolean filter_udf_0(InternalRow row) {
    return row.getInt(1) > 100;  // Direct bytecode, no interpretation
}
```

The benefits:

- **Eliminates interpreter overhead**: Direct CPU execution
- **Vectorization**: Process multiple rows simultaneously
- **CPU cache efficiency**: Better data locality

```python
# Viewing generated code
spark = SparkSession.builder \
    .config("spark.sql.codegen.wholeStage", "true") \
    .getOrCreate()

result = df.filter(col("amount") > 100)
result.explain(mode="codegen")

# Output shows the generated Java code
```

### Memory Organization on Executor

```
┌─────────────────────────────────────────────────┐
│         Executor JVM Heap (e.g., 4GB)           │
├─────────────────────────────────────────────────┤
│                                                 │
│  ┌─────────────────────────────────────────┐    │
│  │   Unified Memory (spark.memory.fraction)│    │
│  │   Default: 75% of heap = 3GB            │    │
│  │                                         │    │
│  │  ┌──────────────────────────────────┐   │    │
│  │  │ Execution Memory (60% of 3GB)    │   │    │
│  │  │ = 1.8 GB                         │   │    │
│  │  │                                  │   │    │
│  │  │ Used for:                        │   │    │
│  │  │ - Shuffle buffers                │   │    │
│  │  │ - Aggregation hashmaps           │   │    │
│  │  │ - Sort buffers                   │   │    │
│  │  │ - Join hashmaps                  │   │    │
│  │  └──────────────────────────────────┘   │    │
│  │                                         │    │
│  │  ┌──────────────────────────────────┐   │    │
│  │  │ Storage Memory (40% of 3GB)      │   │    │
│  │  │ = 1.2 GB                         │   │    │
│  │  │                                  │   │    │
│  │  │ Used for:                        │   │    │
│  │  │ - Cached DataFrames              │   │    │
│  │  │ - RDD partitions                 │   │    │
│  │  │ - Broadcast variables            │   │    │
│  │  └──────────────────────────────────┘   │    │
│  └─────────────────────────────────────────┘    │
│                                                 │
│  ┌─────────────────────────────────────────┐    │
│  │  Other Memory (25% of heap = 0.75GB)    │    │
│  │  - Python processes                     │    │
│  │  - OS memory                            │    │
│  │  - User code allocations                │    │
│  └─────────────────────────────────────────┘    │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│    Off-Heap Memory (if enabled)                 │
│    spark.memory.offHeap.size = 2GB              │
│                                                 │
│  Used for:                                      │
│  - Shuffle data (if shuffle service enabled)    │
│  - Caching (if configured)                      │
└─────────────────────────────────────────────────┘
```

### Memory Spillover

When memory is exhausted, Spark spills data to disk:

```python
# Configuration for spill behavior
spark = SparkSession.builder \
    .config("spark.memory.storageFraction", 0.5) \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# When performing groupBy on 100GB of data:
# 1. Group data in memory using Tungsten binary format
# 2. Once execution memory fills up, spill excess to disk
# 3. Continue processing with disk-backed storage
# 4. Merge results from memory and disk
```

### Monitoring Memory Usage

```python
# Check memory configuration
spark.sparkContext.getConf().get("spark.executor.memory")  # "4g"
spark.sparkContext.getConf().get("spark.memory.fraction")  # "0.75"

# Spark UI: localhost:4040
# - Executors tab shows real-time memory usage
# - Storage tab shows cached DataFrame sizes
# - Stages tab shows shuffle write/read data

# Access Spark Metrics
rdd = sc.parallelize(range(100), numPartitions=4)
metrics = rdd.getNumPartitions()  # Get partition count
```
