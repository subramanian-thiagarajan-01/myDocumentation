# 9. Window Functions ðŸªŸ

**Window functions** compute values over a specified "window" (range) of rows while preserving individual rows. Essential for analytics and time-series operations.

## Window Function Components

```
ROW_NUMBER() OVER (
    PARTITION BY category      <- Divide rows into groups
    ORDER BY date ASC          <- Order within each group
    ROWS BETWEEN 1 PRECEDING AND CURRENT ROW  <- Frame
)
```

### 1. Partition Clause

Divides rows into groups:

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

sales = spark.createDataFrame([
    (1, "Electronics", 100),
    (2, "Electronics", 200),
    (3, "Books", 50),
    (4, "Books", 75),
    (5, "Electronics", 150)
], ["id", "category", "amount"])

# Partition by category
window_spec = Window.partitionBy("category")
result = sales.withColumn(
    "category_total",
    sum("amount").over(window_spec)
)

# Output:
# id | category    | amount | category_total
# 1  | Electronics | 100    | 450  (100+200+150)
# 2  | Electronics | 200    | 450
# 5  | Electronics | 150    | 450
# 3  | Books       | 50     | 125  (50+75)
# 4  | Books       | 75     | 125
```

### 2. Order Clause

Orders rows within each partition:

```python
dates = spark.createDataFrame([
    ("2024-01-01", "Electronics", 100),
    ("2024-01-02", "Electronics", 200),
    ("2024-01-03", "Electronics", 150),
    ("2024-01-01", "Books", 50),
    ("2024-01-02", "Books", 75)
], ["date", "category", "amount"])

# Order by date within each category
window_spec = Window.partitionBy("category").orderBy("date")
result = dates.withColumn(
    "running_total",
    sum("amount").over(window_spec)
)

# Output:
# date       | category    | amount | running_total
# 2024-01-01 | Electronics | 100    | 100
# 2024-01-02 | Electronics | 200    | 300 (100+200)
# 2024-01-03 | Electronics | 150    | 450 (100+200+150)
# 2024-01-01 | Books       | 50     | 50
# 2024-01-02 | Books       | 75     | 125 (50+75)
```

### 3. Frame Clause

Specifies which rows to include in the window:

```python
# UNBOUNDED PRECEDING to CURRENT ROW (default)
window_spec = Window.partitionBy("category") \
    .orderBy("date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Running total (all rows up to current)
result = sales.withColumn(
    "running_total",
    sum("amount").over(window_spec)
)

# Last 2 rows
window_spec = Window.partitionBy("category") \
    .orderBy("date") \
    .rowsBetween(-1, Window.currentRow)

# Average of last 2 rows
result = sales.withColumn(
    "avg_last_2",
    avg("amount").over(window_spec)
)

# Entire partition
window_spec = Window.partitionBy("category") \
    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)

# Partition total
result = sales.withColumn(
    "partition_total",
    sum("amount").over(window_spec)
)
```

## Common Window Functions

### 1. Row Number / Rank / Dense Rank

```python
# ROW_NUMBER: Sequential number within partition
window_spec = Window.partitionBy("category").orderBy(col("amount").desc())
result = sales.withColumn(
    "row_num",
    row_number().over(window_spec)
)

# Output: 1, 2, 3, ... for each category

# RANK: Same rank for ties, skip next rank
result = sales.withColumn(
    "rank",
    rank().over(window_spec)
)

# If amounts are [200, 200, 100]:
# Ranks: 1, 1, 3 (skips rank 2)

# DENSE_RANK: Same rank for ties, no skip
result = sales.withColumn(
    "dense_rank",
    dense_rank().over(window_spec)
)

# If amounts are [200, 200, 100]:
# Dense Ranks: 1, 1, 2 (no skip)
```

### 2. Aggregation Functions

```python
window_spec = Window.partitionBy("category")

result = sales.withColumn(
    "total", sum("amount").over(window_spec)
).withColumn(
    "avg", avg("amount").over(window_spec)
).withColumn(
    "count", count("*").over(window_spec)
).withColumn(
    "max", max("amount").over(window_spec)
).withColumn(
    "min", min("amount").over(window_spec)
)

# All aggregations applied per partition
```

### 3. Lead & Lag

Access previous/next row values:

```python
window_spec = Window.partitionBy("category").orderBy("date")

result = sales.withColumn(
    "prev_amount", lag("amount", 1).over(window_spec)
).withColumn(
    "next_amount", lead("amount", 1).over(window_spec)
).withColumn(
    "prev_3", lag("amount", 3, 0).over(window_spec)  # 3 rows back, default 0
)

# Output:
# date | amount | prev_amount | next_amount
# 01   | 100    | null        | 200
# 02   | 200    | 100         | 150
# 03   | 150    | 200         | null
```

### 4. First & Last

```python
window_spec = Window.partitionBy("category").orderBy("date")

result = sales.withColumn(
    "first_in_partition", first("amount").over(window_spec)
).withColumn(
    "last_in_partition", last("amount").over(window_spec)
).withColumn(
    "first_non_null", first("amount", ignoreNulls=True).over(window_spec)
)
```

## Real-World Examples

### Example 1: Deduplication (Keep Latest Record)

```python
customers = spark.createDataFrame([
    (1, "Alice", "2024-01-01", "Active"),
    (1, "Alice", "2024-01-15", "Inactive"),  # Updated
    (2, "Bob", "2024-01-05", "Active"),
    (2, "Bob", "2024-01-10", "Active")        # Duplicate
], ["cust_id", "name", "date", "status"])

window_spec = Window.partitionBy("cust_id").orderBy(col("date").desc())

result = customers.withColumn(
    "row_num", row_number().over(window_spec)
).filter(col("row_num") == 1) \
.drop("row_num")

# Output: Latest record per customer
# (1, Alice, 2024-01-15, Inactive)
# (2, Bob, 2024-01-10, Active)
```

### Example 2: SCD Type 2 (Slowly Changing Dimension)

```python
# Track all changes over time
source_data = spark.createDataFrame([
    (1, "Alice", "NYC", "2024-01-01"),
    (1, "Alice", "LA", "2024-06-01"),   # Moved to LA
    (2, "Bob", "Seattle", "2024-01-01"),
], ["cust_id", "name", "city", "change_date"])

# Current dimension table
current_dim = spark.createDataFrame([
    (1, "Alice", "NYC", 1, "2024-01-01", None),
    (2, "Bob", "Seattle", 1, "2024-01-01", None)
], ["cust_id", "name", "city", "version", "start_date", "end_date"])

# On new data (Alice moved to LA):
new_record = spark.createDataFrame([
    (1, "Alice", "LA", "2024-06-01")
], ["cust_id", "name", "city", "change_date"])

# Insert new version with new end_date
dim_update = current_dim \
    .filter(col("cust_id") == 1) \
    .withColumn("end_date", lit("2024-05-31")) \
    .withColumn("version", col("version") + 1)

new_dim_record = new_record.select(
    "cust_id", "name", "city",
    lit(2).alias("version"),
    col("change_date").alias("start_date"),
    lit(None).alias("end_date")
)

updated_dim = dim_update.union(new_dim_record)
```

### Example 3: Running Totals & Cumulative Sums

```python
daily_sales = spark.createDataFrame([
    ("2024-01-01", "Electronics", 1000),
    ("2024-01-02", "Electronics", 1500),
    ("2024-01-03", "Electronics", 1200),
    ("2024-01-01", "Books", 500),
    ("2024-01-02", "Books", 600)
], ["date", "category", "sales"])

window_spec = Window.partitionBy("category") \
    .orderBy("date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

result = daily_sales.withColumn(
    "cumulative_sales", sum("sales").over(window_spec)
).withColumn(
    "avg_to_date", avg("sales").over(window_spec)
)

# Output:
# date       | category    | sales | cumulative | avg_to_date
# 2024-01-01 | Books       | 500   | 500        | 500
# 2024-01-02 | Books       | 600   | 1100       | 550
# 2024-01-01 | Electronics | 1000  | 1000       | 1000
# 2024-01-02 | Electronics | 1500  | 2500       | 1250
# 2024-01-03 | Electronics | 1200  | 3700       | 1233
```

### Example 4: Ranking within Groups

```python
# Rank products by sales within each category
products = spark.createDataFrame([
    ("Electronics", "Laptop", 50000),
    ("Electronics", "Phone", 30000),
    ("Electronics", "Tablet", 15000),
    ("Books", "Fiction", 5000),
    ("Books", "Non-Fiction", 3000)
], ["category", "product", "sales"])

window_spec = Window.partitionBy("category").orderBy(col("sales").desc())

result = products.withColumn(
    "sales_rank", rank().over(window_spec)
).withColumn(
    "sales_percent",
    (col("sales") / sum("sales").over(Window.partitionBy("category")) * 100).cast("decimal(5,2)")
)

# Output:
# category    | product      | sales  | rank | percent
# Electronics | Laptop       | 50000  | 1    | 59.52%
# Electronics | Phone        | 30000  | 2    | 35.71%
# Electronics | Tablet       | 15000  | 3    | 17.86%
# Books       | Fiction      | 5000   | 1    | 62.50%
# Books       | Non-Fiction  | 3000   | 2    | 37.50%
```
