# 7. Partitioning & Shuffling ðŸ§©

## Partitioning Strategy

**Partitions** are logical divisions of data. Each partition is processed independently by tasks.

### Default Partitioning

```python
# Reading data determines initial partitions
df = spark.read.parquet("hdfs://path/data.parquet")

# Parquet file has 100 blocks in HDFS â†’ 100 partitions

# Reading CSV
df = spark.read.csv("data.csv")  # Usually 1 partition (slow!)

# Creating from collection
df = spark.createDataFrame([(1, "a"), (2, "b")], ["id", "name"])
# Number of partitions = sc.defaultParallelism (usually number of cores)
```

### Manual Partitioning

#### Repartition

**Repartition** changes the number of partitions (triggers shuffle):

```python
df = spark.read.parquet("data.parquet")  # 100 partitions

# Increase partitions (for more parallelism)
df_repartitioned = df.repartition(400)
# 400 new partitions, triggers full shuffle across cluster

# Specify partitioning column
df_repartitioned = df.repartition(200, "category")
# Partitions data by category, each partition contains one category

# Multiple columns
df_repartitioned = df.repartition(100, "year", "month")
```

Cost of repartition:

```python
# EXPENSIVE: Full shuffle across network
# Each executor sends data for all partition keys to other executors
# Network I/O: High
# Disk I/O: High (shuffle intermediate data)
# Time: Can be significant for large datasets

# Use case:
# - Preparing data for join (hash partitioning)
# - Writing partitioned output
# - Matching downstream task parallelism
```

#### Coalesce

**Coalesce** reduces partitions without shuffle (when possible):

```python
df = spark.read.parquet("data.parquet")  # 400 partitions

# Reduce to 100 partitions
df_coalesced = df.coalesce(100)
# No shuffle: just merges 4 partitions into 1

# This is fast because:
# - Partition 0, 1, 2, 3 â†’ New Partition 0
# - Partition 4, 5, 6, 7 â†’ New Partition 1
# - No data movement across network
```

Coalesce vs Repartition:

| Operation       | Shuffle | Use Case                                      | Performance |
| --------------- | ------- | --------------------------------------------- | ----------- |
| **coalesce**    | No      | Reduce partitions after filtering (less data) | Fast        |
| **repartition** | Yes     | Increase or reorder partitions                | Slow        |

```python
# Example: Filtering then coalescing
df = spark.read.parquet("100gb_data.parquet")  # 1000 partitions
filtered = df.filter(col("status") == "active")  # 99% filtered out

# Now only 1GB remains, reduce partitions
result = filtered.coalesce(10)  # Fast, no shuffle
```

### Partitioning Strategies

#### 1. Range Partitioning

Partition data by value ranges:

```python
# Without explicit partitioning (hash by default):
df_partitioned = df.repartition(10, "salary")  # Hash-based

# With range partitioning (more explicit):
from pyspark.sql.functions import *

ranges = df.selectExpr("percentile_approx(salary, array(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9))") \
    .collect()[0][0]

# Result: [salary values at 10th, 20th, ... 90th percentile]
# Use these as partition boundaries

# Partition by ranges:
# Partition 0: salary < range[0]
# Partition 1: range[0] <= salary < range[1]
# ...
```

#### 2. Hash Partitioning

Default: Hash the key to determine partition:

```python
# Hash partitioning (default with repartition)
df.repartition(100, "customer_id")

# How it works:
# hash(customer_id) % 100 = partition number
# "CUST_001" â†’ hash=12345 â†’ 12345 % 100 = 45 â†’ Partition 45
# "CUST_002" â†’ hash=67890 â†’ 67890 % 100 = 90 â†’ Partition 90
```

#### 3. Directory/File Partitioning

When writing data:

```python
df.write \
    .mode("overwrite") \
    .partitionBy("year", "month", "day") \
    .parquet("s3://bucket/sales/")

# Creates directory structure:
# s3://bucket/sales/
#   year=2024/
#     month=01/
#       day=01/
#         part-00000.parquet
#         part-00001.parquet
#       day=02/
#         part-00000.parquet
#   year=2023/
#     month=12/
#       ...

# Query only specific partitions (partition pruning):
df = spark.read.parquet("s3://bucket/sales/year=2024/month=01/")
# Spark only reads files from these directories
# Much faster than reading entire dataset
```

### Optimal Number of Partitions

```python
# General formula:
# Partitions = (Data size in GB) / (Target partition size in GB)
# Target partition size â‰ˆ 128MB to 256MB per task execution time

# Example: 100GB dataset
data_size = 100  # GB
target_size = 0.256  # 256MB
optimal_partitions = int(data_size / target_size)  # ~400 partitions

# Also consider:
# - Number of executors & cores
# - Memory per executor
# - Network latency

spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# spark.sql.shuffle.partitions: Default partitions after shuffle
# Default is 200, increase for large datasets
```

## Shuffling: Data Movement Across Network

**Shuffle** is the process of redistributing data across the cluster. It's one of the most expensive operations in Spark.

### When Does Shuffle Happen?

```python
# 1. GroupBy (groupByKey, agg)
df.groupBy("category").agg(sum("amount"))

# 2. Join (when not using broadcast join)
df1.join(df2, "id")

# 3. Distinct
df.distinct()

# 4. Repartition
df.repartition(100)

# 5. Order by
df.orderBy("amount")

# 6. ReduceByKey
rdd.reduceByKey(lambda a, b: a + b)
```

### Shuffle Internals: Map-Reduce Model

```
Stage 1 (MAP SIDE - Write):
  Task 1: Reads Partition 1
          - Extract key: hash(key) % num_partitions
          - Write to shuffle file
          - Creates 200 shuffle files (for 200 target partitions)

  Task 2: Reads Partition 2
          - Extract key: hash(key) % num_partitions
          - Write to shuffle file
          - Creates 200 shuffle files

  Total: 1000 tasks Ã— 200 target partitions = 200,000 shuffle files

[SHUFFLE BOUNDARY - Network I/O]

Stage 2 (REDUCE SIDE - Read):
  Task 1: Reads shuffle data for partition 0 from all tasks
          - Collects: All keys that hash to partition 0
          - Performs aggregation
          - Outputs result partition

  Task 2: Reads shuffle data for partition 1 from all tasks
          - Collects: All keys that hash to partition 1
          - Performs aggregation
          - Outputs result partition

  Total: 200 tasks, each reading data from 1000 source tasks
```

### Shuffle Write & Read

```python
# Configuration to tune shuffle
spark = SparkSession.builder \
    .config("spark.shuffle.file.buffer", "32k") \
    .config("spark.shuffle.service.enabled", "true") \
    .config("spark.shuffle.io.retries", "3") \
    .getOrCreate()

# spark.shuffle.file.buffer: Buffer size for shuffle writes
# Larger buffer = more memory used, fewer disk writes

# spark.shuffle.service.enabled: External shuffle service
# Reduces executor memory pressure, uses external service

# spark.shuffle.io.retries: Retries for failed shuffle reads
```

### Shuffle Optimization: Minimize Data Moved

```python
# INEFFICIENT: Shuffle entire dataset
df = spark.read.parquet("100gb_sales.parquet")
result = df.groupBy("customer_id").agg(sum("amount"))
# Shuffles entire 100GB

# EFFICIENT: Filter first, then shuffle
df = spark.read.parquet("100gb_sales.parquet")
result = df.filter(col("amount") > 100) \
    .groupBy("customer_id") \
    .agg(sum("amount"))
# Catalyst pushes filter before shuffle, reduces shuffled data to maybe 10GB

# EVEN MORE EFFICIENT: Partition by join key
df1 = spark.read.parquet("customers.parquet") \
    .repartition(100, "customer_id")
df2 = spark.read.parquet("orders.parquet") \
    .repartition(100, "customer_id")
result = df1.join(df2, "customer_id")
# Both already partitioned by join key, minimizes shuffle
```

### Shuffle Spilling to Disk

```python
# If shuffle data exceeds memory:
# 1. Partial shuffle results written to disk
# 2. Memory freed for next batch
# 3. During reduce phase, merge disk and memory data

# Monitor via Spark UI:
# - Stages tab: "Shuffle Write Size" and "Shuffle Read Size"
# - If much larger than memory, likely spilling
# - If executor logs show "spill", confirm

# To avoid spilling:
# 1. Increase executor memory: spark.executor.memory
# 2. Increase shuffle partition count: spark.sql.shuffle.partitions
# 3. Pre-filter data before shuffle
# 4. Use aggregate functions more efficiently
```
