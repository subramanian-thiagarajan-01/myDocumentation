# 14. File Formats üì¶

## Format Comparison

| Format      | Compression | Schema | Columnar | Speed      | Use Case                            |
| ----------- | ----------- | ------ | -------- | ---------- | ----------------------------------- |
| **Parquet** | Built-in    | Yes    | Yes      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Default for Spark, best performance |
| **ORC**     | Built-in    | Yes    | Yes      | ‚≠ê‚≠ê‚≠ê‚≠ê   | Hive compatibility                  |
| **Delta**   | Built-in    | Yes    | Yes      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ACID, Time travel                   |
| **Avro**    | Built-in    | Yes    | No       | ‚≠ê‚≠ê‚≠ê     | Schema evolution, Kafka             |
| **CSV**     | Optional    | No     | No       | ‚≠ê‚≠ê       | Import/export, human-readable       |
| **JSON**    | Optional    | No     | No       | ‚≠ê‚≠ê       | Semi-structured, APIs               |

## Parquet (Recommended)

Parquet is **columnar binary format** with excellent compression and performance.

### Parquet Benefits

1. **Column pruning**: Only read necessary columns
2. **Predicate pushdown**: Filter at storage level
3. **Compression**: Built-in snappy compression
4. **Statistics**: Min/max for pruning

```python
# Write Parquet
df.write.mode("overwrite").parquet("s3://bucket/data/")

# Read Parquet
df = spark.read.parquet("s3://bucket/data/")

# Read specific columns (column pruning)
df = spark.read.parquet("s3://bucket/data/").select("id", "name")
# Only reads id and name columns from Parquet files

# Partitioned write
df.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("s3://bucket/partitioned/")

# Reads only specific partitions (partition pruning)
df = spark.read.parquet("s3://bucket/partitioned/year=2024/month=01/")
```

### Parquet Compression Codecs

```python
# Write with specific compression
df.write \
    .mode("overwrite") \
    .option("compression", "gzip") \
    .parquet("s3://bucket/data/")

# Compression options: snappy (default), gzip, uncompressed, lz4, zstd
```

## ORC (Optimized Row Columnar)

Similar to Parquet, slightly better compression but less common in Spark.

```python
# Write ORC
df.write.mode("overwrite").orc("s3://bucket/data/")

# Read ORC
df = spark.read.orc("s3://bucket/data/")
```

## Delta Lake

**Delta** is Parquet + transaction log for ACID operations.

```python
# Write Delta
df.write.mode("overwrite").format("delta").save("s3://bucket/delta/")

# Read Delta
df = spark.read.format("delta").load("s3://bucket/delta/")

# Or using shorthand
spark.write.mode("overwrite").option("delta.columnMapping.mode", "name").saveAsTable("delta_table")

# ACID operations (merge)
df_new = spark.read.csv("new_data.csv")
df_new.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save("delta_path")

# Time travel (read past version)
df_past = spark.read.format("delta").option("versionAsOf", 5).load("delta_path")

# History
spark.sql("SELECT * FROM delta.`s3://bucket/delta/`/_delta_log")
```

## Avro (Schema Evolution)

Good for Kafka integration and schema evolution.

```python
# Write Avro
df.write.mode("overwrite").format("avro").save("s3://bucket/data/")

# Read Avro
df = spark.read.format("avro").load("s3://bucket/data/")
```

## CSV (Human-Readable)

```python
# Write CSV
df.write \
    .mode("overwrite") \
    .option("header", "true") \
    .csv("s3://bucket/data.csv")

# Read CSV
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("s3://bucket/data.csv")

# Specify data types
df = spark.read \
    .option("header", "true") \
    .schema("id INT, name STRING, amount DOUBLE") \
    .csv("s3://bucket/data.csv")
```

## JSON

```python
# Write JSON
df.write.mode("overwrite").json("s3://bucket/data/")

# Read JSON
df = spark.read.json("s3://bucket/data/")

# Multiline JSON
df = spark.read \
    .option("multiline", "true") \
    .json("s3://bucket/data.json")
```
