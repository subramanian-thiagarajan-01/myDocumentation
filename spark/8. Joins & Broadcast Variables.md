# 8. Joins & Broadcast Variables ðŸ”—

## Join Types & Strategies

### Join Type Overview

```python
customers = spark.createDataFrame([
    (1, "Alice"),
    (2, "Bob"),
    (3, "Charlie")
], ["cust_id", "name"])

orders = spark.createDataFrame([
    (1, "Order1", 1000),
    (1, "Order2", 1500),
    (2, "Order3", 2000),
    (4, "Order4", 500)  # No matching customer
], ["order_id", "cust_id", "amount"])
```

#### 1. Inner Join

Returns rows with matches in both tables:

```python
result = customers.join(orders, "cust_id", "inner")

# Output:
# cust_id | name    | order_id | amount
# 1       | Alice   | Order1   | 1000
# 1       | Alice   | Order2   | 1500
# 2       | Bob     | Order3   | 2000
# (Charlie has no orders, so not included)
```

#### 2. Left Outer Join

Returns all rows from left table, matching rows from right:

```python
result = customers.join(orders, "cust_id", "left")

# Output:
# cust_id | name    | order_id | amount
# 1       | Alice   | Order1   | 1000
# 1       | Alice   | Order2   | 1500
# 2       | Bob     | Order3   | 2000
# 3       | Charlie | null     | null    (no matching order)
```

#### 3. Right Outer Join

Returns all rows from right table, matching rows from left:

```python
result = customers.join(orders, "cust_id", "right")

# Output:
# cust_id | name    | order_id | amount
# 1       | Alice   | Order1   | 1000
# 1       | Alice   | Order2   | 1500
# 2       | Bob     | Order3   | 2000
# 4       | null    | Order4   | 500     (no matching customer)
```

#### 4. Full Outer Join

Returns all rows from both tables:

```python
result = customers.join(orders, "cust_id", "outer")

# Output:
# cust_id | name    | order_id | amount
# 1       | Alice   | Order1   | 1000
# 1       | Alice   | Order2   | 1500
# 2       | Bob     | Order3   | 2000
# 3       | Charlie | null     | null    (only in left)
# 4       | null    | Order4   | 500     (only in right)
```

#### 5. Left Anti Join

Returns rows from left table with NO match in right (opposite of semi join):

```python
result = customers.join(orders, "cust_id", "anti")

# Output:
# cust_id | name
# 3       | Charlie    (customer with no orders)
```

#### 6. Left Semi Join

Returns rows from left table WITH match in right (no right columns):

```python
result = customers.join(orders, "cust_id", "semi")

# Output:
# cust_id | name
# 1       | Alice      (has orders)
# 2       | Bob        (has orders)
```

### Join Execution Strategies

#### 1. Broadcast Hash Join

When one table is small enough to broadcast to all executors:

```python
from pyspark.sql.functions import broadcast

# Explicitly request broadcast join
result = large_table.join(
    broadcast(small_table),  # < 10GB default
    "id",
    "inner"
)

# Execution:
# 1. Small table broadcast to all executors (memory)
# 2. Large table scanned locally on each executor
# 3. Hash join performed locally (no shuffle needed)
# 4. Results collected
```

Benefits:

- **No shuffle**: Massive performance improvement
- **Network efficient**: Only broadcasts small table once
- **Memory efficient**: If small table fits in broadcast memory

Broadcast limits:

```python
spark = SparkSession.builder \
    .config("spark.sql.autoBroadcastJoinThreshold", "10485760") \
    .getOrCreate()

# 10485760 bytes = 10MB (default)
# Any table < 10MB automatically broadcast
# Increase for larger broadcast tables (be careful about executor memory)
```

#### 2. Sort-Merge Join

For large-large joins:

```python
# When both tables are large and can't broadcast
result = left_table.join(right_table, "id", "inner")

# Execution:
# 1. Both tables shuffled by join key
# 2. Both tables repartitioned to match shuffle partitions
# 3. Both tables sorted by join key
# 4. Merge join: iterate through sorted data
```

This is the default for large joins:

```python
spark = SparkSession.builder \
    .config("spark.sql.join.preferSortMergeJoin", "true") \
    .getOrCreate()
```

#### 3. Shuffle Hash Join

Rarely used (enabled for specific scenarios):

```python
spark = SparkSession.builder \
    .config("spark.sql.join.preferSortMergeJoin", "false") \
    .getOrCreate()

# Execution:
# 1. Both tables shuffled by join key
# 2. Each partition built into hashmap
# 3. Probe hashmap for matches
```

### Join Optimization Tips

```python
# Tip 1: Broadcast small dimension tables
customers = spark.read.parquet("customers.parquet")  # 500MB
orders = spark.read.parquet("orders.parquet")  # 100GB

# Automatic broadcast (if < 10MB)
result = orders.join(broadcast(customers), "cust_id")

# Tip 2: Pre-partition for frequent joins on same key
df1 = spark.read.parquet("table1.parquet") \
    .repartition(100, "join_key")
df2 = spark.read.parquet("table2.parquet") \
    .repartition(100, "join_key")

result = df1.join(df2, "join_key")  # No shuffle, direct join

# Tip 3: Filter before join to reduce data
large_table = spark.read.parquet("large.parquet")
small_table = spark.read.parquet("small.parquet")

result = large_table \
    .filter(col("status") == "active") \
    .join(small_table, "id")  # Less data to shuffle

# Tip 4: Order tables: larger on left
df1 = spark.read.parquet("100gb.parquet")
df2 = spark.read.parquet("50gb.parquet")

# Better to have larger on left (less shuffled data)
result = df1.join(df2, "id")
```

## Broadcast Variables

Broadcast variables allow you to efficiently share read-only data with executors.

### Use Case: Lookup Table

```python
# Lookup table (status codes)
status_mapping = spark.createDataFrame([
    (1, "Active"),
    (2, "Inactive"),
    (3, "Pending")
], ["status_id", "status_name"])

# Large fact table
sales = spark.read.parquet("sales.parquet")  # 100GB

# Join directly (inefficient - shuffles both tables)
result = sales.join(status_mapping, "status_id")

# Better: Broadcast lookup table
result = sales.join(
    broadcast(status_mapping),
    "status_id"
)
# Broadcasts lookup table to all executors
# No shuffle needed
```

### Explicit Broadcast Variable

```python
# Create lookup dictionary
status_dict = {1: "Active", 2: "Inactive", 3: "Pending"}

# Broadcast to all executors
broadcast_var = sc.broadcast(status_dict)

# Use in transformation
def map_status(row):
    lookup = broadcast_var.value  # Access broadcast value
    return Row(
        id=row.id,
        status=lookup.get(row.status_id, "Unknown")
    )

result = df.rdd.map(map_status).toDF()

# Inside map function:
# - broadcast_var.value: Access broadcasted dictionary
# - No serialization/deserialization for each row
# - Memory efficient: One copy per executor
```

### Broadcast Size Limits

```python
spark = SparkSession.builder \
    .config("spark.broadcast.maxOutBytes", "1073741824") \
    .getOrCreate()

# Max broadcast size: 1GB
# Broadcasts larger than this fail by default
# Increase carefully (impacts executor memory)

# Maximum number of broadcast variables
spark = SparkSession.builder \
    .config("spark.sql.broadcastTimeout", "300") \
    .getOrCreate()

# Timeout for broadcast operations: 300 seconds
```

### When to Broadcast

```python
# DO broadcast:
# - Small lookup tables (< 100MB)
# - Dimension tables in star schema
# - Configuration mappings

# DON'T broadcast:
# - Large tables (> 1GB)
# - Frequently changing data
# - Data that fits in broadcast threshold

# Rule of thumb:
# IF table_size < (num_executors * executor_memory * 0.1)
#   THEN can broadcast
```
