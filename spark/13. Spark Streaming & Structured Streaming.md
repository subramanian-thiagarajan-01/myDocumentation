# 13. Spark Streaming & Structured Streaming ðŸ”„

## Structured Streaming (Recommended)

**Structured Streaming** treats streaming data as an infinite table.

### Kafka Source Example

```python
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Read from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# Kafka data:
# key: binary
# value: binary
# topic: string
# partition: int
# offset: long
# timestamp: long
# timestampType: int

# Parse JSON from Kafka value
schema = StructType([
    StructField("user_id", IntegerType()),
    StructField("event", StringType()),
    StructField("amount", DoubleType())
])

parsed = df.select(
    col("timestamp"),
    from_json(col("value").cast("string"), schema).alias("data")
).select(
    col("timestamp"),
    col("data.*")
)

# Transform
transformed = parsed.withColumn(
    "hour", window(col("timestamp"), "1 hour")
)

# Aggregation
aggregated = transformed.groupBy("hour", "event") \
    .agg(sum("amount").alias("total_amount"))

# Write to console (for testing)
query = aggregated.writeStream \
    .format("console") \
    .option("truncate", "false") \
    .option("checkpointLocation", "hdfs://checkpoints/kafka") \
    .start()

query.awaitTermination()
```

### Watermarking

Watermarking handles late-arriving data:

```python
from pyspark.sql.functions import *

# Define watermark: Allow data 10 minutes late
streaming_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

parsed = streaming_df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Watermark: Data arriving more than 10 minutes late is dropped
watermarked = parsed \
    .withWatermark("timestamp", "10 minutes")

# Window aggregation respects watermark
result = watermarked \
    .groupBy(window(col("timestamp"), "5 minutes")) \
    .agg(count("*").alias("event_count"))

query = result.writeStream \
    .format("parquet") \
    .option("path", "hdfs://output/events") \
    .option("checkpointLocation", "hdfs://checkpoints/events") \
    .start()
```

Example:

| Event Time | Arrival Time            |     |
| ---------- | ----------------------- | --- |
| 10:01      | 10:01                   |     |
| 10:02      | 10:02                   |     |
| 10:03      | 10:20 (late by 17 min!) |     |

Your watermark is 10 min.

Window: 10:00â€“10:05  
Watermark cutoff: 10:15

So the event that arrives at 10:20 is **discarded** because:

- It is more than 10 minutes later than max event time seen
- The window has already been closed and dropped

### Stream Joins

```python
# Stream-stream join
events = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

users = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "users") \
    .load()

# Parse both
events_parsed = events.select(from_json(...).alias("data")).select("data.*")
users_parsed = users.select(from_json(...).alias("data")).select("data.*")

# Join with state (stateful operation)
joined = events_parsed.join(
    users_parsed,
    "user_id",
    "inner"
)

query = joined.writeStream \
    .format("parquet") \
    .option("checkpointLocation", "hdfs://checkpoints/joined") \
    .start()

# State size grows indefinitely - use timeouts
```
