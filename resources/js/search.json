[[{"l":"01 Unity Catalog"},{"l":"1. Unity Catalog Fundamentals"},{"l":"1.1 Definition and Purpose","p":["What It Is: Unity Catalog (UC) is Databricks' unified governance solution for data and AI assets across clouds. It provides a single place to manage access control, auditing, lineage, and data discovery.","Key Problem It Solves: Before UC, Databricks used workspace-level Hive metastore with limited governance capabilities. UC centralizes governance across multiple workspaces and clouds.","Core Properties:","Access Control: Fine-grained permissions (catalog, schema, table, column, row levels)","Auditing: Comprehensive audit logs for all data access","Lineage: Automated capture of data flow (table-to-table, column-to-column)","Data Discovery: Search and explore data assets across the organization","Quality Monitoring: Integration with Databricks Lakehouse Monitoring","Interview Question: Why Unity Catalog over Hive Metastore?","Centralized governance across workspaces","Built-in RBAC and ABAC","Automated lineage capture","Multi-cloud metadata layer","Direct compatibility with Delta Sharing"]},{"l":"2. Namespace & Object Model"},{"l":"2.1 Three-Level Namespace","p":["Structure:","Example:","Comparison with Hive Metastore:","Hive: database.table(2-level)","UC: catalog.schema.table(3-level)","Why Three Levels?","Catalog: Logical boundary for environment (prod, dev, staging)","Schema: Logical grouping of related tables (sales, marketing, finance)","Table: Actual data object"]},{"l":"2.2 Object Types","p":["Catalog","Example","Function","mask_pii()","Materialized View","ml_models_volume","mv_customer_summary","Namespace for tables/views","Non-tabular data storage","Object Type","Pre-computed view with refresh","prod_catalog","Purpose","sales_schema","Schema","Structured data (managed/external)","Table","Top-level container","transactions","User-defined functions","View","Virtual table","Volume","vw_monthly_sales"]},{"l":"2.3 Object Hierarchy","p":["Code Example:","Interview Pitfall:","Cannot skip levels: Must specify catalog.schema.table, not just catalog.table","Default catalog/schema: Can be set per session to avoid repetition"]}],[{"l":"02 Metastore, Metadata"},{"l":"3. Metastore Architecture"},{"l":"3.1 Metastore Concept","p":["What It Is: The metastore is the top-level container for Unity Catalog metadata. It stores:","Catalog definitions","Schema definitions","Table metadata","Permissions","Lineage information","Audit logs","Key Constraint: One metastore per region (not per workspace)"]},{"l":"3.2 Metastore Creation (Azure Example)","p":["Prerequisites:","ADLS Gen2 storage account","Storage container for metastore root","Access Connector for Azure Databricks (managed identity)","Creation Steps:","Metastore Assignment:","Multiple workspaces can be assigned to the same metastore","Workspaces in the same region share the metastore","Enables cross-workspace data sharing and governance","Interview Question: Can you have multiple metastores in one region?","No. One metastore per region is the design constraint.","Workspaces in the region must use that metastore or none at all."]},{"l":"3.3 Managed Storage Locations","p":["Hierarchy of Managed Storage:","Metastore-level: Default location for all catalogs","Catalog-level: Overrides metastore default for specific catalog","Schema-level: Overrides catalog default for specific schema","Example:","Interview Pitfall:","If no location specified, uses parent's managed location","Changing managed location does not migrate existing data"]},{"l":"4. Storage & Security Abstractions"},{"l":"4.1 Storage Credentials","p":["Purpose: Define authentication to cloud storage without exposing secrets in table definitions.","Types by Cloud:","Azure:","AWS:","GCP:"]},{"l":"4.2 External Locations","p":["Purpose: Map storage credentials to specific storage paths. Enables governed access to external data.","Example:","Interview Question: Storage Credential vs External Location?","Storage Credential: Authentication mechanism (HOW to authenticate)","External Location: Specific path + credential (WHERE + HOW)","External Location references a Storage Credential"]},{"l":"4.3 Access Connector (Azure-Specific)","p":["What It Is: Azure-managed identity that Databricks uses to access ADLS Gen2 without storing credentials.","Setup Flow:","Create Access Connector in Azure Portal","Grant Access Connector RBAC permissions on ADLS Gen2 (Storage Blob Data Contributor)","Create Storage Credential in UC referencing the Access Connector","Create External Location using the Storage Credential","Interview Pitfall:","Access Connector must have proper RBAC on storage account","Common error: \"403 Forbidden\" due to missing RBAC"]}],[{"l":"03 Tables, Volumes, Views and M.Views"},{"l":"5. Tables in Unity Catalog"},{"l":"5.1 Managed Tables","p":["Definition: UC manages both metadata and underlying data files. When table is dropped, both metadata and data are deleted.","Storage: Data stored in managed location (metastore/catalog/schema level).","Example:","Underlying Storage:","Interview Question: What happens when you DROP a managed table?","Metadata deleted: Table definition removed from metastore","Data deleted: Underlying files deleted from storage","Cannot recover unless you have storage-level backups"]},{"l":"5.2 External Tables","p":["Definition: UC manages metadata only. User manages underlying data files. When table is dropped, only metadata is removed.","Example:","Supported Formats:","Delta Lake(recommended)","Parquet","Apache Iceberg(Databricks Runtime 13.0+)","CSV, JSON, ORC (read-only in UC)","Interview Question: Why use External Tables?","Data shared across platforms (Databricks, Snowflake, Presto)","Data lifecycle managed independently","Compliance requirements for data retention"]},{"l":"5.3 Managed vs External Comparison","p":["Aspect","Code Example - Checking Table Type:","Data","Deletes data","DROP behavior","External Table","Keeps data","Managed location","Managed Table","Metadata","Shared data, compliance","Standard workflows","Storage location","UC controls","Use case","User controls","User-specified path"]},{"l":"6. Volumes (Non-Tabular Data)"},{"l":"6.1 Purpose","p":["What Are Volumes? Storage for non-tabular data (PDFs, images, ML models, binaries) with UC governance.","Why Volumes?","Replaces legacy DBFS mounts","Governed access via UC permissions","Lineage tracking for non-tabular files"]},{"l":"6.2 Managed Volumes","p":["Definition: UC manages lifecycle and storage. Deleting volume deletes files.","Example:","Path Structure:"]},{"l":"6.3 External Volumes","p":["Definition: Points to external storage path. Deleting volume does not delete files.","Example:"]},{"l":"6.4 Permissions on Volumes","p":["Interview Question: Volumes vs DBFS?","Volumes: Governed, UC-integrated, recommended for UC-enabled workspaces","DBFS: Legacy, no UC governance, being deprecated"]},{"l":"7. Views & Materialized Views"},{"l":"7.1 Standard Views in UC","p":["Definition: Virtual table defined by a query. No data stored; query executed each time view is accessed.","Example:","Governance:","Views respect UC permissions","User needs SELECT on underlying tables","Views can enforce row/column-level security"]},{"l":"7.2 Materialized Views","p":["Automatic Refresh (Databricks SQL Warehouses):","Cannot be directly written to (read-only from user perspective)","Dashboard backend requiring low-latency access","Definition: Pre-computed view where results are stored and incrementally refreshed.","Delta Lake backed: Results stored as Delta table","Example:","Expensive aggregations queried frequently","Incremental refresh from streaming sources","Incremental refresh: Only processes new data since last refresh","Interview Question: Materialized View vs Table?","Key Features (Databricks Runtime 12.2+):","Limitation:","Materialized View: Auto-refreshed, lineage-tracked, tied to source query","Refresh may lag behind source data","Table: Manual load, no automatic refresh, no query dependency","UC-governed: Permissions and lineage tracked","When to Use Materialized Views:"]}],[{"l":"04 Data Governance, Lineage, Observability, Lakehouse Federation"},{"l":"8. Data Governance Features"},{"l":"8.1 Object-Level Permissions","p":["All permissions","ALL PRIVILEGES","Any object","Applies To","Catalog","Common Privileges:","CREATE SCHEMA","Create schemas in catalog","CREATE TABLE","Create tables in schema","Example:","Grant Syntax:","Insert/Update/Delete data","Meaning","MODIFY","Privilege","Query data","Schema","SELECT","Table","Table/View","USE CATALOG","USE SCHEMA","View catalog, traverse to schemas","View schema, traverse to tables"]},{"l":"8.2 Row-Level Security (Dynamic Views)","p":["Mechanism: Use current_user() or is_member() functions in view definition to filter rows.","Example:","Interview Question: How does row-level security work?","View evaluates user context at query time","current_user() returns logged-in user email","is_member('group') checks group membership","Base table should be restricted; grant access to view only"]},{"l":"8.3 Column-Level Masking (UC Functions)","p":["Mechanism: Create SQL UDF that applies masking logic, use in views or enforce via policies.","Example:","Databricks Runtime 13.3+ Feature: Column-level access control via GRANT SELECT(column)(limited preview as of 2025)."]},{"l":"8.4 Audit Logging","p":["System Tables: Unity Catalog maintains audit logs in system tables.","Example Query:","Interview Question: What is logged?","All data access (SELECT, INSERT, UPDATE, DELETE)","DDL operations (CREATE, ALTER, DROP)","Permission changes (GRANT, REVOKE)","User identity, timestamp, object accessed"]},{"l":"9. Lineage & Observability"},{"l":"9.1 Automated Lineage Capture","p":["What Is Captured:","Table-to-table lineage: Which tables are used to create other tables","Column-to-column lineage: Which source columns feed into target columns","Notebook/job lineage: Which notebooks/jobs accessed data","Example:"]},{"l":"9.2 Viewing Lineage","p":["UI:","Databricks Data Explorer → Select table → Lineage tab","Visual graph showing upstream and downstream dependencies","API/SQL:"]},{"l":"9.3 Column-Level Lineage","p":["Example: If customer_summary.total_spent is derived from orders.amount, UC tracks this relationship.","Interview Question: How is lineage captured?","Databricks runtime intercepts Spark operations","Parses SQL/DataFrame operations to extract dependencies","Stores lineage metadata in Unity Catalog metastore","Limitation: May not capture lineage from external tools (e.g., dbt, Airflow) unless they use Databricks APIs"]},{"l":"10. Lakehouse Federation"},{"l":"10.1 Concept","p":["Definition: Query external data systems (Snowflake, PostgreSQL, Redshift, MySQL) directly from Databricks without copying data.","Key Feature:","Query external system using Spark SQL syntax","Data stays in source system","UC governs access via connections","Supported Systems (as of Databricks Runtime 13.3+):","Snowflake","PostgreSQL","MySQL","Amazon Redshift","Azure SQL Database","Google BigQuery"]},{"l":"10.2 Setup Example (PostgreSQL)","p":["Step 1: Create Connection","Step 2: Query External Table","Step 3: Join with Delta Table"]},{"l":"10.3 Performance Considerations","p":["Pushdown Optimization: Databricks pushes filters to external system when possible.","Interview Question: When to use Federation vs ETL?","Federation: Real-time queries, small result sets, infrequent access","ETL: Large data volumes, frequent queries, complex transformations"]}],[{"l":"05 Metrics, User Identity, Delta Sharing, Secret Management"},{"l":"11. UC Metrics (Semantic Layer - 2025 Feature)"},{"l":"11.1 Purpose","p":["What Are UC Metrics? Business-defined metrics stored in Unity Catalog. Provides consistent metric definitions across tools (Databricks SQL, BI tools, notebooks).","Key Concepts:","Metric View: Container for metrics","Measures: Aggregations (SUM, AVG, COUNT, etc.)","Dimensions: Group-by attributes","Window Functions: Time-based calculations"]},{"l":"11.2 Creating Metric Views","p":["Example:","Querying Metric View:"]},{"l":"11.3 Benefits","p":["Single source of truth: Metrics defined once, used everywhere","Governance: UC permissions apply to metrics","Lineage: Track metric dependencies","BI tool integration: Expose metrics to Tableau, Power BI","Interview Question: UC Metrics vs Materialized Views?","UC Metrics: Business logic layer, consistent definitions, BI integration","Materialized Views: Performance optimization, pre-computed results"]},{"l":"12. User & Identity Management"},{"l":"12.1 Databricks Users","p":["User Types:","Workspace users: Created directly in Databricks","Federated users: Synced from Entra ID (Azure AD), Okta, OneLogin","Recommended Approach: Use identity federation for centralized user management."]},{"l":"12.2 Entra ID Integration (Azure)","p":["Setup:","Configure SCIM provisioning in Azure Portal","Map Azure AD groups to Databricks groups","Users automatically synced to Databricks","Group-Based Access:"]},{"l":"12.3 Service Principals","p":["What Are Service Principals? Non-human identities for automation (pipelines, jobs, applications).","Example:","Interview Question: Users vs Service Principals?","Users: Human identities, interactive access","Service Principals: Machine identities, automated workflows"]},{"l":"13. Delta Sharing"},{"l":"13.1 Concept","p":["What Is Delta Sharing? Open protocol for secure data sharing. Share Delta tables with external organizations without copying data.","Key Features:","Open source protocol: Works with non-Databricks recipients","UC-governed: Permissions managed via UC","Read-only: Recipients can query but not modify data","Audit logs: Track who accessed shared data"]},{"l":"13.2 Setup Example","p":["Step 1: Create Share","Step 2: Create Recipient","Step 3: Generate Activation Link"]},{"l":"13.3 Querying Shared Data (Recipient Side)","p":["Non-Databricks Recipients: Use Delta Sharing connectors (Python, Pandas, PrestoDB, Trino).","Example (Python):","Interview Question: Delta Sharing vs API?","Delta Sharing: Standardized, governed, no custom API needed","API: Custom code, harder to govern, more maintenance"]},{"l":"14. Secret Management"},{"l":"14.1 Secret Scopes","p":["Purpose: Store sensitive information (passwords, API keys, tokens) securely. Secrets not exposed in code or notebooks.","Types:","Databricks-backed scope: Secrets stored in Databricks control plane","Azure Key Vault-backed scope: Secrets stored in Azure Key Vault (recommended for production)"]},{"l":"14.2 Azure Key Vault Integration","p":["Setup:","Create Azure Key Vault","Create secrets in Key Vault","Create Databricks secret scope linked to Key Vault","CLI Example:","Accessing Secrets in Notebook:"]},{"l":"14.3 Secret Scope Permissions","p":["Interview Question: Why not hardcode secrets?","Security risk: Secrets exposed in code, version control, logs","Audit trail: Secret access logged","Centralized management: Rotate secrets without code changes"]}],[{"l":"06 Misc"},{"l":"15. Common Interview Questions & Scenarios"},{"l":"Q1: How do you migrate from Hive Metastore to Unity Catalog?","p":["Answer:","Enable UC on workspace: Assign metastore to workspace","Create UC catalogs/schemas: Mirror existing Hive database structure","Sync external tables: Use SYNC command or manual CREATE EXTERNAL TABLE","Sync managed tables: Use DEEP CLONE or CREATE TABLE AS SELECT","Update code: Change table references from database.table to catalog.schema.table","Migrate permissions: Recreate GRANT statements in UC","Validate lineage and audit logs: Ensure governance is working","Code Example:"]},{"l":"Q2: How do you implement multi-region data sharing with UC?","p":["Answer:","Problem: One metastore per region, but need to share data across regions","Solution: Use Delta Sharing","Create share in Region A","Add tables to share","Create recipient in Region B","Recipient queries data via Delta Sharing protocol (no data replication)"]},{"l":"Q3: What happens if you drop a catalog with existing tables?","p":["Answer:","Correct Approach:","Interview Pitfall:","CASCADE deletes ALL data in managed tables","Use with extreme caution in production"]},{"l":"Q4: How do you troubleshoot \"Permission Denied\" errors in UC?","p":["Answer:","Check if user has USE CATALOG and USE SCHEMA permissions","Check if user has SELECT permission on table","For external tables, check if user has access to External Location","For external tables, check if Storage Credential has valid cloud permissions","Check audit logs to see which permission check failed","Code Example:"]},{"l":"Q5: How do you enforce data retention policies in UC?","p":["Answer:","Table-level: Use Delta table properties delta.deletedFileRetentionDuration","Governance-level: Use UC permissions to restrict DELETE/TRUNCATE","Audit-level: Monitor audit logs for data deletion","Example:"]},{"l":"16. Critical Exam Topics Summary","p":["Assuming DBFS and Volumes are interchangeable (they're not)","Common Pitfalls:","Delta Sharing: Protocol, shares, recipients","Expecting multi-region metastore (constraint: one per region)","Forgetting USE CATALOG permission when granting SELECT","High-Priority Areas:","Lakehouse Federation: Query external systems without ETL","Lineage: Automated capture, column-level lineage","Managed vs External tables: Lifecycle, DROP behavior, use cases","Materialized Views: Incremental refresh, use cases","Metastore architecture: One per region, multi-workspace assignment","Mixing up Storage Credential (auth) and External Location (path + auth)","Not understanding DROP behavior for managed vs external tables","Permissions model: GRANT/REVOKE, object hierarchy, row/column security","Storage Credentials & External Locations: How they work together","Three-level namespace: Understand catalog → schema → table hierarchy","Volumes: Purpose, managed vs external, replacing DBFS"]},{"l":"17. Hands-On Practice Exercises"},{"l":"Exercise 1: Create End-to-End UC Catalog"},{"l":"Exercise 2: Implement Column Masking"},{"l":"Exercise 3: Setup Delta Sharing","p":["End of Unity Catalog Interview Preparation Notes"]}],[{"l":"1. Databricks Platform & Core Architecture"},{"l":"1.1 Databricks Overview"},{"l":"What is Databricks?","p":["Databricks is a cloud-native Data Intelligence Platform that unifies:","Data engineering","Data warehousing (SQL analytics)","Machine learning","Governance","on top of Apache Spark + Delta Lake, delivered as a managed service.","Key characteristics:","Built around open data formats(Parquet, Delta)","Separates storage and compute","Strong focus on end-to-end lifecycle(ingestion → analytics → ML → BI)"]},{"l":"Why Databricks vs Traditional Systems"},{"l":"vs Traditional Data Warehouses","p":["Traditional DW","Databricks","Proprietary storage formats","Open formats (Delta Lake)","Schema-on-write only","Schema enforcement + evolution","Expensive scaling","Elastic, cloud-native","SQL-only","SQL + Python + Scala + ML","Interview angle:","Databricks eliminates data duplication between lake + warehouse.","Same data serves BI + ML."]},{"l":"vs Hadoop","p":["Hadoop","Databricks","Heavy ops (YARN, HDFS)","Fully managed","Batch-oriented","Batch + Streaming","On-prem focused","Cloud-native","Key interview phrase:","Databricks removed Hadoop’s operational complexity while retaining Spark’s scale."]},{"l":"vs Spark Standalone","p":["Spark standalone:","You manage clusters, configs, upgrades","No governance, no orchestration, no lineage","Databricks:","Managed Spark runtimes","Built-in jobs, monitoring, security, SQL, MLflow"]},{"l":"Databricks as a Data Intelligence Platform","p":["Interviewers increasingly expect this framing (2024+).","“Data Intelligence” = Data + AI + Governance + Cost Control","Key pillars:","Lakehouse (Delta Lake)","Photon (performance)","Unity Catalog (governance – later topic)","AI-assisted analytics (AI/BI, ML integration)"]},{"l":"1.2 Lakehouse Architecture"},{"l":"Definition","p":["A Lakehouse is an architecture that:","Uses data lakes as the single source of truth","Adds warehouse-like guarantees directly on the lake"]},{"l":"Core Principles","p":["Open storage formats","Decoupled compute","Transactional guarantees","Unified analytics + ML"]},{"l":"How Lakehouse Combines Both Worlds"},{"l":"Data Lake Traits","p":["Object storage (S3 / ADLS / GCS)","Cheap, scalable","Open formats (Parquet)"]},{"l":"Data Warehouse Traits","p":["ACID transactions","Indexing / metadata","Performance optimization","Governance","Databricks Lakehouse = Delta Lake + Optimized Spark execution"]},{"l":"Role of Delta Lake","p":["Delta Lake is non-negotiable in interviews.","Delta provides:","ACID transactions on object storage","Schema enforcement & evolution","Time travel","Efficient metadata (transaction log)","Without Delta:","Lakehouse does not exist","Spark reads are eventually consistent","No reliable incremental pipelines","Key interview soundbite:","Delta Lake is the foundation that makes the Lakehouse reliable."]},{"l":"1.3 Control Plane vs Data Plane"},{"l":"Control Plane","p":["Managed by Databricks.","Includes:","Workspace metadata","Notebooks, repos","Jobs orchestration","Cluster definitions","User & group management","No customer data stored here."]},{"l":"Data Plane","p":["Runs in customer’s cloud account.","Includes:","Compute (VMs)","Customer data","Object storage","Network traffic","Databricks deploys compute inside your VPC/VNet."]},{"l":"Security Implications (Very High Yield)","p":["Databricks cannot access your raw data","Data never leaves your cloud account","Control plane compromise ≠ data access","Interview pitfall:","Saying Databricks “hosts your data” → incorrect"]},{"l":"1.4 Compute Architecture"},{"l":"Databricks Compute Types"},{"l":"1. All-Purpose Clusters","p":["Interactive","Multi-user","Used for:","Notebooks","Ad-hoc analysis","Development","Trade-off:","More expensive","Long-running"]},{"l":"2. Job Clusters","p":["Ephemeral","One job run = one cluster lifecycle","Used for:","Production pipelines","ETL jobs","Interview expectation:","Prefer job clusters for production due to isolation + cost efficiency."]},{"l":"3. SQL Warehouses","p":["Optimized for SQL","Used by:","DBSQL","Dashboards","BI tools","Variants covered later (Serverless / Pro / Classic)."]},{"l":"Access Modes"},{"l":"Single User","p":["One user identity","Required for:","Unity Catalog write access (non-shared)","Best isolation"]},{"l":"Shared","p":["Multiple users","User isolation via Spark","Some features restricted"]},{"l":"No Isolation","p":["Legacy","Least secure","Generally discouraged","Interview note:","Unity Catalog restricts access modes intentionally."]},{"l":"Cluster Permissions","p":["Controls:","Who can attach","Who can restart","Who can terminate","Used to:","Prevent accidental cluster misuse","Enforce least privilege"]},{"l":"Cluster Policies","p":["Declarative rules:","Limit instance types","Enforce auto-termination","Control DBU cost","Common interview use case:","Prevent developers from creating oversized clusters."]},{"l":"Instance Pools","p":["Pre-warmed VMs","Faster startup","Lower cost for frequent jobs","Trade-off:","Idle pool capacity costs money"]},{"l":"1.5 Serverless Compute"},{"l":"Serverless SQL Warehouses","p":["Fully managed","Instant startup","Auto-scaling","No cluster visibility","Best for:","BI","Dashboards","Ad-hoc SQL"]},{"l":"Serverless Notebooks","p":["No cluster management","Elastic compute","Ideal for exploration","Limitations:","Less control","Not all Spark configs exposed"]},{"l":"Interview Angle","p":["Serverless = Operational simplicity over configurability"]},{"l":"1.6 Photon Engine"},{"l":"What is Photon?","p":["Vectorized execution engine","Written in C++","Replaces parts of Spark execution engine"]},{"l":"When Photon Is Used","p":["SQL workloads","DataFrame APIs","Automatic (no code change)","Not used for:","RDD-based logic","Some UDF-heavy workloads"]},{"l":"Performance Implications","p":["2–10x faster SQL","Lower DBU cost for same workload","Better CPU cache utilization","Interview trap:","Photon is not a separate cluster","Photon is not optional at runtime(auto-enabled when supported)"]},{"l":"1.7 Databricks Roles & Administration"},{"l":"Account Console","p":["Account-level management:","Users","Workspaces","Identity federation","Unity Catalog metastore"]},{"l":"Workspace vs Account Level","p":["Workspace","Account","Notebooks","Users","Jobs","Metastores","Clusters","Cloud credentials","Interview emphasis:","Governance lives at account level, not workspace level."]},{"l":"Databricks Roles & Personas","p":["Data Engineers","Analytics Engineers","Data Scientists","Platform Admins","Each persona maps to:","Different compute","Different access modes","Different tools"]},{"l":"Identity Federation Basics","p":["SSO via:","Azure AD","Okta","IAM federation","Centralized identity","Enables auditability"]},{"l":"Interview Red Flags to Avoid","p":["Confusing control plane with data plane","Treating Databricks as a proprietary warehouse","Ignoring Delta Lake’s role","Overusing all-purpose clusters in production"]}],[{"l":"3. Data Engineering & ETL (Lakeflow Era)","p":["Context shift (important for interviews) Databricks is moving from imperative Spark pipelines→ declarative, optimized-by-default pipelines. Lakeflow is the umbrella for this evolution."]},{"l":"3.1 Lakeflow (2025+)"},{"l":"What is Lakeflow?","p":["Lakeflow is Databricks’ unified ingestion + transformation + orchestration framework for data engineering.","It:","Standardizes ingestion, ETL, and orchestration","Uses declarative definitions","Automatically applies incremental processing and optimizations","Lakeflow does not replace Spark— it abstracts execution decisions."]},{"l":"What Lakeflow Replaces / Augments","p":["Older Tool","Status","Custom Spark jobs","Augmented","DLT (classic)","Evolved into SDP","Complex orchestration logic","Reduced","Hand-written incremental logic","Eliminated","Interview phrasing:","Lakeflow shifts responsibility from developers to the platform."]},{"l":"Key Characteristics","p":["Declarative pipelines","SQL-first, Python-supported","Built-in quality, lineage, retries","Tight integration with Jobs & Unity Catalog"]},{"l":"3.2 File Ingestion – Auto Loader (cloudFiles)"},{"l":"What is Auto Loader?","p":["Auto Loader is a structured streaming–based ingestion mechanism for cloud object storage.","Supported formats:","JSON","CSV","Parquet","Avro","ORC","Text","Supported storage:","AWS S3","Azure ADLS Gen2","GCP GCS"]},{"l":"Notification Mode vs Directory Listing Mode"},{"l":"Notification Mode (Preferred)","p":["Uses cloud-native notifications (SQS, Event Grid, Pub/Sub)","Near real-time","Scales to millions of files","Requirements:","Cloud permissions","Event infrastructure"]},{"l":"Directory Listing Mode","p":["Periodic file listing","Slower","Higher storage API cost","Use only when:","Notifications cannot be configured"]},{"l":"Example: Auto Loader Ingestion (PySpark)","p":["Key interview points:","Uses Structured Streaming","Schema is tracked externally","Exactly-once semantics via checkpointing"]},{"l":"3.3 Declarative ETL"},{"l":"Declarative vs Imperative","p":["Declarative = define what the table should look like, not how to compute it.","Databricks handles:","Dependency graph","Incremental recomputation","Retry logic","Backfills"]},{"l":"Supported Languages","p":["SQL (primary)","Python (secondary)","SQL-first is intentional:","Easier lineage","Easier optimization","Easier governance"]},{"l":"3.4 Data Quality (Expectations)"},{"l":"What are Expectations?","p":["Expectations are data quality rules enforced at write time.","They:","Validate incoming data","Track violations","Control failure behavior"]},{"l":"Example Expectation","p":["Violation actions:","DROP ROW","FAIL UPDATE","WARN","Interview insight:","Expectations are enforced before data is committed."]},{"l":"Observability","p":["Metrics per expectation","Failures visible in pipeline UI","Integrated with lineage"]},{"l":"3.5 Enzyme Engine (Internal)"},{"l":"What is Enzyme?","p":["Enzyme is Databricks’ internal optimization engine for incremental processing.","It:","Tracks data dependencies","Minimizes recomputation","Applies state-aware optimizations"]},{"l":"Why Enzyme Matters","p":["Without Enzyme:","Small upstream changes → full recompute","With Enzyme:","Only affected downstream data is recomputed","Interview phrasing:","Enzyme makes declarative pipelines cost-efficient at scale.","Status:","Internal","Not directly configurable","Interviewers expect conceptual understanding only"]},{"l":"3.6 Data Layout Optimization – Liquid Clustering (\uD83D\uDD25 Very High Yield)"},{"l":"What is Liquid Clustering?","p":["Liquid Clustering is a dynamic, adaptive data layout optimization for Delta tables.","It:","Eliminates static partitioning","Replaces Z-Ordering","Continuously optimizes layout"]},{"l":"Why Partitioning Is Problematic","p":["Requires choosing columns upfront","Causes data skew","Hard to evolve","Liquid Clustering:","No fixed partition columns","Adapts based on query patterns","Optimizes automatically"]},{"l":"Key Interview Statement","p":["Liquid Clustering removes the need to design partitions manually."]},{"l":"Usage Example","p":["Notes:","Column list is advisory","Databricks may change clustering internally","Works best with large tables"]},{"l":"Trade-offs","p":["Not ideal for very small tables","Background optimization cost"]},{"l":"3.7 Ingestion Patterns"},{"l":"COPY INTO vs Auto Loader","p":["Aspect","Auto Loader","Batch","Checkpoints","Complexity","COPY INTO","File tracking","Higher","Incremental","Latency","Low","Medium","Metadata","Mode","Streaming","Yes"]},{"l":"When to Use COPY INTO","p":["One-time backfills","Scheduled batch ingestion","Simple pipelines"]},{"l":"When to Use Auto Loader","p":["Continuous ingestion","Event-driven pipelines","High file arrival rate"]},{"l":"COPY INTO Example","p":["Internals:","Tracks loaded files","Idempotent","No streaming state"]},{"l":"Job Integration","p":["COPY INTO → scheduled Jobs","Auto Loader → continuous or triggered Jobs"]},{"l":"Common Interview Pitfalls (Section 3)","p":["Treating Lakeflow as a product, not a framework","Confusing Liquid Clustering with partitioning","Overusing directory listing mode","Writing manual incremental logic when declarative is available"]},{"l":"Next Section","p":["Reply “Proceed to Section 4 (Databricks SQL)” when ready."]}],[{"l":"1. Foundational Concept","p":["What is a Databricks Asset Bundle (DAB)?","A Databricks Asset Bundle is a declarative, version-controlled packaging format that enables Infrastructure-as-Code (IaC) for Databricks. It is a single deployable unit containing:","Notebook and Python code (source files)","Databricks resource definitions (jobs, pipelines, clusters, dashboards, etc.)","Environment configurations (dev, staging, prod)","Parameterized variables","Deployment metadata and identity settings","Why It Matters","Environment Portability: Deploy the same code to multiple environments with different configurations","CI/CD Integration: Automate deployments via GitHub Actions, Azure DevOps, or similar systems","Version Control: All assets tracked in Git; no manual UI-based resource creation","Reproducibility: Identical deployments across workspaces; no configuration drift","Single Source of Truth: Git becomes the authoritative state for your Databricks infrastructure"]}],[{"l":"2. The Control Plane: databricks.yml","p":["Access control (CAN_VIEW, CAN_RUN, CAN_MANAGE)","artifacts","Build instructions for Python wheels, JARs, etc.","bundle","Bundle metadata: name(required), git, deployment, cluster_id","Concept","Core Top-Level Sections","databricks.yml is the single source of truth for a bundle. It defines bundle identity, deployment targets, variables, resources, and behavior.","Deployable assets (jobs, pipelines, notebooks, clusters)","Environment definitions (dev, staging, prod)","File synchronization rules to workspace","Identity for execution (user_name or service_principal_name)","include","Minimum Valid Configuration","Parameterized values; resolved at deployment time","Path globs to include additional YAML files","permissions","Purpose","resources","run_as","Section","sync","targets","variables"]}],[{"l":"3. Bundle Targets (Environments)","p":["Allows cluster override via --cluster-id flag","Concept","Deployment Mode Behaviors","Development Mode( mode: development):","Disables deployment lock (faster iteration)","Each target can override:","Enables concurrent job runs","Enforces Git branch validation","Keeps deployment lock enabled","Marks DLT pipelines as development: true","mode(development or production)","Pauses all job schedules","presets(name_prefix, pipeline_development, jobs_max_concurrent_runs, etc.)","Production Mode( mode: production):","Requires service principal for run_as","resources(target-specific job configurations, cluster sizes, schedules)","Respects configured cluster definitions","run_as(execution identity)","Target-Level Overrides","Targets are named deployment contexts. Each target can have its own workspace, configuration, and resource overrides.","Typical Target Structure","variables(environment-specific values)","workspace(host URL, root_path for artifact storage)"]}],[{"l":"4. Variables: Runtime-Resolved Parameters","p":["Bundle variables are deployment-time parameters injected into resource definitions. They enable parameterized deployments without hardcoding environment-specific values.","Bundle variables are resolved at deployment time (when databricks bundle deploy runs)","CLI flag: --var=my_var=value","Concept","Critical Distinction: Deployment-Time vs. Runtime","default value in variable definition","Environment variable: BUNDLE_VAR_my_var=value","Job parameters and notebook widgets are resolved at runtime (when the job executes)","Once deployed, changing a variable requires re-deployment; changing parameters at run-time does NOT reflect the deployment","Sources of Variable Values(in order of precedence)","Target override in databricks.yml","Target-Specific Variable Overrides","Variable Definition","Variable Reference Syntax","variable-overrides.json file","Variables are referenced using ${var.variable_name} throughout databricks.yml:"]}],[{"l":"5. Resources: The Deployable Assets","p":["clusters","Code checked into Git, deployed to workspace","Concept","Declarative data transformations, continuous ingestion","Delta Live Tables (DLT) workflows","Each resource must have a unique programmatic key within its type:","First deployment creates the resource","Idempotency","jobs","jobs.my_job(key: my_job)","notebooks","notebooks.shared_lib(key: shared_lib)","Optional; usually referenced by job tasks","Orchestrate tasks: notebooks, Python scripts, JARs","Pareto Core Resources","pipelines","pipelines.etl_pipeline(key: etl_pipeline)","Production workflows, scheduled ETL","Purpose","Removed resources are deleted from workspace if previously deployed","Resource Declaration Structure","Resource Type","Resource Uniqueness","Resources are Databricks objects managed by the bundle. They are declarative definitions of jobs, pipelines, notebooks, clusters, etc.","Resources are idempotent:","Reusable compute definitions","Subsequent deployments update existing resource (no duplication)","The key is used internally by the CLI; the name field is what users see in Databricks UI.","Typical Use Case","Versioned notebook artifacts"]}],[{"l":"6. Jobs: Task Orchestration","p":["A single unit of work (notebook, Python script, JAR, SQL query)","Cluster Assignment","Concept","Define execution order and data passing","Definition","Example:","Global parameters available to all tasks","In the notebook:","Input values passed to a task","Job Parameters","Job Parameters Flow","Jobs define multi-task workflows in Databricks. A job orchestrates execution order, passes data between tasks, and manages retries/alerts.","Key Concepts","Task","Task Dependencies","Task Key","Task Parameters","Task runs on existing cluster or new cluster","Task Types (Pareto)","Unique identifier within a job; used for dependencies"]}],[{"l":"7. Pipelines: Delta Live Tables (DLT)","p":["⚠️ DLT pipelines do NOT use notebook widgets like jobs do. Use:","Aspect","Avoiding Widget Confusion in Pipelines","Bundle variables","Concept","Data transformation, incremental ingestion","Declarative (what state to reach)","Environment-Specific Pipeline Parameterization","Error Handling","Execution Mode","General orchestration","Imperative (how to do it)","Jobs","Key Differences from Jobs","Paradigm","Parameterization","Pipeline code accesses configuration via Spark:","Pipeline Configuration","Pipeline configuration + Spark conf","Pipeline configuration section (key-value pairs)","Pipelines (DLT)","Pipelines are declarative data transformation frameworks. Unlike jobs (imperative orchestration), pipelines define the desired state of data.","Quality checks; data validation","Secrets (via dbutils.secrets.get())","Spark session configuration","Task parameters + widgets","Task-level retries","Triggered or continuous","Use Case"]}],[{"l":"8. Notebook Packaging and Synchronization","p":["Concept","Notebooks are first-class bundle artifacts. They are checked into Git, deployed to the workspace, and versioned as part of the bundle.","Notebook Resource Definition","File Synchronization: The sync Section","By default, DAB syncs all files defined in bundle configuration to the workspace. Fine-tune synchronization:","Deployment Path","Files are deployed to a workspace path managed by the bundle:","Or with custom root_path:"]}],[{"l":"9. Parameterization Model: The Complete Flow","p":["Pareto Pattern: How Parameters Flow Through the System","Example: Dev-to-Prod Promotion","Deploy to dev:","Deploy to prod:"]}],[{"l":"10. The Include Mechanism: Modular Configuration","p":["Concept","include allows you to split databricks.yml across multiple files for modularity.","Include Syntax","Glob Patterns","Common Structure"]}],[{"l":"11. Artifacts: Python Wheels and Build Artifacts","p":["Concept","Artifacts are built dependencies packaged as part of a bundle (e.g., Python wheels, JAR files).","Python Wheel Artifacts","Build Workflow","Run databricks bundle deploy","CLI executes: poetry build","Wheel file is created locally","Wheel is uploaded to workspace","Job references the uploaded wheel","Supported Artifact Types","whl(Python wheels)","jar(Java/Scala JARs)"]}],[{"l":"12. Authentication and Identity","p":["Attended","Authentication Scenarios","Authentication Setup","Automated deployments","Best Practices","Bundle authentication determines:","CI/CD (GitHub Actions)","CI/CD (multi-workspace)","CI/CD: Use service principal with minimal permissions for the environment it accesses","Concept","Databricks CLI reads from ~/.databrickscfg(local) or environment variables:","Deploy to multiple workspaces","Developer iterating locally","Local development","Local development: Run as your user ( user_name)","Method","OAuth M2M or PAT","OAuth U2M or PAT","Or:","Production: Always run as a production service principal","Run Identity (run_as)","Scenario","Separate the identity that deploys from the identity that runs:","Service Principal","Staging: Run as a staging service principal","Type","Unattended","Use Case","Who deploys the bundle","Who runs the deployed job/pipeline (via run_as)"]}],[{"l":"13. Bundle Lifecycle Commands","p":["--auto-approve","--cluster-id id","--force","--var key=value","-t, --target target","bundle deploy","bundle destroy","bundle plan","bundle run","bundle validate","Command","Common Flag Options","Concept","Core Commands","Delete all deployed resources","Deploy resources; upload files","Deploy with Approval","Execute a job or pipeline","Flag","Idempotent; safe to re-run","List Deployed Resources","Notes","Override cluster (dev mode only)","Override Git branch validation","Override variable at deploy-time","Permanent; use carefully","Preview changes (diff)","Purpose","Run a Job","Runs deployed job; not a deployment step","Safe to run anytime","Shows create/update/delete actions","Skip confirmation prompts","Syntax check; outputs bundle identity","Target environment (dev, prod, etc.)","The Databricks CLI is the execution engine for bundle operations.","Validate Before Deploy"]}],[{"l":"14. State Management and Idempotency","p":["Concept","DAB maintains deployment state to ensure idempotent, repeatable deployments.","State File Location","Idempotency Guarantees","First deploy: Creates all resources","Nth deploy with no changes: No Databricks API calls made","Deploy after edit: Only changed resources updated","Remove from YAML, redeploy: Resource deleted from workspace","Resource Identity","A bundle resource is uniquely identified by:","Bundle name","Bundle target","Workspace host","Resource key (e.g., jobs.my_job)","Changing any of these is considered a new resource:"]}],[{"l":"15. CI/CD Integration: GitHub Actions Example","p":["Typical Workflow","Environment Promotion Pattern"]}],[{"l":"16. Opinionated Best Practices (The DAB Philosophy)","p":["Implicit Assumptions Built Into DAB","Git is Source of Truth: All infrastructure defined in version control; no UI-driven resource creation","Declarative Over Imperative: YAML configuration defines desired state; CLI executes it","Environment Parity: Same code, different configurations via targets","Reproducibility: Deployments are deterministic and repeatable across workspaces","Separation of Concerns: Deployment identity ≠ Execution identity (via run_as)","Recommended Directory Structure","Deployment Checklist","databricks bundle validate -t target passes","All variables have values (no missing required vars)","Git branch matches bundle.git.branch(prevents accidental deployments from wrong branch)","Service principal (for run_as) exists and has necessary permissions","Secrets are stored in Databricks Secret Scopes, not hardcoded","CI/CD pipeline uses service principal credentials, not user PATs","sync.exclude filters out .git, __pycache__, .venv, etc."]}],[{"l":"17. Practical Mental Model for LLMs and Humans","p":["\"Can I deploy to both workspaces from one databricks.yml?\"","\"Can I run bundle deploy from the workspace UI?\"","\"Can I use widgets in my DLT pipeline?\"","\"I changed a variable; why didn't my job use the new value?\"","\"Why does DAB delete my resource when I remove it from YAML?\"","By design. Deployment state tracks what was deployed; removing it from YAML signals deletion intent.","Common Confusion Points","Confusion","databricks.yml is the control plane: Everything flows from this file; targets, variables, and resources define behavior","If you understand these 5 things, you understand 80% of DAB:","Jobs pass parameters to widgets: Job base_parameters→ notebook dbutils.widgets.get(); DLT pipelines use configuration instead","No. DLT uses pipeline configuration and Spark session configuration, not widgets. Widgets are job/notebook-only.","Reality","Resources are idempotent: Deploy 100 times; same result each time","Targets are environment configurations: Same code, different variables and overrides per environment","Variables are deployment-time; jobs run with values from last bundle deploy. Re-deploy to pick up changes.","Variables are deployment-time: Set once during bundle deploy; changing them requires re-deployment","Yes (if bundle is in workspace Git folder). Limited support; CLI is recommended for production.","Yes. Define multiple targets with different workspace.host values; deploy to each separately."]}],[{"l":"18. Advanced Concepts (Beyond the Pareto 80/20)","p":["Listed for completeness; less critical for productivity","Dynamic Value References: {{ ERROR }}, {{ ERROR }} for context-aware job parameters","Permissions: permissions section for granular ACLs on bundle resources","Source-Linked Deployments: Dev mode option to reference workspace code instead of synced copies","Binding: databricks bundle bind to associate existing workspace resources with bundle management","Environments: Different cloud providers (AWS, Azure, GCP) supported; configuration differs slightly","MLflow Experiments & Models: Bundles can manage registered models and experiments","Dashboards & SQL Assets: Bundles support AI/BI Dashboards and SQL queries as resources","Workspace Git Folders: Deploy bundles directly from workspace Git integration (experimental)"]}],[{"l":"19. Troubleshooting and Debugging","p":["Add variables section with definition","Add workspace: host: https://... to target definition","Cause","Common Issues and Resolutions","Debugging Commands","Deployed from wrong Git branch in prod target","Fix","Git branch does not match","Grant permissions via Databricks workspace admin","Issue","Missing workspace.host in target","Notebook path in job definition is incorrect","Path does not exist","Permission denied","Resource already exists in workspace","Resource created outside bundle (UI); conflicts with deploy","Run databricks bundle bind resource or delete and redeploy","Service principal lacks permissions to resource","Use --force to override (not recommended in prod)","Variable 'var' is not defined","Variable referenced in YAML but not declared","Verify notebook exists in notebooks/ and path is relative","Workspace host is not configured"]}],[{"l":"20. Quick Reference: From Concept to Deployment","p":["Minimal DAB Project (10 minutes to first deploy)","Template-Generated databricks.yml"]}],[{"l":"21 summary dab mindset"},{"l":"Summary: The DAB Mindset","p":["Databricks Asset Bundles enforce a GitOps, infrastructure-as-code approach to Databricks. Master these layers:","Declarative YAML: Define resources once; deploy to any environment","Environment Abstraction: Same code, different configurations via targets","Parameterization: Bundle variables (deploy-time) flow to job parameters (run-time)","Idempotent Deployments: Safe, repeatable, trackable via Git","CI/CD Integration: Automate promotions dev → staging → prod","This mental model unlocks reproducible, scalable, enterprise-grade Databricks deployments."]}],[{"l":"1. Delta Lake Interview Preparation Notes"},{"l":"1. Delta Lake / Delta Format Introduction","p":["What is Delta Lake?","Open-source storage framework that brings ACID transactions to data lakes","Built on top of Parquet files with a transaction log","Not a separate storage system—it's a layer on top of existing object stores (S3, ADLS, GCS, HDFS)","Provides table abstraction over data lake files","What is Delta Format?","File format specification for storing data with ACID guarantees","Consists of:","Data files: Parquet files containing actual data","Transaction log: JSON files tracking all changes (the _delta_log directory)","Delta Table = Delta Format + APIs to read/write","Key Point for Interviews:","Delta Lake is NOT a database—it's a storage layer","No separate compute engine required—works with Spark, Presto, Trino, Flink, etc."]},{"l":"2. Data Storage Evolution","p":["ACID transactions + schema enforcement on data lakes","Be clear: Delta Lake doesn't replace data lakes—it enhances them","Cheap storage (S3, ADLS)","Combines data lake flexibility with warehouse reliability","Data Lake Era (2010-2015)","Data quality issues","Delta Lake specifically introduced by Databricks in 2019, open-sourced immediately","Delta Lake, Apache Iceberg, Apache Hudi","Difficult to maintain consistency","Expensive, limited scalability","Interview Trap:","Lakehouse Era (2019-present)","No ACID transactions","No schema enforcement","Open formats (Parquet, ORC, Avro)","Problems:","Proprietary formats","Small file problem","Still uses same underlying storage (S3/ADLS/etc.)","Store raw data (structured, semi-structured, unstructured)","Strong ACID guarantees","Structured data only","Traditional Data Warehouse (Pre-2010)"]},{"l":"3. Why Delta Lake?","p":["Auto-optimize, Z-ordering","Bad data ingestion with no guarantees","Bad data quality","Data Quality Issues","Delta Lake Solutions:","Delta Solution","DESCRIBE HISTORY command","Difficult to handle late-arriving data","Failed jobs leave partial writes","Full table scans for updates/deletes","Manual compaction needed","MERGE, UPDATE, DELETE support","No ACID","No ACID Transactions in Data Lakes","No audit","No audit trail","No data clustering","No rollback mechanism","No schema enforcement","No time travel","No updates/deletes","Operational Complexity","Performance Problems","Problem","Problems Delta Lake Solves:","Reading while writing = inconsistent data","Schema evolution without validation","Schema validation, constraints, enforcement","Small file problem (millions of tiny files)","Small files","Time travel not possible","Transaction log with optimistic concurrency","Version history in transaction log"]},{"l":"4. Features of Delta Lake"},{"l":"4.1 ACID Transactions","p":["Atomicity: All or nothing writes","Consistency: Schema validation before write","Isolation: Serializable isolation level (strongest)","Durability: Once committed, guaranteed to persist","Code Example:"]},{"l":"4.2 Schema Enforcement & Evolution","p":["Schema Enforcement (default):","Schema Evolution:","Interview Point:","mergeSchema=true adds new columns, doesn't remove or change types","Type changes require explicit ALTER TABLE"]},{"l":"4.3 Time Travel (Data Versioning)","p":["Every write creates a new version:","Interview Trap:","Versions retained based on retention period (default: 30 days)","After VACUUM, old versions unreadable","Time travel uses transaction log, not data file copies"]},{"l":"4.4 MERGE (UPSERT) Operations","p":["SQL Equivalent:"]},{"l":"4.5 DELETE and UPDATE","p":["SQL:","Interview Point:","DELETE/UPDATE not in-place—creates new Parquet files","Old files marked as removed in transaction log","Requires rewriting affected data files"]},{"l":"4.6 Unified Batch and Streaming","p":["Interview Point:","Same table accessed by batch and streaming—no separate pipelines","Streaming writes are also ACID","No \"exactly-once\" issues with proper checkpointing"]},{"l":"4.7 Automatic File Management","p":["OPTIMIZE (Compaction):","Z-ORDER (Data Clustering):","VACUUM (Cleanup):","Interview Trap:","OPTIMIZE doesn't run automatically (unless Auto-Optimize enabled)","VACUUM deletes physical files—time travel stops working for vacuumed versions","Minimum retention: 7 days (safety check to prevent breaking time travel)"]},{"l":"4.8 Audit History","p":["Output Example:"]},{"l":"4.9 Data Quality Constraints","p":["NOT NULL:","CHECK Constraints:","Code Example:","Interview Point:","Constraints enforced at write time","Constraint violations fail the entire transaction","Not all SQL constraints supported (e.g., FOREIGN KEY not supported)"]},{"l":"5. Delta Lake Architecture"},{"l":"5.1 Physical Storage Layout","p":["Directory Structure:"]},{"l":"5.2 Transaction Log (_delta_log)","p":["Core Concept:","Single source of truth for table state","Ordered sequence of JSON files","Each file = one atomic transaction","Version number = sequential commit","Transaction Log Entry Structure:","Key Actions in Log:","add: New file added","remove: File logically deleted (not physically)","metaData: Schema changes","protocol: Protocol version","commitInfo: Transaction metadata"]},{"l":"5.3 Checkpoints","p":["Automatic every 10 commits by default","Build current list of active files","Checkpoint = snapshot of entire table state at version N","Checkpoint Creation:","Checkpoint interval configurable: delta.checkpointInterval","Checkpoints don't replace JSON files—both coexist","Contains all add actions (active files) at that version","Example:","How Table State is Reconstructed:","Interview Point:","Parquet format (faster to read)","Read latest checkpoint","Reading 10,000 JSON files = slow","Replay JSON commits after checkpoint","Why Checkpoints?","Without checkpoints, reads get slower as versions grow"]},{"l":"5.4 Optimistic Concurrency Control","p":["\"Optimistic\" means: assume no conflicts, check at commit time","Append-only writes rarely conflict","Checks for conflicts","Checks if anyone committed v6 already → No","Checks if anyone committed v6 already → Yes (Writer A did)","Code Example - Conflict:","Conflict Detection:","How Concurrent Writes Work:","If conflict: Retry from latest version (v6)","If no conflict: Write as v7","Interview Trap:","NOT pessimistic locking (no table locks during write)","Retries are automatic, transparent to user","Success","Two writes conflict if they modify the same data files","UPDATE/DELETE/MERGE on same partitions = conflict","Writer A attempts commit to v6:","Writer A reads current version (v5)","Writer B attempts commit to v6:","Writer B reads current version (v5)","Writes transaction log file 00000000000000000006.json"]},{"l":"5.5 Read and Write Flow","p":["\"Active files list\" = files added but not removed in log","Atomic commit by successfully writing log file","Build active files list at that version","Build final list of active files","Check for conflicts (read latest version)","Compute file statistics (min/max, count)","Find latest checkpoint","Interview Point:","List _delta_log directory","Multiple readers can read different versions simultaneously","Read checkpoint (active files at checkpoint version)","Read Flow (Current Version):","Read Flow (Historical Version):","Read Parquet files from list","Read those Parquet files","Reads don't lock table","Replay commits after checkpoint","Replay transaction log from v0 to requested version","Schema validation","Write data as Parquet files","Write Flow:","Write transaction log JSON file"]},{"l":"5.6 Metadata and Statistics","p":["File-level Statistics (in Transaction Log):","Usage:","Data skipping: Skip files not matching query predicates","Example: SELECT * FROM table WHERE id = 500","Delta reads stats, skips files where max(id) < 500 or min(id) > 500","Interview Point:","Stats collected automatically during write","Only for first 32 columns by default","Critical for query performance—enables data skipping without reading files"]},{"l":"5.7 Data Skipping","p":["How It Works:","Query: SELECT * FROM orders WHERE date = '2024-01-15'","Delta reads transaction log stats","Skips files where:","max(date) '2024-01-15' OR","min(date) '2024-01-15'","Only reads files where '2024-01-15' falls in [min, max] range","Z-Ordering Impact:","Co-locates similar values in same files","Improves data skipping effectiveness","Example: Z-order by date→ all records for same date mostly in same file","Code Example:"]},{"l":"Key Interview Points Summary","p":["ACID via Optimistic Concurrency","Automatic retries on conflict","Checkpoints for faster reads","Common Pitfall:","Conflict detection at commit time","Constraints enforced at write time","Data skipping via file statistics","Default: strict enforcement","Delta Lake = Parquet + Transaction Log","Every commit = new version","Forgetting to OPTIMIZE → small file problem","Mark old files as removed in log","MERGE/UPDATE/DELETE Not In-Place","mergeSchema=true: allow evolution","No locks during write","Not a separate storage system","Not Z-ordering frequently filtered columns → poor data skipping","OPTIMIZE for small file compaction","Performance Features","Retained for 30 days by default","Rewrite affected files","Schema Enforcement vs Evolution","Time Travel via Versions","Transaction log in _delta_log is the single source of truth","VACUUM before retention period → breaks time travel","VACUUM deletes old data files","VACUUM deletes physical files later","Z-ordering for better clustering"]}],[{"l":"2. Delta Lake Transaction Log & ACID Implementation"},{"l":"Delta Log Overview","p":["What it is: Ordered record of every transaction ever performed on a Delta table. Stored in _delta_log/ subdirectory.","Purpose:","Single source of truth for table state","Enables ACID guarantees","Powers time travel","Coordinates concurrent readers/writers"]},{"l":"Delta Log File Structure"},{"l":"Directory Layout","p":["Key components:","JSON files: One per transaction (version), named with 20-digit zero-padded version number","CRC files: Checksum for JSON integrity validation","Checkpoint files: Aggregated state snapshots every N commits (default: 10)","_last_checkpoint: Metadata file pointing to latest checkpoint"]},{"l":"Delta Log Files Explained"},{"l":"1. JSON Transaction Files (.json)","p":["Action","Action Types:","add","Application transaction ID for idempotency","cdc","Change Data Capture records","commitInfo","Content: Single transaction's actions encoded as newline-delimited JSON.","Deletion vectors, column mapping, etc.","domainMetadata","Example: Creating a table with 2 files","File added to table","File removed from table (logical deletion)","metaData","protocol","Purpose","Reader/writer version requirements","remove","Resulting 00000000000000000000.json:","Schema, partition columns, table properties, table ID","Transaction metadata (timestamp, operation, user, params)","txn"]},{"l":"2. CRC Files (.crc)","p":["Purpose: Validate JSON file integrity.","When created: After JSON commit file is written.","Content: Checksum of the JSON file.","Example: 00000000000000000002.crc","Validation: On read, Delta recomputes JSON checksum and compares with CRC. Mismatch = corrupted transaction log.","Note: Not always present in cloud storage (S3, ADLS). Delta handles gracefully."]},{"l":"3. Checkpoint Files (.checkpoint.parquet)","p":["Purpose: Avoid replaying entire log history. Aggregate all table state up to version N.","When created: Every 10 commits by default (configurable via delta.checkpointInterval).","Structure: Parquet file containing all active add actions, latest metaData, and protocol.","Example: After 10 commits, 00000000000000000010.checkpoint.parquet created.","Checkpoint content:","All add actions for current table files","Current metaData","Current protocol","No remove, commitInfo(not needed for state reconstruction)","Multi-part checkpoints: For large tables, checkpoint split into multiple files:"]},{"l":"4. _last_checkpoint File","p":["Purpose: Quick discovery of latest checkpoint.","Content: JSON with checkpoint version and file count.","Example:","Usage: Readers start here to find checkpoint, then replay subsequent JSON commits."]},{"l":"How Delta Ensures ACID Properties"},{"l":"Atomicity","p":["Mechanism: Optimistic concurrency control + atomic file operations.","Process:","Writer reads latest version from log","Performs operation (e.g., writes new Parquet files)","Creates JSON commit file for version N+1","Attempts atomic write (PUT-if-absent in S3, conditional create in ADLS/HDFS)","If write succeeds: Transaction committed","If write fails: Another writer committed first. Retry with conflict resolution.","Key: JSON file write is atomic. Either version N exists or doesn't—no partial state.","Code Example - Concurrent Writes:","Result: Both succeed (append is commutative) or one retries. No data loss, no partial commits."]},{"l":"Consistency","p":["Mechanism: Single-version snapshot isolation.","Guarantee: Readers see complete snapshot at version N. Never see partial transaction.","Example:","Implementation:","Reader lists _delta_log/, finds latest version (or checkpoint)","Constructs file list from add actions at that version","Reads only those files","Concurrent writes to version N+1 invisible until reader explicitly refreshes"]},{"l":"Isolation","p":["Behavior: Second transaction detects conflict (overlapping predicate/partition), retries.","Behind the scenes:","Both read version N","Both succeed","Both write new Parquet files","Code Example - Conflict Detection:","Concurrent appends to disjoint partitions","Concurrent appends to non-partitioned table","Concurrent appends to same partition","Conflict types:","Conflict?","Level: Serializable (strongest isolation).","Mechanism: Write conflict detection.","No","One retries","One retries, checks for logical conflicts","Resolution","Scenario","Schema evolution + Write","Transaction 1 writes version N+1 successfully","Transaction 2 attempts N+1, fails(file exists)","Transaction 2 reads N+1, detects logical conflict, retries as version N+2","Update + Delete on same records","Yes"]},{"l":"Durability","p":["Mechanism: Write-ahead log (Delta Log) committed before data visible.","Process:","Write new Parquet files to table directory","Write JSON commit file","Only after JSON commit succeeds, transaction visible","Guarantee: If JSON commit exists, data files exist. If system crashes before JSON write, data files ignored (orphaned).","Example:","Cleanup: Orphaned files removed by VACUUM command."]},{"l":"Schema Evolution, Enforcement, and Overwrite"},{"l":"Schema Enforcement (Default Behavior)","p":["Rule: Write schema must match existing table schema. Type mismatches or missing columns rejected.","Example - Schema Enforcement Failure:","Error:","Enforcement at: Write time, before Parquet files written."]},{"l":"Schema Evolution","p":["Enable with: .option(mergeSchema, true)","Behavior: Add new columns, compatible type widening.","Allowed changes:","Add new columns (nullable by default)","Type widening (e.g., int → long, float → double)","Not allowed:","Drop columns (use ALTER TABLE DROP COLUMN)","Incompatible type changes (e.g., string → int)","Change nullability (nullable → not null)","Example - Add Column:","Output:","Delta Log Change:","Version 1 JSON includes:","Old data files: Retain old schema. Delta fills missing age with null on read."]},{"l":"Schema Overwrite Modes","p":["1. overwriteSchema = true","Usage: Replace entire schema.","Output:","Use case: Complete table restructure.","2. replaceWhere (Partial Overwrite)","Usage: Overwrite specific partitions while preserving schema and other partitions.","Delta Log: Adds remove actions for old 2023-01-02 files, add actions for new file."]},{"l":"Schema Changes in Delta Log"},{"l":"Table Property: delta.columnMapping.mode","p":["Purpose: Allow column renames, drops, type changes without rewriting data.","Modes:","none(default): Physical column names must match schema","name: Use field IDs, allow renames","id: Full column mapping with IDs","Example - Column Rename with Mapping:","Output:","Delta Log: Schema updated, but Parquet files unchanged (field ID mapping used)."]},{"l":"How Delta Operations Work Internally"},{"l":"INSERT / APPEND","p":["Process:","Write new Parquet files","Create JSON commit with add actions","No remove actions (pure append)","Example:","Delta Log Version N:","Key: isBlindAppend=true means no reads required, conflict-free."]},{"l":"UPDATE","p":["Process:","Read phase: Scan files matching WHERE clause (using stats)","Rewrite phase: Rewrite affected files with updated rows","Commit phase: Add remove for old files, add for new files","Example:","Delta Log:","Optimization: If predicate matches entire file, skipped (no rewrite). If matches no rows, skipped.","Performance:","Best case: Partition pruning + Z-ordering reduce files scanned","Worst case: Full table scan if no partition/stats pruning"]},{"l":"DELETE","p":["Two modes: Copy-on-Write(default) vs. Deletion Vectors(Delta 2.0+)."]},{"l":"Copy-on-Write DELETE","p":["Process:","Scan files matching WHERE clause","Rewrite files, excluding deleted rows","Commit remove(old) + add(rewritten)","Example:","Delta Log:","Cost: Proportional to data rewritten (even if deleting 1 row)."]},{"l":"Deletion Vectors (Preferred for Small Deletes)","p":["Binary RoaringBitmap with deleted row indices (e.g., [1] for row index 1).","cardinality: Number of deleted rows","Commit add action with deletionVector metadata—no remove","Compaction: OPTIMIZE merges DVs into rewritten files.","Copy-on-Write: Large deletes, infrequent deletes, already rewriting","dataChange=false: No new data, only metadata","Deletion Vector File( deletion_vector_ab3efd67-xxxx-xxxx-xxxx-xxxxxxxxxxxx.bin):","deletionVector fields:","Delta Log:","DVs: Small deletes (<10% of file), frequent deletes","Enable:","Example:","Generate deletion vector (RoaringBitmap) marking deleted row positions","pathOrInlineDv: DV file path or inline bitmap","Process:","Read behavior: Readers load DV, skip deleted rows.","Result: DV files removed, data files rewritten without deleted rows.","Scan files matching WHERE clause","storageType: u(UUID path) or i(inline)","What: Bitmap index marking deleted rows. No file rewrite.","When to use:","Write DV file (.bin format)"]},{"l":"MERGE (Upsert)","p":["Process: Combines UPDATE, INSERT, DELETE in single transaction.","Phases:","Match phase: Join source with target using ON condition","Action phase: Execute matched/not-matched clauses","Rewrite phase: Rewrite affected files","Commit phase: Atomic commit with mixed add/ remove actions","Example:","Output:","Delta Log:"]},{"l":"Key Interview Takeaways"},{"l":"What Interviewers Test","p":["Log replay mechanism: How readers construct table state from JSON + checkpoints","Conflict resolution: Write-write conflicts, retry logic","File-level operations: No row-level deletes in Parquet—always file rewrites (or DVs)","Checkpoint usage: Why needed, when created, how reduces read latency","Schema evolution pitfalls: mergeSchema vs. overwriteSchema, backward compatibility","Deletion Vectors: Trade-offs vs. copy-on-write, when to optimize","ACID guarantees: Atomicity via file atomicity, isolation via versioning, durability via WAL"]},{"l":"Common Pitfalls","p":["Checkpoint lag: If checkpoints disabled, readers replay 1000s of commits—slow reads","Small files problem: Frequent appends create many small files—use OPTIMIZE","DV accumulation: Many deletes create many DV files—run OPTIMIZE periodically","Schema evolution without mergeSchema: Fails silently if schema mismatch not caught","Concurrent overwrites: Two mode(overwrite) writers can conflict—use replaceWhere for partitions"]},{"l":"Quick Wins for Interviews","p":["Know JSON actions: add, remove, metaData, protocol, txn, cdc, domainMetadata","Explain checkpoint necessity: \"Avoids O(N) log replay for readers\"","Articulate DV benefits: \"Efficient small deletes, deferred file rewrites, compaction required\"","Describe conflict detection: \"Optimistic concurrency—writers check overlapping predicates/partitions on retry\"","Schema evolution rules: \"Add columns, type widening allowed; drop/incompatible changes require explicit ALTER TABLE\""]},{"l":"Additional Code: Inspecting Delta Log","p":["Output Example:"]}],[{"l":"3. Delta Lake / Delta Format Code"},{"l":"1. Creating Delta Tables"},{"l":"1.1 Basic Delta Table Creation","p":["Using SQL (CREATE TABLE)","Using PySpark DataFrame API","Using DeltaTableBuilder API"]},{"l":"1.2 Identity Columns (Auto-increment)","p":["Key Concepts:","Identity columns auto-generate unique, monotonically increasing values","Introduced in Delta Lake 2.3.0 / DBR 11.3 LTS","Guarantees: uniqueness, monotonicity (not gapless)","Cannot be updated manually","Survives MERGE, INSERT operations","SQL Syntax","Using DeltaTableBuilder","Insert Examples","Interview Points:","Identity values are NOT gapless (gaps occur after failures, MERGE operations)","Identity column cannot be part of PARTITION BY","Cannot manually INSERT into GENERATED ALWAYS columns","Identity state stored in Delta table metadata","High-water mark persists across sessions"]},{"l":"1.3 Generated/Computed Columns","p":["Key Concepts:","Values computed from expressions based on other columns","Computed at write time, stored physically (not virtual)","Immutable - cannot be updated directly","Useful for partitioning, indexing, data quality","SQL Syntax","Using DeltaTableBuilder","Insert Examples","Interview Points:","Generated columns are computed at WRITE time (not read time)","Values are physically stored in data files","Cannot violate the generation expression","Expression must be deterministic","Common use case: partition pruning with derived date columns","Schema evolution compatible - can add generated columns via ALTER TABLE"]},{"l":"2. Reading Delta Format vs Delta Table"},{"l":"2.1 Delta Table (Metastore-Registered)","p":["What It Is:","Table registered in Hive metastore or Unity Catalog","Metadata includes table name, schema, location, properties","Supports SQL DDL operations (ALTER, DROP)","SQL Read","PySpark Read"]},{"l":"2.2 Delta Format (Path-Based)","p":["What It Is:","Direct read from file path (no metastore entry)","Access underlying Delta log and data files","Useful for external tables, ad-hoc analysis","SQL Read","PySpark Read"]},{"l":"2.3 Key Differences","p":["Access","ALTER, DROP supported","Aspect","DDL Operations","Delta Format","Delta Table","Discoverability","File system permissions only","Full governance support","Limited governance","Metastore entry required","Metastore-level ACLs","Must know path","No registration needed","Not supported","Only in _delta_log","Permissions","Registration","SHOW TABLES lists it","spark.read.format(delta).load(path)","spark.table(name)","Stored in metastore","Table Properties","Unity Catalog"]},{"l":"2.4 Reading Delta Transaction Log","p":["Structure:","Reading Transaction Log Directly","Interview Points:","Delta Format = raw files + _delta_log","Delta Table = Delta Format + metastore metadata","Both use the same underlying format","Path-based reads bypass metastore, useful for data recovery","Delta Format can be read by non-Databricks Spark with Delta Lake library"]},{"l":"3. Upsert/Merge Operations (SCD Type 1 & 2)"},{"l":"3.1 MERGE Syntax Basics","p":["SQL MERGE","PySpark MERGE"]},{"l":"3.2 SCD Type 1 (Overwrite Changes)","p":["Scenario: Update customer records, overwriting old values","SQL","PySpark","Result:"]},{"l":"3.3 SCD Type 2 (Maintain History)","p":["Scenario: Track historical changes with effective dates","SQL","PySpark (Better Approach)","Result:"]},{"l":"3.4 Advanced MERGE Patterns","p":["Conditional MERGE","Delete on MERGE","Interview Points:","MERGE is atomic (all-or-nothing)","MERGE creates a single transaction version","Each record in target matched at most once (no duplicates)","Order of clauses matters: WHEN MATCHED before WHEN NOT MATCHED","Can have multiple WHEN MATCHED clauses with different conditions","MERGE INTO requires target to be Delta table","Source can be any DataFrame/Table/View"]},{"l":"4. Table Utility Commands"},{"l":"4.1 DESCRIBE Commands","p":["DESCRIBE (Basic Schema)","Output:","DESCRIBE EXTENDED (Full Metadata)","DESCRIBE DETAIL (Delta-Specific Metadata)","PySpark:","DESCRIBE DETAIL Output:"]},{"l":"4.2 HISTORY","p":["SQL:","PySpark:","Output:","Interview Points:","History stored in _delta_log transaction log","Each version is immutable","History enables time travel","History does NOT show data-level changes (use CDF for that)"]},{"l":"4.3 RESTORE","p":["Restore to Specific Version","PySpark:","What Happens:","Creates new version in transaction log","Does NOT delete data files (VACUUM needed for cleanup)","Metadata points to files from target version","Restores schema, partitioning, table properties","Example:","Interview Points:","RESTORE is metadata operation (fast)","No data duplication","Old versions remain accessible until VACUUM","Cannot restore if target version VACUUMed","RESTORE creates a new version"]},{"l":"4.4 Table Properties (TBLPROPERTIES)","p":["View Properties","Set Properties","PySpark:","Common Table Properties:"]},{"l":"4.5 VACUUM","p":["Cannot time travel beyond VACUUMed versions","Checkpoint files older than retention","Data files not referenced by any version within retention","Default 7-day retention protects time travel","Files referenced by versions within retention","Interview Points:","Latest checkpoint","Long-running queries reading old versions can fail during VACUUM","Permanently delete data files not required by recent versions","Purpose:","PySpark:","Reclaim storage space","Remove uncommitted files","Retention Periods:","Set spark.databricks.delta.retentionDurationCheck.enabled = false to override (risky)","SQL:","Transaction log JSON files","Uncommitted files (from failed writes)","VACUUM is irreversible","VACUUM runs in two phases: identify files, delete files","What Gets Deleted:","What Survives:"]},{"l":"4.6 CLONE (Shallow vs Deep)","p":["Affects clone","Aspect","Backups, disaster recovery","Clones are independent tables (separate history)","Clones can have different properties, schema evolution","Complete isolation from source","Complete isolation from source changes","Copied (independent)","Copies transaction log AND data files","Copies transaction log only","Creating table variants with different properties","Data archival","Data exploration without duplication","Data Files","Deep Clone","Deep Clone (Full Copy)","Deep Clone:","Deep clones survive source VACUUM/deletion","Disaster recovery","Fast (metadata only)","Fast and space-efficient","Full copy","Full independent copy","Fully independent","Independence","Independent table metadata","Interview Points:","Key Differences:","Migrating tables across workspaces/regions","Minimal overhead","No impact","Production backups","PySpark:","Quick environment setup (dev, staging, test)","Referenced (shared)","References same data files as source","REPLACE TABLE with CLONE syntax overwrites target","Shallow Clone","Shallow Clone (Metadata Only)","Shallow Clone:","Shallow clones become \"dangling\" if source VACUUMed","Shares files with source","Slow (copies data)","Slower and space-consuming","Source VACUUM","Speed","Storage","Testing schema changes","Testing, dev/staging","Use Cases","Use Cases:","What Happens:"]},{"l":"4.7 Change Data Feed (CDF)","p":["1. Incremental ETL","2. Audit/Compliance","3. Replication to External Systems","4. Slowly Changing Dimensions","Cannot enable CDF retroactively for past versions","CDF adds ~ 5-10% storage overhead","CDF data retained per delta.deletedFileRetentionDuration","CDF Output Schema:","CDF requires Delta Lake 2.0+","CDF Use Cases:","CDF useful for CDC, incremental processing, audit logs","Change Types:","Changes tracked at row level","delete: Row deleted","Enable CDF","insert: New row added","Interview Points:","PySpark:","Reading CDF","update_postimage: New value after update","update_preimage: Old value before update"]},{"l":"4.8 OPTIMIZE","p":["1. File Selection","2. Coalesce/Repartition","3. Transaction Commit","Adds new large files to transaction log","Atomic operation","Coalesces into fewer, larger files","Compact small files into larger files","Default targetFileSize= 128 MB","Example:","Groups files by partition","How OPTIMIZE Works (Internals):","Identifies small files (< targetFileSize)","Improve read performance","Marks old small files as removed","OPTIMIZE Configuration:","Purpose:","PySpark:","Reads small files into DataFrame","Reduce metadata overhead","SQL:","Writes new files"]},{"l":"4.9 ZORDER","p":["1. Data Arrangement","2. Column Statistics","Advanced (file-level)","Arbitrary/insertion order","Aspect","Basic (partition-level)","Choosing Z-ORDER Columns:","Clustered by Z-ORDER columns","Co-locate related data in files","Co-locates rows with similar values","Collected","Collects min/max stats per file per column","Column Stats","Column Stats Example:","Data Skipping","Default (No Z-ORDER)","Enables data skipping during reads","Enhanced for Z-ORDER columns","Example:","Fast","File Organization","Frequently filtered columns (WHERE clauses)","High-cardinality columns (department, user_id)","How Z-ORDER Works:","Improve data skipping","Improved for filtered queries","Interview Points:","Limit to 3-4 columns (diminishing returns)","Low-to-medium cardinality better than very high","Multi-dimensional clustering","Order matters: most selective column first","Purpose:","PySpark:","Query Performance","Reduce data scanning for filtered queries","Slower (Z-ORDER computation)","SQL:","Standard","Stats stored in _delta_log JSON files","Stores stats in transaction log ( add action)","Uses space-filling Z-curve algorithm","With Z-ORDER","Write Performance","Z-ORDER improves read, slows write","Z-ORDER incompatible with partitioning on same columns","Z-ORDER is a one-time operation (must re-run after new writes)","Z-ORDER leverages Delta's data skipping","Z-ORDER requires full table rewrite","Z-ORDER vs Default Behavior:"]},{"l":"4.10 Liquid Clustering","p":["1. Automatic Clustering on Write","2. Adaptive Layout","3. Z-ORDER Replacement","Aspect","Automatic incremental clustering (no manual OPTIMIZE needed)","Automatic on write","Batch-optimized tables","Better for high-volume writes","Clusters data during INSERT/MERGE/UPDATE","Dynamically adjusts clustering layout","Enable Liquid Clustering","High-volume streaming","How Liquid Clustering Works:","Incremental","Incremental clustering","Key Differences: Z-ORDER vs Liquid Clustering","Learns from query patterns","Liquid Clustering","Lower maintenance overhead","Maintenance","Manual OPTIMIZE","Manual re-run needed","Minimal impact","More efficient than manual Z-ORDER","No (full rewrite)","No manual OPTIMIZE needed","Optimizes both read and write performance","Purpose:","PySpark:","Replaces manual Z-ORDER for clustering","Self-maintaining","Self-tunes over time","Slows writes","Trigger","Use Case","Write Performance","Yes (incremental)","Z-ORDER"]}],[{"l":"1. Spark Architecture – Deep Dive \uD83D\uDE80"},{"l":"Understanding Distributed Computing in Spark","p":["Apache Spark is a distributed computing framework that processes large datasets across clusters of machines in parallel. The architecture is built on a master-slave (driver-executor) model where the work is coordinated centrally and executed distributedly."]},{"l":"1. The Driver Process","p":["The Driver is the process running your main application. It's responsible for:","Creating the SparkSession: Entry point for all Spark operations","Parsing user code: Translating PySpark/SQL code into a physical execution plan","Managing the Spark Context: Coordinating communication with executors","Scheduling tasks: Determining which tasks run on which executors","Collecting results: Pulling results back from executors for actions like collect() and show()","The driver maintains the following critical data structures:","When you call spark.read.parquet(), the driver:","Sends metadata reading requests to the cluster","Parses the schema","Plans task distribution across executors"]},{"l":"2. Executor Processes","p":["Executors are processes running on worker nodes in the cluster. Each executor:","Executes tasks: Receives serialized tasks from the driver and runs them","Manages data in memory: Stores RDD partitions, cached data, and shuffle data","Reports results back: Sends task results to the driver or writes directly to storage","Key points about executors:","Each executor has its own JVM instance","Multiple executors can run on a single node","Each executor manages a fixed amount of memory(heap size)","Executors are launched when the Spark application starts and remain alive until completion"]},{"l":"3. Cluster Manager","p":["Built-in, simple to set up, minimal configuration","Characteristics","Cloud-native deployments","Cluster Manager","Container orchestration, auto-scaling, cloud integration","Development, small clusters","Fine-grained resource sharing, multiple frameworks","Hadoop ecosystems","Job scheduling: Determining when executors are launched","Kubernetes","Mesos","Mixed workloads","Node management: Managing worker nodes in the cluster","Production, integrates with Hadoop, supports multi-tenant deployments","Resource allocation: Assigning resources (CPU, memory) to Spark executors","Spark supports multiple cluster managers:","Standalone","The Cluster Manager is responsible for:","When to Use","YARN"]},{"l":"4. Memory Hierarchy & Memory Management","p":["Spark divides memory into several categories on each executor:"]},{"l":"Execution Memory","p":["Execution Memory(also called Runtime Memory) is used for:","Shuffle operations: Storing intermediate shuffle results during joins, groupBy, etc.","Aggregations: Holding partial aggregations during reduce phases","Broadcasting: Holding broadcast variables","Default allocation: 60% of available heap(controlled by spark.memory.fraction)"]},{"l":"Storage Memory","p":["Storage Memory is used for:","Cached DataFrames: When you call df.cache() or df.persist()","RDD Partitions: When caching RDD transformations","Block storage: Temporary storage during broadcast operations","Default allocation: 40% of available heap(within the spark.memory.fraction)"]},{"l":"Memory Configuration"},{"l":"Off-Heap Memory","p":["Spark can use off-heap memory for:","Shuffle data: When configured via spark.shuffle.service.enabled","Caching: External block store"]},{"l":"5. Schedulers & Task Execution","p":["Spark uses a DAG (Directed Acyclic Graph) scheduler to manage execution:"]},{"l":"DAG Scheduler","p":["Converts transformations into a DAG of stages:","Builds the logical plan: From all transformations","Identifies stage boundaries: Stages are separated by shuffle operations","Creates tasks: Each stage produces a set of tasks","Submits to Task Scheduler: Passes stages to the task scheduler"]},{"l":"Task Scheduler","p":["The Task Scheduler(FIFO or Fair) assigns tasks to executors:","FIFO Scheduler: First-in, first-out; all tasks from the first job run first","Fair Scheduler: Round-robin between jobs; prevents one job from starving others"]},{"l":"Task Assignment & Locality","p":["The scheduler attempts to schedule tasks with data locality:","PROCESS_LOCAL: Data is in executor JVM (best)","NODE_LOCAL: Data is on same node, different executor/JVM","RACK_LOCAL: Data is on same rack, different node","ANY: Data locality not achievable"]},{"l":"6. Complete Spark Architecture Diagram"},{"l":"7. Example: Complete Request Flow"}],[{"l":"2. Core Abstractions: RDD, DataFrame & Dataset \uD83D\uDCDA"},{"l":"RDD (Resilient Distributed Dataset)","p":["RDDs are the lowest-level abstraction in Spark. They represent an immutable, distributed collection of objects that can be processed in parallel."]},{"l":"Characteristics of RDD","p":["Resilient (Fault-tolerant): If a partition is lost, Spark can recompute it using the original transformation lineage.","Immutable: Once created, RDDs cannot be changed. Transformations create new RDDs.","Distributed: Data is partitioned across multiple nodes and processed in parallel."]},{"l":"RDD Creation Methods"},{"l":"RDD Transformations vs Actions","p":["Transformations: Create new RDD from existing RDD (lazy)","Actions: Trigger actual computation and return results to driver"]},{"l":"RDD Internal Structure: Lineage","p":["Each RDD maintains a lineage(DAG) showing how it was created:","You can view the lineage:"]},{"l":"Why RDD is Less Used in Modern Data Engineering","p":["Untyped: No schema information, requires manual parsing","Less optimizable: Catalyst optimizer can't optimize RDD operations","Lower performance: Slower than DataFrames for similar operations","More verbose: Requires more boilerplate code"]},{"l":"DataFrame","p":["DataFrames are distributed collections of data organized into named columns. They're similar to relational tables or Pandas DataFrames, but distributed across a cluster."]},{"l":"DataFrame vs RDD","p":["API","Aspect","Binary format (Tungsten)","Catalyst optimizer","DataFrame","Faster","Full object serialization","Functional (map, filter)","Has schema (typed)","No","None (untyped)","Optimization","Performance","RDD","Schema","Serialization","Slower","SQL-like and functional"]},{"l":"DataFrame Schema"},{"l":"Creating DataFrames"},{"l":"DataFrame Properties"},{"l":"Dataset","p":["Datasets are type-safe versions of DataFrames (strongly-typed). They combine the benefits of RDDs (type safety) and DataFrames (optimization)."]},{"l":"Important: Dataset is Scala/Java Only","p":["Datasets are NOT available in PySpark. In PySpark, you work with DataFrames exclusively."]},{"l":"Why Not in PySpark?","p":["Python is dynamically typed","Type safety is a Scala/Java feature","PySpark DataFrames handle most use cases"]},{"l":"Scala/Java Datasets (for reference)","p":["In Scala:"]}],[{"l":"3. Spark SQL & Catalyst Optimizer ⚡"},{"l":"Spark SQL & Catalyst Optimizer"},{"l":"Spark SQL Architecture","p":["Spark SQL is a module that allows you to:","Query structured data using SQL","Create temporary views and databases","Leverage the Catalyst optimizer for performance"]},{"l":"Unified API: SQL vs DataFrame API","p":["Both SQL and DataFrame API access the same Catalyst optimizer:"]},{"l":"Registering Temporary Views"},{"l":"Working with Databases"},{"l":"Catalyst Optimizer: Deep Dive","p":["The Catalyst Optimizer is the heart of Spark's performance. It transforms logical plans into optimized physical plans."]},{"l":"Optimization Pipeline"},{"l":"Optimization Rules Applied By Catalyst"},{"l":"1. Predicate Pushdown","p":["Move filter operations as early as possible (before joins):"]},{"l":"2. Column Pruning","p":["Only read necessary columns:"]},{"l":"3. Constant Folding","p":["Evaluate constant expressions at compile time:"]},{"l":"4. Dead Code Elimination","p":["Remove unused expressions:"]},{"l":"5. Boolean Simplification","p":["Simplify boolean expressions:"]},{"l":"6. Join Reordering","p":["Reorder joins for optimal execution:"]},{"l":"Viewing the Execution Plan"},{"l":"Custom Optimization Rules"}],[{"l":"4. Transformations & Actions \uD83D\uDD01"},{"l":"Transformations & Actions"},{"l":"Transformations: Creating New DataFrames","p":["Transformations are lazy operations that don't execute immediately. They build a computation plan."]},{"l":"Common Transformations"},{"l":"1. Select & SelectExpr"},{"l":"2. Filter/Where"},{"l":"3. WithColumn & WithColumnRenamed"},{"l":"4. Join"},{"l":"5. GroupBy & Aggregations"},{"l":"6. Distinct & DropDuplicates"},{"l":"7. Union & UnionByName"},{"l":"8. Explode"},{"l":"9. Pivot"},{"l":"10. Melt/Unpivot"},{"l":"11. OrderBy & Sort"},{"l":"12. Limit & Offset"},{"l":"Complex Transformation Example"},{"l":"Actions: Triggering Computation","p":["Actions execute the computation and return results to the driver or write to storage."]},{"l":"Common Actions"},{"l":"1. Show"},{"l":"2. Count"},{"l":"3. Collect"}],[{"l":"5. Lazy Evaluation & Execution Model \uD83E\uDDE0"},{"l":"Understanding Lazy Evaluation","p":["Spark uses lazy evaluation, meaning transformations are not executed immediately. Instead, Spark builds a Directed Acyclic Graph (DAG) of transformations and only executes when an action is called."]},{"l":"Why Lazy Evaluation?","p":["Optimization: Spark can see the full computation plan and optimize globally","Efficient Resource Use: Unnecessary computations are avoided","Chain Operations: Combine multiple transformations efficiently"]},{"l":"Example: Lazy Evaluation in Action"},{"l":"Viewing the DAG"},{"l":"Breakdown of Explain Output","p":["The physical plan shows:","FileScan: Read the parquet file","Filter: Apply WHERE clause","HashAggregate (partial): Per-partition aggregation","Exchange: Shuffle data (brings together same keys)","HashAggregate (final): Final aggregation after shuffle","The (1), (2) indicate stage boundaries."]},{"l":"Execution Model: Stages & Tasks"},{"l":"Stage Creation","p":["Spark divides execution into stages at shuffle boundaries. Each stage contains tasks that can run in parallel without shuffling."]},{"l":"Task Distribution"},{"l":"Task Scheduling & Locality","p":["The scheduler attempts data locality:"]},{"l":"Task Retries","p":["Spark automatically retries failed tasks:"]},{"l":"Complete Execution Flow Example"}],[{"l":"6. Tungsten Engine & Memory Management ⚙️"},{"l":"Tungsten: Project Tungsten","p":["Project Tungsten is Spark's initiative for optimizing physical execution. It focuses on memory management, CPU efficiency, and code generation."]},{"l":"Key Components of Tungsten"},{"l":"1. Memory Management","p":["Tungsten introduced off-heap memory management using sun.misc.Unsafe(or equivalent):","Problem with Java Objects: Each Java object has overhead (header, pointers, alignment)","A simple (id: Long, name: String) takes 50+ bytes","Tungsten Solution: Store data in binary format with minimal overhead","Same (id: Long, name: String) takes ~ 25-30 bytes","40-50% memory savings"]},{"l":"2. Unsafe Memory Operations","p":["Tungsten uses direct memory access:"]},{"l":"3. Code Generation","p":["Tungsten generates custom Java bytecode for specific operations:","The benefits:","Eliminates interpreter overhead: Direct CPU execution","Vectorization: Process multiple rows simultaneously","CPU cache efficiency: Better data locality"]},{"l":"Memory Organization on Executor"},{"l":"Memory Spillover","p":["When memory is exhausted, Spark spills data to disk:"]},{"l":"Monitoring Memory Usage"}],[{"l":"7. Partitioning & Shuffling \uD83E\uDDE9"},{"l":"Partitioning Strategy","p":["Partitions are logical divisions of data. Each partition is processed independently by tasks."]},{"l":"Default Partitioning"},{"l":"Manual Partitioning"},{"l":"Repartition","p":["Repartition changes the number of partitions (triggers shuffle):","Cost of repartition:"]},{"l":"Coalesce","p":["Coalesce reduces partitions without shuffle (when possible):","Coalesce vs Repartition:","Operation","Shuffle","Use Case","Performance","No","Reduce partitions after filtering (less data)","Fast","repartition","Yes","Increase or reorder partitions","Slow"]},{"l":"Partitioning Strategies"},{"l":"1. Range Partitioning","p":["Partition data by value ranges:"]},{"l":"2. Hash Partitioning","p":["Default: Hash the key to determine partition:"]},{"l":"3. Directory/File Partitioning","p":["When writing data:"]},{"l":"Optimal Number of Partitions"},{"l":"Shuffling: Data Movement Across Network","p":["Shuffle is the process of redistributing data across the cluster. It's one of the most expensive operations in Spark."]},{"l":"When Does Shuffle Happen?"},{"l":"Shuffle Internals: Map-Reduce Model"},{"l":"Shuffle Write & Read"},{"l":"Shuffle Optimization: Minimize Data Moved"},{"l":"Shuffle Spilling to Disk"}],[{"l":"8. Joins & Broadcast Variables \uD83D\uDD17"},{"l":"Join Types & Strategies"},{"l":"Join Type Overview"},{"l":"1. Inner Join","p":["Returns rows with matches in both tables:"]},{"l":"2. Left Outer Join","p":["Returns all rows from left table, matching rows from right:"]},{"l":"3. Right Outer Join","p":["Returns all rows from right table, matching rows from left:"]},{"l":"4. Full Outer Join","p":["Returns all rows from both tables:"]},{"l":"5. Left Anti Join","p":["Returns rows from left table with NO match in right (opposite of semi join):"]},{"l":"6. Left Semi Join","p":["Returns rows from left table WITH match in right (no right columns):"]},{"l":"Join Execution Strategies"},{"l":"1. Broadcast Hash Join","p":["When one table is small enough to broadcast to all executors:","Benefits:","No shuffle: Massive performance improvement","Network efficient: Only broadcasts small table once","Memory efficient: If small table fits in broadcast memory","Broadcast limits:"]},{"l":"2. Sort-Merge Join","p":["For large-large joins:","This is the default for large joins:"]},{"l":"3. Shuffle Hash Join","p":["Rarely used (enabled for specific scenarios):"]},{"l":"Join Optimization Tips"},{"l":"Broadcast Variables","p":["Broadcast variables allow you to efficiently share read-only data with executors."]},{"l":"Use Case: Lookup Table"},{"l":"Explicit Broadcast Variable"},{"l":"Broadcast Size Limits"},{"l":"When to Broadcast"}],[{"l":"9. Window Functions \uD83E\uDE9F","p":["Window functions compute values over a specified \"window\" (range) of rows while preserving individual rows. Essential for analytics and time-series operations."]},{"l":"Window Function Components"},{"l":"1. Partition Clause","p":["Divides rows into groups:"]},{"l":"2. Order Clause","p":["Orders rows within each partition:"]},{"l":"3. Frame Clause","p":["Specifies which rows to include in the window:"]},{"l":"Common Window Functions"},{"l":"1. Row Number / Rank / Dense Rank"},{"l":"2. Aggregation Functions"},{"l":"3. Lead & Lag","p":["Access previous/next row values:"]},{"l":"4. First & Last"},{"l":"Real-World Examples"},{"l":"Example 1: Deduplication (Keep Latest Record)"},{"l":"Example 2: SCD Type 2 (Slowly Changing Dimension)"},{"l":"Example 3: Running Totals & Cumulative Sums"},{"l":"Example 4: Ranking within Groups"}],[{"l":"10. UDF & Pandas UDF \uD83D\uDC0D"},{"l":"UDF (User-Defined Functions)","p":["UDFs allow you to create custom functions that execute on every row."]},{"l":"Row-by-Row UDF"},{"l":"UDF Performance Issues","p":["UDFs are slow because:","Serialization overhead: Data serialized to Python","Row-by-row execution: No vectorization","Python GIL: Single-threaded Python execution"]},{"l":"UDF Use Cases","p":["Only use UDFs when:","Complex business logic not available in SQL","External API calls (rate conversion API, ML model)","Legacy code integration"]},{"l":"UDF With Complex Return Type"},{"l":"Pandas UDF (Vectorized UDF)","p":["Pandas UDF is a high-performance alternative that processes batches using Apache Arrow."]},{"l":"Benefits of Pandas UDF","p":["Vectorized execution: Process entire batch at once","Apache Arrow: Efficient data transfer between JVM and Python","10-100x faster than row-by-row UDF"]},{"l":"Creating Pandas UDF"},{"l":"Pandas UDF for Aggregation"},{"l":"Pandas UDF for Grouped Transform"},{"l":"Pandas UDF with ML Models"},{"l":"Performance Comparison"}],[{"l":"11. Caching & Persistence \uD83D\uDCBE","p":["Caching stores DataFrame/RDD in memory to speed up repeated access."]},{"l":"Cache vs Persist"},{"l":"Storage Levels"},{"l":"When to Cache"},{"l":"Monitoring Cache"}],[{"l":"12. Checkpointing ⛓️","p":["Checkpointing breaks the lineage and saves the current state to stable storage (HDFS, S3, etc.)."]},{"l":"Use Cases","p":["Long lineage: Break expensive computation chains","Iterative algorithms: ML algorithms with many iterations","Streaming: Stable state for recovery"]},{"l":"Difference from Caching","p":["Aspect","Cache","Checkpoint","Storage","Memory (default)","Disk (HDFS, S3)","Lineage","Preserved","Broken","Recovery","Lost if executor fails","Survives executor failure","Use","Short-term performance","Long-term stability"]},{"l":"Checkpointing Example"},{"l":"Eager vs Lazy Checkpoint"}],[{"l":"13. Spark Streaming & Structured Streaming \uD83D\uDD04"},{"l":"Structured Streaming (Recommended)","p":["Structured Streaming treats streaming data as an infinite table."]},{"l":"Kafka Source Example"},{"l":"Watermarking","p":["Watermarking handles late-arriving data:","Example:","Event Time","Arrival Time","10:01","10:02","10:03","10:20 (late by 17 min!)","Your watermark is 10 min.","Window: 10:00–10:05 Watermark cutoff: 10:15","So the event that arrives at 10:20 is discarded because:","It is more than 10 minutes later than max event time seen","The window has already been closed and dropped"]},{"l":"Stream Joins"}],[{"l":"14. File Formats \uD83D\uDCE6"},{"l":"Format Comparison","p":["⭐⭐","⭐⭐⭐","⭐⭐⭐⭐","⭐⭐⭐⭐⭐","ACID, Time travel","Avro","Built-in","Columnar","Compression","CSV","Default for Spark, best performance","Delta","Format","Hive compatibility","Import/export, human-readable","JSON","No","Optional","ORC","Parquet","Schema","Schema evolution, Kafka","Semi-structured, APIs","Speed","Use Case","Yes"]},{"l":"Parquet (Recommended)","p":["Parquet is columnar binary format with excellent compression and performance."]},{"l":"Parquet Benefits","p":["Column pruning: Only read necessary columns","Predicate pushdown: Filter at storage level","Compression: Built-in snappy compression","Statistics: Min/max for pruning"]},{"l":"Parquet Compression Codecs"},{"l":"ORC (Optimized Row Columnar)","p":["Similar to Parquet, slightly better compression but less common in Spark."]},{"l":"Delta Lake","p":["Delta is Parquet + transaction log for ACID operations."]},{"l":"Avro (Schema Evolution)","p":["Good for Kafka integration and schema evolution."]},{"l":"CSV (Human-Readable)"},{"l":"JSON"}],[{"l":"15. SparkSession & Configuration ⚙️"},{"l":"SparkSession: Entry Point"},{"l":"Critical Configurations","p":["% of heap for Spark","0.5","0.6","1","1g","200","Adaptive query optimization","Config","CPU cores per executor","Default","false","Heap memory for driver","Heap memory per executor","Impact","Number of executors","Partitions after shuffle","spark.driver.memory","spark.executor.cores","spark.executor.instances","spark.executor.memory","spark.memory.fraction","spark.memory.storageFraction","spark.sql.adaptive.enabled","spark.sql.shuffle.partitions","Storage vs execution memory","varies"]}],[{"l":"16. Cluster Managers \uD83D\uDDA7"},{"l":"YARN (Hadoop Cluster Manager)"},{"l":"YARN Resource Management"},{"l":"Kubernetes (K8s)"},{"l":"Standalone"}],[{"l":"17. Adaptive Query Execution (AQE) ♻️","p":["AQE dynamically optimizes queries at runtime based on actual data statistics."]},{"l":"AQE Features"},{"l":"1. Dynamic Coalescing","p":["Reduces shuffle partitions based on actual data size:"]},{"l":"2. Skew Join Optimization","p":["Detects and handles skewed data:"]},{"l":"3. Join Strategy Adaptation","p":["Changes join type based on runtime statistics:"]}],[{"l":"18. Delta Lake Integration \uD83C\uDFF7️","p":["Delta Lake brings ACID properties and time travel to Spark."]}],[{"l":"19. PySpark & SQL Common Interview Scenarios \uD83E\uDDEA"},{"l":"Scenario 1: ETL Pipeline in Spark","p":["Problem: Build an ETL pipeline that:","Reads raw sales data from CSV","Cleans and transforms data","Joins with product information","Aggregates by category","Writes to Parquet","Solution:"]},{"l":"Scenario 2: Incremental Load (CDC)","p":["Problem: Load only new/changed records since last load.","Solution:"]},{"l":"Scenario 3: SCD Type 2 Implementation","p":["Problem: Maintain historical changes in a dimension table.","Solution:"]},{"l":"Scenario 4: Handling Skew","p":["Problem: Some keys have disproportionately more data (skew).","Solution:"]},{"l":"Scenario 5: Large File Processing","p":["Problem: Process 100GB+ file efficiently.","Solution:"]},{"l":"Scenario 6: Complex Window Function","p":["Problem: Rank products by sales within category, keep top 3, calculate rank% of category total.","Solution:"]},{"l":"Scenario 7: Data Quality Checks","p":["Problem: Validate data before processing.","Solution:"]},{"l":"Scenario 8: Optimized Join","p":["Problem: Join 10GB table with 5GB table efficiently.","Solution:"]},{"l":"Conclusion","p":["This comprehensive guide covers:","Architecture: Driver, executors, cluster managers, memory, schedulers","Core Abstractions: RDD, DataFrame, Dataset differences","Optimization: Catalyst, Tungsten, code generation","Operations: Transformations, actions, lazy evaluation","Performance: Partitioning, shuffling, joins, broadcasting","Analytics: Window functions, aggregations, deduplication","Advanced: UDF, Pandas UDF, caching, checkpointing","Streaming: Structured Streaming, watermarking, joins","Storage: Parquet, ORC, Delta, CSV, JSON","Real-World Scenarios: ETL, CDC, SCD Type 2, skew handling, data quality","Master these concepts to become a strong Spark engineer capable of building scalable, performant data pipelines."]}],[{"l":"1. Introduction"},{"l":"1. Core Concepts"},{"l":"1.1 Declarative vs Imperative Paradigm","p":["Imperative (traditional Spark jobs):","Declarative (DLT):","Key difference: DLT infers dependency graph and handles orchestration. You declare what, not how."]},{"l":"1.2 Streaming-First Design","p":["DLT is built on Spark Structured Streaming fundamentals, enabling:","Exactly-once semantics via checkpointing","Incremental processing of append-only data","Unified batch/stream execution model","Critical insight: Streaming tables process each record once (assuming append-only source). Materialized views compute full state on each trigger."]},{"l":"1.3 Pipeline Lifecycle","p":["Create Update: DLT analyzes code, discovers tables/views, builds DAG","Initialize: First run processes all historical data (full refresh)","Incremental Updates: Subsequent runs process only new/changed data","State Persistence: Checkpoints saved at storage_location/system/checkpoints/{table_name}","Not officially guaranteed: Exactly how DLT decides when to do full vs incremental refresh for materialized views depends on cost-based optimizer (varies by version)."]},{"l":"1.4 Medallion Architecture in DLT","p":["Bronze → Silver → Gold pattern fits naturally:","Bronze: Raw ingestion via streaming tables with minimal validation","Silver: Cleaned, deduplicated tables with Expectations; mix of streaming tables and MVs","Gold: Aggregated/curated tables; typically MVs for performance"]}],[{"l":"2. Delta Live Tables Basics"},{"l":"2.1 Table vs View Distinction","p":["@dlt.table or @dlt.create_table","@dlt.view or @dlt.create_view","Aspect","Cached; fast queries","Data storage, sharing","Intermediate transformations","Interview note: Views are cheaper because they don't persist; use them for intermediate transformations.","Live Table","Live View","Logical view (no storage)","Materialization","Performance","Persistent Delta table","Recomputed on each reference","Syntax","Typical use"]},{"l":"2.2 Streaming Table vs Materialized View","p":["Aspect","Cost","Full recompute (default) or incremental (serverless)","High (deduplicate if replayed)","Higher for large aggregations (unless serverless)","Higher; depends on refresh schedule","Idempotency requirement","Incremental (only new data)","Key insight: Streaming tables ≠ always running. They're triggered (continuous mode or scheduled); they maintain checkpoints to track processed data.","Latency","Low; continuous or frequent triggers","Lower (deterministic recompute)","Lower for high-volume ingestion","Materialized View (LIVE TABLE)","Processing model","Schema changes","Schema evolution via DDL supported","State management","Stateful; maintains checkpoints","Stateless (batch recompute)","Streaming source must append-only","Streaming Table"]},{"l":"2.3 Managed vs Unmanaged Tables","p":["Managed (default): DLT owns storage location; dropped when table dropped","Unmanaged: You specify location; survives table drop","Interview note: Unity Catalog integration prefers managed tables for governance."]},{"l":"2.4 Schema Inference and Evolution","p":["DLT auto-infers schema on first run (if not explicitly provided). Evolution:","Additive: New columns appended to schema (safe)","Restrictive: Column removal or type change requires manual intervention","Auto Loader schema evolution modes:","ADDITIVE(default): Accept new columns","FAILONCOLUMNJROPOUT: Fail if column removed","RESCUE: Unknown fields → _rescue_data JSON column"]},{"l":"2.5 Table Dependencies and Lineage","p":["DLT parses all dlt.read() calls to build dependency graph. Circular references cause CircularDependencyError.","Lineage tracking: DLT UI shows data flow. Event log contains flow_definition entries with input/output relationships."]}],[{"l":"3. Pipeline Configuration"},{"l":"3.1 JSON Configuration Structure","p":["Key fields:","storage: Root path for output tables + checkpoints + event logs","configuration: Spark configs (applied to all clusters)","continuous vs development vs production modes (mutually exclusive scheduling)"]},{"l":"3.2 Development vs Production Mode","p":["Aspect","Cluster persistence","Cost","Dashboard accessible","Development","Full retry logic","Higher (dedicated clusters)","Interview note: Development mode speeds iteration; always validate in production mode before deploying.","Limited (faster feedback)","Lower (shared clusters)","Manual or scheduled","Observability","Persists between updates","Production","Retry behavior","Same as dev","Scheduled only","Terminated after update","Trigger"]},{"l":"3.3 Continuous vs Triggered Execution","p":["Continuous: Cluster always running, processes data as it arrives","Triggered: Cluster starts only on schedule or manual trigger","Cost/latency tradeoff: Continuous = lower latency but constant DBU burn."]},{"l":"3.4 Autoscaling and Photon","p":["Photon: Vectorized query execution engine","2-4x speedup for ETL workloads","Higher cost (~ 2x DBU multiplier)","Best for aggregations, joins; not I/O-bound operations","When NOT to use Photon: Data extraction, lightweight ingestion, single-node Python code."]}],[{"l":"4. Data Ingestion"},{"l":"4.1 Auto Loader (cloudFiles)","p":["Auto Loader is the streaming source for cloud storage with built-in:","Incremental file discovery","Schema inference/evolution","Exactly-once delivery","Rescue column for unexpected data","Schema inference: Samples first 50GB or 1000 files (whichever limit crossed first). Stores inferred schema in _schemas/ directory.","Schema evolution:"]},{"l":"4.2 Streaming vs Batch Ingestion Trade-offs","p":["Aspect","Batch","Best for","Checkpoint-based; may require manual cleanup","Checkpoints required","Cost","Failure recovery","High (hours to days)","Historical loads, one-time migrations","Interview insight: Streaming ingestion is stateful; batch is stateless. State corruption is the #1 streaming failure mode.","Latency","Low (seconds to minutes)","Not needed","Predictable (full scans)","Real-time data, continuous sources","Simple; just rerun","State management","Streaming","Variable (per-micro-batch overhead)"]},{"l":"4.3 Handling Late and Out-of-Order Data","p":["Streaming tables without watermarks process all data; with watermarking:","Watermarks only affect stateful operations(windows, joins). For append-only ingestion, watermarks have no effect."]}],[{"l":"5. Transformations"},{"l":"5.1 SQL vs Python DLT Pipelines","p":["SQL Pipeline (notebook .sql file):","Python Pipeline:","Key difference: Python allows for complex logic, loops, conditionals; SQL is pure declarative."]},{"l":"5.2 Incremental Transformations","p":["For stateless operations (filters, selects), all table types are inherently incremental. For stateful operations (aggregations, joins), only streaming tables guarantee true incremental processing on classic compute.","Serverless compute(as of 2025): Materialized views get incremental refresh via cost-based optimizer."]},{"l":"5.3 Aggregations and Joins","p":["Aggregations in streaming tables:","Joins: Streaming table + streaming table requires watermarks. Streaming + static is safe."]}],[{"l":"6. Streaming Concepts in DLT"},{"l":"6.1 Structured Streaming Fundamentals","p":["Spark Structured Streaming models streams as unbounded tables with micro-batches:","Each micro-batch executes as a mini-Spark job","Fault tolerance via Write-Ahead Log (WAL) and checkpoints","Exactly-once semantics for deterministic sinks (Delta, databases with idempotent writes)","DLT abstracts: You declare a streaming table; DLT manages checkpoints, triggers, micro-batch size."]},{"l":"6.2 Event-Time vs Processing-Time vs Ingestion-Time","p":["Concept","Definition","Example","Event-time","When event occurred (in data)","order_timestamp in JSON","Processing-time","When Spark processes event","Micro-batch execution time","Ingestion-time","When file written to storage","File modification time","DLT does not expose processing-time directly; you work with event-time via withWatermark()."]},{"l":"6.3 Exactly-Once Semantics in DLT","p":["Enabled by:","Idempotent producer: Auto Loader deduplicates based on file path + offset","Transactional sink: Delta Lake ACID guarantees","Checkpointing: Track processed offsets; resume from last checkpoint","Critical caveat: Exactly-once is per-partition for Kafka/message queues. DLT + Auto Loader guarantees exactly-once per file."]}],[{"l":"7. Data Quality & Expectations"},{"l":"7.1 Expectations (EXPECT, EXPECT_OR_DROP, EXPECT_OR_FAIL)","p":["EXPECT(default): Log violations; write valid + invalid records","EXPECT_OR_DROP: Remove violating records","EXPECT_OR_FAIL: Fail entire pipeline on violation"]},{"l":"7.2 Advanced: Group Multiple Expectations","p":["Use expect_all_or_drop, expect_all_or_fail for grouped actions."]},{"l":"7.3 Metrics & Monitoring","p":["Event log stores expectation results in details:flow_progress.data_quality.expectations:"]},{"l":"7.4 Quarantine Pattern (Advanced)"}],[{"l":"8. Change Data Capture (CDC)"},{"l":"8.1 APPLY CHANGES INTO (SCD Type 1 & 2)"},{"l":"8.2 Handling Deletes and Truncates","p":["Important: apply_as_deletes applies at row level; apply_as_truncates clears entire table."]},{"l":"8.3 Out-of-Sequence Record Handling","p":["DLT automatically deduplicates out-of-order updates using sequence_by column:","Non-monotonic sequences: If sequence_by values jump backward (clock skew, late-arriving updates), DLT ignores the out-of-sequence record."]}],[{"l":"9. Delta Lake Integration"},{"l":"9.1 ACID Transactions & Time Travel","p":["DLT writes to Delta tables, inheriting ACID guarantees:","Atomicity: Entire table version succeeds or fails","Consistency: No partial updates visible","Isolation: Snapshot isolation; concurrent readers see consistent snapshot","Durability: Transaction committed to storage","Time travel:"]},{"l":"9.2 Optimize and Z-Ordering","p":["DLT does not automatically call OPTIMIZE. Manual maintenance required:","Z-ordering clusters data by columns for faster pruning."]},{"l":"9.3 Vacuum","p":["Remove old data files (default: 7-day retention):","Critical: Cannot vacuum data referenced by time-travel queries. Retention period protects concurrent readers."]},{"l":"9.4 Schema Enforcement vs Evolution","p":["Enforcement: Strict mode; new columns rejected","Evolution: New columns appended (default in DLT)"]},{"l":"9.5 MERGE Operations in DLT","p":["Limitation: DLT tables read-only within DLT. MERGE requires raw Spark SQL outside DLT or within spark.sql()."]}],[{"l":"10. Orchestration & Dependencies"},{"l":"10.1 Automatic Dependency Resolution","p":["DLT builds DAG from dlt.read() calls:","Execution order: bronze → silver → gold (automatically scheduled)."]},{"l":"10.2 Multi-Pipeline Dependencies","p":["DLT pipelines are isolated. To depend on tables from other pipelines:","Not natively supported: DLT does not expose cross-pipeline dependency tracking. Use Databricks Workflows for multi-pipeline orchestration."]},{"l":"10.3 Circular Dependency Detection","p":["DLT validates DAG at pipeline startup. Circular refs cause immediate failure:"]}],[{"l":"11. Monitoring & Observability"},{"l":"11.1 DLT Event Log Structure","p":["Every pipeline writes events to storage_location/_event_log(Delta format, incremental).","Key event types:","create_update: Pipeline run started","flow_definition: Table metadata (schema, input/output datasets)","flow_progress: Execution metrics (rows, duration, data quality)","update_end: Pipeline run completed"]},{"l":"11.2 Querying Event Logs"},{"l":"11.3 Data Quality Metrics"},{"l":"11.4 Lineage & Data Provenance","p":["UI shows visual DAG. Event log contains:","flow_definition.input_datasets: Table(s) this table reads from","flow_definition.output_dataset: Name of this table","Timestamp, duration, status (success/failure/skipped)"]}],[{"l":"12. Performance Optimization"},{"l":"12.1 Incremental Processing (Key to Cost Reduction)","p":["Streaming tables: Always incremental (only process new data).","Materialized views on serverless: Automatically incremental refresh (cost-based optimizer).","Materialized views on classic compute: Full recompute by default (unless you manually write incremental logic with MERGE).","Benchmark: 6.5x throughput improvement and 85% lower latency with incremental MVs on serverless (200B rows)."]},{"l":"12.2 Partitioning Strategies","p":["Partitioning enables:","Partition pruning: Queries filter out partitions (faster)","Incremental ingestion: Auto Loader discovers new partitions only"]},{"l":"12.3 Join Optimization","p":["Best practices:","Filter early: Reduce join input size","Broadcast small tables: Use broadcast() hint for tables <2GB","Avoid self-joins: Reframe logic to eliminate"]},{"l":"12.4 State Store Optimization (RocksDB)","p":["For stateful streaming (aggregations, joins with windows), enable RocksDB:","Serverless automatically manages state store; classic compute requires manual configuration."]},{"l":"12.5 Backfill Strategies","p":["Full backfill(reset + reprocess):","Incremental backfill(process new data only):","Streaming tables: Automatic after recovery","Materialized views: No explicit control; DLT decides","Manual incremental:"]}],[{"l":"13. Security & Governance"},{"l":"13.1 Unity Catalog Integration","p":["DLT tables automatically reside in Unity Catalog (if workspace enabled). Ownership and permissions:"]},{"l":"13.2 Object Ownership","p":["Every table has an owner (user, service principal, or group). Owner can grant/revoke privileges:"]},{"l":"13.3 Row-Level and Column-Level Security","p":["Row-level security: Dynamic views with current_user():","Column-level security: Column masking via SQL UDF (not native to DLT, requires wrapper function)."]},{"l":"13.4 Secrets Management","p":["Use Databricks Secrets API (not environment variables):"]}],[{"l":"14. CI/CD & DevOps"},{"l":"14.1 Version Control & Code Organization","p":["Typical structure:"]},{"l":"14.2 Environment Promotion (Dev → Staging → Prod)","p":["Strategy: Single codebase, different configs per environment."]},{"l":"14.3 Parameterization"},{"l":"14.4 Testing Strategies","p":["Unit testing(locally or in notebook):","Integration testing: Deploy test pipeline to staging; validate outputs."]}],[{"l":"15. Advanced & Edge Cases"},{"l":"15.1 Backfills and Reprocessing","p":["Scenario: Historical data source added; need to reprocess last 2 years.","Approach 1: Full refresh (reset entire pipeline)","Approach 2: Windowed incremental load","Approach 3: External orchestration Use Databricks Workflows to trigger multiple DLT pipeline runs with different parameters (not natively supported in DLT itself)."]},{"l":"15.2 Schema Drift Handling","p":["Problem: Source schema changes unexpectedly; pipeline breaks.","Solutions:","Auto Loader rescue column(capture unexpected fields):","Lenient schema inference:","Manual schema hints:","Validate in Silver layer:"]},{"l":"15.3 Idempotency and Deduplication","p":["Challenge: Pipeline failure mid-run causes duplicate writes. Solution: Make transformations idempotent.","Idempotent pattern 1: Upsert (overwrite, not append)","Idempotent pattern 2: Deduplication key","Not idempotent: Append-only inserts"]},{"l":"15.4 Checkpoint Corruption Recovery","p":["Symptom: Pipeline halts; error message references checkpoint.","Recovery:","Stop pipeline","Delete checkpoint directory: storage_location/system/checkpoints/{table_name}","Restart pipeline (triggers full reprocess of streaming table)","Monitor for duplicates if logic is not idempotent"]},{"l":"15.5 Handling Large Stateful Operations","p":["Problem: Window joins or large aggregations consume excessive state store memory.","Solutions:","Increase state store size(serverless auto-manages):","Partition state:","Reduce window size or retention period:"]}],[{"l":"16. Misc"},{"l":"16. Comparison & Alternatives"},{"l":"16.1 DLT vs Traditional Spark Jobs","p":["Aspect","Automatic (checkpoint replay)","Automatic (checkpoints)","Automatic (DAG inferred)","Built-in (Expectations)","Cost","Custom logic required","Data quality","Depends on implementation","DLT","Failure recovery","Higher (imperative)","Learning curve","Lower (declarative)","Manual (script order)","Manual implementation","Manual validation","Optimized (incremental, autoscaling)","Orchestration","Spark Jobs","State management"]},{"l":"16.2 DLT vs Databricks Workflows","p":["DLT: Table-to-table dependencies; ideal for pure data pipeline orchestration","Workflows: Task orchestration; SQL queries, Python scripts, shell commands; more flexible","Use DLT for ETL pipelines. Use Workflows for orchestrating heterogeneous tasks (DLT pipeline + Notebook + SQL query + API call)."]},{"l":"16.3 DLT vs Airflow","p":["DLT: Native Databricks; simpler; automatic retry, monitoring, state management","Airflow: Open-source; more flexible; requires manual Databricks hook configuration","DLT is not a replacement for Airflow; different abstraction levels."]},{"l":"16.4 DLT vs Kafka Streams / Flink","p":["DLT: Batch + stream unified; managed by Databricks","Kafka Streams / Flink: Lower-level stream processing; more control, steeper ops burden","DLT is higher-level; preferred for lakehouse pipelines."]},{"l":"17. Interview-Specific Scenarios"},{"l":"17.1 Designing an End-to-End Pipeline","p":["Scenario: Ingest real-time e-commerce transactions, clean, aggregate hourly revenue.","Interview talking points:","Streaming table at Bronze (incremental)","Expectations at Silver (quality enforcement)","Materialized view at Gold (cost-effective aggregation)","Windowing/groupBy automatically handles lateness with watermarks"]},{"l":"17.2 Debugging a Failed Pipeline","p":["Scenario: DLT pipeline fails with \"Cannot cast StringType to IntegerType\" error.","Approach:","Check event log: Query event_log() for update_status = 'FAILED', read error details","Examine source data: Sample raw table, identify rows with unexpected types","Add debugging expectations: Log data quality metrics before transformation","Use development mode: Faster iteration for troubleshooting","Extract problematic step: Run in separate notebook for interactive debugging"]},{"l":"17.3 Handling Schema Evolution","p":["Scenario: Source system adds new column; existing pipeline breaks.","Root cause: Explicit schema definition is too strict.","Solution:"]},{"l":"17.4 Trade-offs: Continuous vs Triggered","p":["Scenario: CEO demands real-time dashboard; cost is high.","Decision matrix:","Continuous: Latency <5 min; cost ~$10k/month (constant cluster)","Triggered (5-min schedule): Latency ~ 5 min; cost ~$2k/month","Recommendation: Triggered pipeline with 5-minute schedule; achieves real-time for most business use cases at 1/5 the cost."]},{"l":"17.5 CDC Implementation with SCD Type 2","p":["Scenario: Track customer attribute changes over time; maintain full history."]},{"l":"Critical Knowledge for Interviews"},{"l":"Mechanics You Must Know","p":["Checkpoints are incremental: Streaming tables store processed offsets; recovery resumes from last offset","Exactly-once is per-partition: Kafka/message queues guarantee exactly-once per partition; Auto Loader per file","Materialized views are deterministic: Same input always produces same output; safe to recompute fully","Expectations are real-time: Metrics logged as data passes through; not post-hoc validation","DLT infers streaming vs batch: If source is streaming (readStream), table is streaming; else, it's a materialized view","State is locality-sensitive: Large state stores benefit from RocksDB; serverless auto-manages"]},{"l":"Common Pitfalls","p":["Idempotency: Streaming + append-only = duplicates on restart","Circular deps: Will be caught immediately; design tables bottom-up","Checkpoint corruption: Manual recovery required; plan for it","Schema drift: Auto Loader rescue column is your friend","Over-specifying schema: Avoid explicit schemas in streaming ingestion; use inference + evolution","Stateful ops without watermarks: Windows/joins will backlog indefinitely without watermarks","Confusing EXPECT variants: EXPECT (logs), EXPECT_OR_DROP (silent removal), EXPECT_OR_FAIL (hard stop)"]},{"l":"Trade-offs to Articulate","p":["Latency vs Cost: Streaming continuous (low latency, high cost) vs triggered (higher latency, lower cost)","Incremental vs Determinism: Streaming tables are incremental but require idempotent logic; MVs are deterministic but expensive","Strictness vs Flexibility: Early schema enforcement prevents bad data; late schema enforcement (rescue columns) allows drift discovery","Complexity vs Automation: DLT abstracts away (simpler, less control); Workflows/Spark jobs expose (more control, more work)"]},{"l":"References & Latest Updates (as of Dec 2025)","p":["Spark Declarative Pipelines(Apache Spark 4.1+): Databricks contributing DLT to open-source Spark","Serverless Incremental MVs: 6.5x throughput improvement, 85% latency reduction (200B-row benchmark)","RocksDB state management: Default for stateful ops; automatically tuned on serverless","Photon Acceleration: 2-4x speedup; ~ 2x DBU cost; skip for I/O-bound workloads","Unity Catalog: All DLT tables eligible for row/column security","Event log querying: event_log(table(...)) syntax enables centralized monitoring"]},{"l":"No-Fluff Summary","p":["DLT is a declarative ETL framework built on Spark Structured Streaming. You declare tables; DLT handles DAG construction, orchestration, state management, and failure recovery. Streaming tables are incremental; materialized views deterministic. Expectations enforce quality in real-time. APPLY CHANGES handles CDC (SCD1/2) natively. Checkpoints enable exactly-once semantics. Event logs provide deep observability. Biggest pitfall: assuming idempotency; design for it. Trade-offs are latency vs cost, complexity vs control. Master checkpoints, expectations, and the streaming-batch unification; you'll ace the interview."]}],[{"l":"Databricks Declarative Pipelines - Interview Preparation Notes","p":["Knowledge Current Through: December 2025"]},{"l":"6.1 SDP Overview"},{"l":"What is SDP?","p":["Lakeflow Spark Declarative Pipelines (SDP) is a declarative framework for building batch and streaming data pipelines in SQL and Python.","Key characteristics:","Runs on performance-optimized Databricks Runtime","Extends and is interoperable with Apache Spark Declarative Pipelines (available in Spark 4.1+)","Code written for open-source Apache Spark pipelines runs without modification on Databricks","Formerly known as Delta Live Tables (DLT) — no migration required for existing DLT code","Requires Premium plan on Databricks","Relationship to DLT:","DLT was rebranded/evolved into Lakeflow SDP","Old dlt module → new pyspark.pipelines module (alias dp)","Existing DLT pipelines run seamlessly within Lakeflow SDP","Classic SKUs still begin with \"DLT\" prefix","Event log schemas with \"dlt\" name unchanged"]},{"l":"Why SDP?","p":["Analyzes dependencies automatically","AUTO CDC API handles out-of-order events automatically","Automatic Orchestration","Automatic watermark management (no manual watermark configuration needed)","Built-in best practices","Common use cases:","Core benefits over manual Spark/Structured Streaming:","Declarative Processing","Determines optimal execution order","Eliminates manual incremental processing code","Focus on what to compute, not how","Hierarchical retry logic: Spark task → Flow → Pipeline","Incremental batch and streaming transformations","Incremental data ingestion from cloud storage (S3, ADLS Gen2, GCS)","Incremental Processing","Materialized views process only new data/changes when possible","Maximizes parallelism for performance","Message bus ingestion (Kafka, Kinesis, EventHub, Pub/Sub, Pulsar)","No complex merge logic required","No manual orchestration via Lakeflow Jobs required","Real-time stream processing between message buses and databases","Reduces hundreds/thousands of lines to just a few","Simplified CDC","Supports SCD Type 1 and Type 2 out-of-the-box"]},{"l":"SDP Package Introduction","p":["Architecture:","AUTO CDC flows","AUTO CDC FROM SNAPSHOT","Cannot be used in notebooks/scripts outside pipelines","Code is evaluated multiple times during planning and execution","Critical constraints:","Databricks-only features (not in Apache Spark):","Enhanced expectation actions","ForEachBatch sink","New keywords: CREATE OR REFRESH, STREAM, CONSTRAINT EXPECT","Not intended for interactive execution","Pipelines separate dataset definitions from update processing","pyspark.pipelines module only available within pipeline execution context","Python Module:","Sinks (Kafka, EventHub, custom)","Source files stored in Databricks workspace or synced from local IDE","SQL Interface:","Syntax: CREATE OR REFRESH STREAMING TABLE/ MATERIALIZED VIEW"]},{"l":"6.2 Core Components"},{"l":"Pipelines","p":["Checks for analysis errors (invalid columns, missing dependencies, syntax)","Compute settings","Configuration categories:","Contains:","Creates or updates tables/views with latest data","Critical configurations:","Definition: A pipeline is the unit of development and execution in SDP.","Dependency management","Discovers all defined tables/views","Each file contains only one language","Evaluated to build dataflow graph before query execution","Execution mechanics:","Flows (streaming and batch)","Infrastructure, dependencies, update processing, table storage","Key interview insight:","Materialized views","Order in files defines evaluation order, not execution order","Pipeline orchestration is automatic— SDP analyzes dependencies and parallelizes optimally","Pipeline source code:","Python or SQL (can mix both in one pipeline)","Sinks","Source code collection definition","Starts a cluster with correct configuration","Storage location","Streaming tables","Target schema(Hive metastore) or catalog + schema(Unity Catalog) — required for external data access","Unlike Lakeflow Jobs, no manual task dependencies required"]},{"l":"Flows","p":["Always defined implicitly within materialized view definition","Append","Append Flow characteristics:","AUTO CDC","AUTO CDC Flow characteristics:","Batch","Batch semantics","Can be defined explicitly (separate from target) or implicitly (within table definition)","CDC with SCD Type 1/2","Continuous streaming ingestion","Databricks-only(not in Apache Spark)","Definition: A flow is the foundational data processing concept in SDP — reads data from source, applies transformations, writes to target.","Flow level","Flow retry logic:","Flow Type","Flow types:","Handles out-of-order events automatically","Incremental batch processing","Incremental processing engine — processes only new data/changes","Materialized View","Materialized View Flow characteristics:","No separate flow object","Only writes to streaming tables","Pipeline level (entire pipeline)","Python example - Explicit append flow:","Python example - Implicit flow in streaming table:","Requires sequencing column (monotonically increasing)","Semantics","Spark task level (most granular)","Streaming","Streaming table only","Streaming table or sink","Supports once=True parameter for one-time backfills","Target","Use Case","Uses Spark Structured Streaming append mode","Writes to streaming tables or sinks"]},{"l":"6.3 Datasets"},{"l":"Streaming Tables","p":["Always backed by Delta format","Can have one or more streaming flows (Append, AUTO CDC) writing to it","Changes/deletions to records in source cause errors","Created with CREATE OR REFRESH STREAMING TABLE(SQL) or dp.create_streaming_table()/ @dp.table()(Python)","Critical interview points:","Definition: Unity Catalog managed table that serves as a streaming target for SDP flows.","Key characteristics:","Python - Explicit:","Python - Implicit:","SQL - Explicit creation:","SQL - Implicit creation:","Streaming tables are always streaming— use streaming semantics for reads","Support AUTO CDC flows (Databricks-only feature)","Supports both explicit and implicit flow definitions","Throws error if source has changes/deletions to existing records","Use the STREAM keyword to read with streaming semantics:"]},{"l":"Materialized Views","p":["Automatically detects changes in source tables","Batch semantics (not streaming)","Best for aggregations, joins, complex transformations with batch semantics","Cannot be target of explicit flows (always implicit)","Created with CREATE OR REFRESH MATERIALIZED VIEW(SQL) or @dp.materialized_view()(Python)","Critical interview points:","Definition: Unity Catalog managed table that serves as a batch target with incremental processing.","Eliminates need for manual change tracking","Flows always defined implicitly within view definition","Full refresh update triggered","Incremental processing engine— only processes new data/changes when possible","Incremental processing mechanics:","Key characteristics:","Processes only modified partitions/data","Python syntax:","Reduces cost and latency vs. full recomputation","Source doesn't support change tracking","Source schema changes","SQL syntax:","View definition changes","When materialized views recompute fully:","Write batch logic; SDP handles incremental execution","Write transformation logic with batch semantics; engine handles incremental processing"]},{"l":"Views","p":["Aggregations, batch","Batch (incremental)","Cannot have expectations applied (only streaming tables and materialized views support expectations)","Comparison:","Data quality filtering","Deduplication logic","Definition: Temporary datasets within a pipeline — not persisted as tables.","Either","Exist only during pipeline execution","Expectations","Feature","Intermediate steps","Intermediate transformations before loading to target","Key characteristics:","Materialized View","No","Not stored in metastore","Persisted","Processing","Python syntax:","Real-time data","Save compute/storage for ephemeral data","Schema transformations","SQL syntax:","Streaming","Streaming Table","Unity Catalog","Use case","Use cases:","Used for intermediate transformations","View","Yes"]},{"l":"6.4 Data Movement"},{"l":"Sinks","p":["Apache Kafka topics","AUTO CDC flows not supported for sinks","Azure EventHub topics","Cannot use sink in dataset definition (only as target)","Checkpoints managed per-flow automatically","Critical constraints:","Custom Python data sources (via ForEachBatch)","Definition: Streaming targets for writing data outside Databricks managed tables.","Delta tables (external Unity Catalog tables)","ForEachBatch characteristics:","ForEachBatch custom sink:","Full refresh does not clean up sink data (appends only)","Full Spark DataFrame API available per batch","Google Pub/Sub topics","headers— optional","Kafka/EventHub required columns:","key— optional","Not compatible with Databricks Connect","Only append_flow can write to sinks","partition— optional","Processes stream as micro-batches","Python - Delta sink:","Python - EventHub sink:","Python - Kafka sink:","Python-only API (no SQL support for sink creation)","Supported sink types:","Supports merge, upsert, multi-target writes","topic— optional (overrides sink-level topic)","value— mandatory(message payload)"]},{"l":"Append Flows","p":["Definition: Streaming flows that continuously append new data to targets.","Key capabilities:","Target: streaming tables or sinks","Semantics: Spark Structured Streaming append output mode","Can be explicit (separate decorator) or implicit (within table)","Supports one-time execution with once=True","Explicit append flow:","One-time backfill:","Multiple append flows to same table:","Critical interview points:","Multiple append flows can write to same streaming table","Useful for backfilling + continuous streaming scenarios","Use once=True for one-time historical loads","Append mode only — no updates or deletes"]},{"l":"Jobs","p":["SDP pipelines are orchestrated automatically— no manual Lakeflow Jobs configuration for internal dependencies.","When to use Lakeflow Jobs with SDP:","Scheduling pipeline updates (daily, hourly, triggered)","Coordinating pipelines with other workflows (notebooks, SQL queries, ML models)","Conditional logic (if/else, for-each loops)","Notifications and alerting","Multi-pipeline orchestration","Key distinction:","Within pipeline: Flows orchestrated automatically by SDP","Across pipelines: Use Lakeflow Jobs for coordination","Example Lakeflow Job with pipeline:"]},{"l":"6.5 Data Quality"},{"l":"Expectations","p":["Definition: Optional clauses in dataset definitions that apply data quality checks on each record.","Key characteristics:","Use standard SQL Boolean expressions","Can combine multiple expectations per dataset","Collect metrics on pass/fail rates","Only supported on streaming tables and materialized views(not views)","Three actions: retain (default), drop, fail","Expectation components:","Name— identifies the constraint","Constraint— SQL Boolean expression","Action— what to do when constraint violated"]},{"l":"Actions","p":["@dp.expect_or_drop(name, expr)","@dp.expect_or_fail(name, expr)","@dp.expect(name, expr)","Action","Advanced pattern - Centralized expectation repository:","API: Query SDP event log (Delta table)","Behavior","Critical interview points:","Default behavior: retain invalid records (enables processing messy data)","Drop","Drop invalid records","EXPECT (expr)","EXPECT (expr) ON VIOLATION DROP ROW","EXPECT (expr) ON VIOLATION FAIL UPDATE","Expectations are soft constraints(unlike CHECK constraints in databases)","Fail","Fail entire update","Flow doesn't support expectations (e.g., sinks)","Flow type doesn't support expectations","Keep invalid records, log metrics","Metrics reporting disabled in pipeline config","No expectations defined","No updates to table/materialized view in flow run","Only streaming tables and materialized views support expectations","Python - Grouped expectations (Python-only):","Python examples:","Python provides expect_all* decorators for grouping (SQL doesn't)","Python Syntax","Retain(default)","SQL examples:","SQL Syntax","UI: Pipeline → Dataset → \"Data quality\" tab","Viewing data quality metrics:","When expectations don't generate metrics:"]},{"l":"6.6 Change Data Capture"},{"l":"Auto CDC","p":["APPLY AS DELETE clause:","Can be timestamp, sequence number, version, etc.","COLUMNS * EXCEPT (col1, col2)— exclude specific columns","COLUMNS *— include all columns","COLUMNS clause:","Definition: Databricks-only streaming flow that processes CDC events with automatic out-of-order handling.","Eliminates need for complex merge logic","Excluded columns typically: operation, sequence number","For multiple columns, use STRUCT: orders by first field, then second on ties","Handles out-of-order events automatically","If omitted, no deletes processed (only inserts/updates)","Key capabilities:","Must be monotonically increasing","NULL sequencing values unsupported","One distinct update per key at each sequencing value","Only available in Lakeflow SDP(not Apache Spark)","Only writes to streaming tables","Python syntax:","Requirements:","Requires sequencing column (monotonically increasing)","Sequencing column — represents proper ordering of source data","Sequencing column characteristics:","Sortable data type for sequencing column","Specifies condition for treating event as DELETE","SQL syntax:","Supports SCD Type 1 and Type 2"]},{"l":"SCD Type 1","p":["1","123","2","3","After event 1: (123, Alice, NYC)","After event 2: (123, Alice, LA) — overwrites","After event 3: empty— deleted","Alice","Behavior:","city","customer_id","Definition: Overwrite existing records — no history retention.","DELETE","Delete event → DELETE","Event","Example data flow:","Existing record → UPDATE (overwrites all columns)","INSERT","LA","name","New record → INSERT","NULL","NYC","operation","Python example:","Resulting table (SCD Type 1):","sequence_num","SQL example:","UPDATE","Use case: Correcting errors, updating non-critical fields (e.g., email address)."]},{"l":"SCD Type 2","p":["__END_AT","__END_AT— sequence value when record became inactive (NULL = current)","__START_AT","__START_AT— sequence value when record became active","1","123","2","2024-01-01 10:00","2024-01-02 11:00","2024-01-03 12:00","3","Alice","Alice Smith","city","customer_id","Definition: Retain full history of changes by creating new records for each version.","Ensures uniqueness per version","Event","event_timestamp","Example data flow:","INSERT","Keys + __START_AT(composite primary key)","LA","name","Note: City change (NYC → LA) didn't create new version because city is excluded from tracking.","NULL","NYC","Only creates new version if tracked columns change","operation","Primary key for SCD Type 2:","Python syntax:","Querying as of specific time:","Querying current records only:","Resulting table (SCD Type 2, TRACK HISTORY ON * EXCEPT (city)):","Resulting table (SCD Type 2, TRACK HISTORY ON *):","SCD Type 2 metadata columns:","SQL syntax:","TRACK HISTORY clause:","TRACK HISTORY ON * EXCEPT (col1, col2)— track all except specified","TRACK HISTORY ON *— track changes on all columns","UPDATE","Use case: Audit trails, historical analysis, compliance requirements."]},{"l":"SCD Type 3","p":["Not officially supported in AUTO CDC.","SCD Type 3 (add columns for previous values) must be implemented manually using:","Custom merge logic in ForEachBatch sink","Manual DataFrame operations","Manual Type 3 pattern:"]},{"l":"Auto CDC FROM SNAPSHOT","p":["Definition: Processes CDC from snapshots rather than change streams.","Use case:","Source system provides periodic full snapshots","No incremental change feed available","Historical backfill from snapshots","Requirements:","Snapshots must be in ascending order by version","Out-of-order snapshots throw error","Python syntax:","Critical difference vs. AUTO CDC:","AUTO CDC: processes change events (INSERT/UPDATE/DELETE)","AUTO CDC FROM SNAPSHOT: compares snapshots to infer changes"]},{"l":"Critical Interview Insights"},{"l":"SDP vs. Manual Spark","p":["10-100 lines","100-1000+ lines","Aspect","AUTO CDC API","Automatic","Built-in (materialized views)","CDC handling","Code volume","Complex merge logic","Dependency management","Hierarchical (task→flow→pipeline)","Incremental processing","Manual","Manual (Lakeflow Jobs)","Manual code","Manual configuration","Manual Spark/Streaming","Orchestration","Retry logic","SDP","Watermarks"]},{"l":"When NOT to Use SDP","p":["Need non-Delta outputs (use sinks or ForEachBatch)","Complex custom stateful operations (use manual Structured Streaming)","Interactive/ad-hoc queries (use notebooks)","Batch-only with no incremental processing needs","Need fine-grained control over checkpoints/watermarks"]},{"l":"Common Pitfalls","p":["Applying expectations to views","AUTO CDC only writes to streaming tables","AUTO CDC requires non-NULL sequencing column","Filter or handle NULLs before CDC processing","Forgetting STREAM keyword in SQL","Materialized views only support implicit flows (within definition)","Multiple flows to materialized views","New: from pyspark import pipelines as dp","Not handling NULL sequencing values","Old: import dlt","Only streaming tables and materialized views support expectations","pyspark.pipelines only available in pipeline context","Running pipeline code interactively","Streaming reads require STREAM(table) or STREAM read_files(...)","Test logic separately, not pipeline decorators","Use append_flow for sinks","Use streaming tables for multiple explicit flows","Using AUTO CDC with sinks","Using dlt module instead of pyspark.pipelines"]},{"l":"Performance Considerations","p":["Append-only or low-update-rate sources","CDC performance:","Complex constraints increase evaluation time","Definition changes","Delta change data feed enabled","Enable schema evolution when needed","Expectation performance:","Expectations evaluated per-record (can impact throughput)","Full recomputation triggers:","Incremental processing most effective with:","Materialized view optimization:","Minimize tracked columns in SCD Type 2","Monitor streaming metrics (latency, throughput)","Non-trackable sources","Partitioned source tables","Schema changes","Sequencing column should be indexed in source","Streaming table optimization:","Use appropriate key cardinality (not too fine-grained)","Use Auto Loader for cloud storage ingestion","Use expect_or_drop to reduce downstream processing"]},{"l":"December 2025 Updates Summary","p":["Lakeflow SDP is the current name (formerly DLT)","pyspark.pipelines module (alias dp) replaces dlt module","Apache Spark 4.1+ includes open-source declarative pipelines","Databricks extends with AUTO CDC, sinks, ForEachBatch","Full backward compatibility — no migration required","Enhanced Lakeflow integration for end-to-end workflows","Premium plan required for Lakeflow SDP","End of Interview Preparation Notes"]}],[{"l":"01 Introduction & Core Concepts"},{"l":"What is SQL and PostgreSQL?","p":["SQL (Structured Query Language) is the standard language for managing relational databases. PostgreSQL is an advanced, open-source relational database management system (RDBMS) that implements SQL standards with extensive features like JSON support, full-text search, and advanced indexing.","Key characteristics of PostgreSQL:","ACID compliant","MVCC (Multi-Version Concurrency Control) for concurrency","Extensible type system","Advanced indexing (B-tree, Hash, GiST, GIN, BRIN, SP-GiST)","PL/pgSQL procedural language","Foreign Data Wrappers for external data access"]},{"l":"Relational Database Model","p":["The relational model organizes data into tables (relations) with rows (tuples) and columns (attributes). Each table has:","Primary Key: Uniquely identifies each row","Foreign Key: References a primary key in another table","Constraints: Rules ensuring data integrity"]},{"l":"DBMS (Database Management System)","p":["A DBMS is software that manages database creation, modification, and access. PostgreSQL provides:","Data storage & retrieval","Concurrency control via MVCC","Transaction management with ACID properties","Query optimization via query planner","Security mechanisms(roles, permissions)"]},{"l":"SQL Language Categories"},{"l":"1. DQL (Data Query Language)","p":["Retrieves data from the database without modification.","Commands: SELECT"]},{"l":"2. DML (Data Manipulation Language)","p":["Modifies data within existing tables.","Commands: INSERT, UPDATE, DELETE"]},{"l":"3. DDL (Data Definition Language)","p":["Defines the structure of the database and its objects.","Commands: CREATE, ALTER, DROP, TRUNCATE"]},{"l":"4. DCL (Data Control Language)","p":["Manages database access and permissions.","Commands: GRANT, REVOKE"]},{"l":"5. TCL (Transaction Control Language)","p":["Manages transactions and ensures data consistency.","Commands: BEGIN, COMMIT, ROLLBACK, SAVEPOINT"]},{"l":"Data Types in PostgreSQL"},{"l":"Numeric Types","p":["-2,147,483,648 to 2,147,483,647","-32,768 to 32,767","-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807","Approximate decimals","Approximate decimals (more precision)","Arbitrary precision","BIGINT","DECIMAL(p, s)/ NUMERIC(p, s)","DOUBLE PRECISION","Double-precision floating-point","INTEGER/ INT","Large integers","Precise decimal calculations (finance)","Range","REAL","Single-precision floating-point","Small integers","SMALLINT","Standard integers","Type","Usage"]},{"l":"Character Types","p":["Type","Characteristics","CHAR(n)","Fixed-length string (padded with spaces)","VARCHAR(n)","Variable-length string with max length","VARCHAR(no limit)","Variable-length string, no limit","TEXT","Variable-length string, no practical limit"]},{"l":"Date & Time Types","p":["00:00:00 to 23:59:59.999999","1900-01-01 to 9999-12-31","DATE","Date + Time","Date + Time + TZ","Day","INTERVAL","Microsecond","Microsecond (no TZ)","Microsecond (with TZ)","Precision","Range","TIME","Time duration","TIMESTAMP/ TIMESTAMP WITHOUT TIME ZONE","TIMESTAMPTZ/ TIMESTAMP WITH TIME ZONE","Type"]},{"l":"Boolean Type"},{"l":"JSON Types"},{"l":"Binary Type"},{"l":"UUID Type"},{"l":"Array Types"}],[{"l":"02 Basic SQL Syntax & Operations"},{"l":"SELECT Statement","p":["The SELECT statement retrieves data from tables."]},{"l":"FROM Clause","p":["Specifies the source table(s)."]},{"l":"WHERE Clause","p":["Filters rows based on conditions."]},{"l":"ORDER BY Clause","p":["Sorts query results."]},{"l":"GROUP BY Clause","p":["Groups rows by specified columns for aggregation."]},{"l":"HAVING Clause","p":["Filters groups based on aggregate conditions (applied AFTER grouping)."]}],[{"l":"03 Query Filtering & Sorting"},{"l":"Logical Operators (AND, OR, NOT)"},{"l":"IN, BETWEEN, LIKE Operators"},{"l":"Advanced Filtering"}],[{"l":"04 Data Aggregation Functions"},{"l":"Standard Aggregate Functions","p":["Average values","AVG()","AVG(column)","Count rows","COUNT()","COUNT(*) or COUNT(column)","Function","Ignores NULLs","Ignores NULLs (except COUNT(*))","MAX()","MAX(column)","Maximum value","MIN()","MIN(column)","Minimum value","Null Handling","Purpose","Standard deviation","STDDEV()","STDDEV(column)","Sum values","SUM()","SUM(column)","Syntax","Variance","VARIANCE()","VARIANCE(column)"]},{"l":"String Aggregation"}],[{"l":"05 Joins: The Complete Deep-Dive","p":["Joins combine rows from multiple tables based on related columns."]},{"l":"1. INNER JOIN (Intersection)","p":["Returns only matching rows from both tables.","Interview Question: What rows does INNER JOIN return if the join column contains NULL values? Answer: INNER JOIN does NOT return rows where the join condition is NULL, because NULL != NULL (NULL comparisons are always UNKNOWN). This is a common gotcha!"]},{"l":"2. LEFT JOIN (LEFT OUTER JOIN)","p":["Returns all rows from left table + matching rows from right table (NULLs for non-matches)."]},{"l":"3. RIGHT JOIN (RIGHT OUTER JOIN)","p":["Returns all rows from right table + matching rows from left table (NULLs for non-matches)."]},{"l":"4. FULL OUTER JOIN (FULL JOIN)","p":["Returns all rows from both tables (NULLs where no match)."]},{"l":"5. CROSS JOIN","p":["Cartesian product: combines every row from left table with every row from right table."]},{"l":"6. Self-Join","p":["Joining a table to itself for hierarchical or comparative data."]},{"l":"JOIN Tricky Interview Scenarios"},{"l":"Scenario 1: Join on NULL values"},{"l":"Scenario 2: Duplicate join keys"},{"l":"Scenario 3: Multiple join conditions"}],[{"l":"06 Subqueries & Correlated Subqueries"},{"l":"Subqueries (Non-Correlated)","p":["Independent queries executed once, returning result to outer query."]},{"l":"Correlated Subqueries","p":["Subqueries that reference columns from outer query; executed once per outer row."]},{"l":"Subquery Performance Considerations"}],[{"l":"07 Window Functions","p":["Window functions perform calculations over a set of rows (a \"window\") without collapsing rows like GROUP BY."]},{"l":"Window Function Syntax"},{"l":"1. Ranking Window Functions"},{"l":"2. Aggregate Window Functions"},{"l":"3. Value Window Functions"},{"l":"Window Frame Specifications"},{"l":"Interview Question: Window Functions vs GROUP BY"}],[{"l":"08 Common Table Expressions (CTEs)","p":["CTEs (WITH clauses) create named temporary result sets for better readability and recursion."]},{"l":"Non-Recursive CTEs"},{"l":"Recursive CTEs","p":["For hierarchical data (organization structure, bill of materials, etc.)."]}],[{"l":"09 Data Definition & Constraints"},{"l":"CREATE TABLE","p":["Defines table structure and constraints."]},{"l":"Constraints"},{"l":"Primary Key Constraint"},{"l":"Foreign Key Constraint"},{"l":"Unique Constraint"},{"l":"Check Constraint"},{"l":"Default Values"},{"l":"ALTER TABLE","p":["Modifies existing table structure."]},{"l":"DROP TABLE"},{"l":"TRUNCATE vs DELETE"}],[{"l":"10 Transactions & ACID Properties"},{"l":"ACID Properties","p":["Atomicity: Transaction is \"all or nothing\" - either all changes commit or all rollback. Consistency: Database moves from one valid state to another. Isolation: Concurrent transactions don't interfere with each other. Durability: Committed data survives system failures."]},{"l":"Transaction Control"},{"l":"Savepoints","p":["Nested rollback points within a transaction."]},{"l":"Isolation Levels","p":["PostgreSQL implements MVCC (Multi-Version Concurrency Control) with 4 isolation levels:"]},{"l":"1. Read Uncommitted (Not explicitly supported in PostgreSQL)","p":["Allows reading uncommitted changes from other transactions (dirty reads). PostgreSQL treats this as Read Committed."]},{"l":"2. Read Committed (Default)","p":["Each statement sees committed data as of statement start. Prevents dirty reads but allows non-repeatable reads."]},{"l":"3. Repeatable Read","p":["Transaction sees consistent snapshot from transaction start. Prevents non-repeatable reads but allows phantom reads (new rows appearing)."]},{"l":"4. Serializable","p":["Highest isolation level. Transactions behave as if executed sequentially (no concurrency anomalies)."]},{"l":"Setting Isolation Level"},{"l":"Deadlock Handling","p":["When two transactions wait for each other's locks."]}],[{"l":"11 Stored Procedures & User-Defined Functions"},{"l":"User-Defined Functions (UDFs)","p":["Functions return a value and can be used in queries."]},{"l":"Stored Procedures","p":["Procedures execute operations without returning values (though they can have OUT parameters)."]},{"l":"Control Structures in PL/pgSQL"},{"l":"Error Handling"}],[{"l":"12 Triggers","p":["Triggers execute automatically in response to database events (INSERT, UPDATE, DELETE)."]},{"l":"Trigger Syntax"},{"l":"Practical Trigger Examples"},{"l":"Managing Triggers"}],[{"l":"13 Views","p":["Views are virtual tables created from queries; they don't store data."]},{"l":"Create Views"},{"l":"Modify & Drop Views"}],[{"l":"14 Normalization & Database Design"},{"l":"1NF - First Normal Form","p":["Ensures atomicity: all column values are atomic (indivisible)."]},{"l":"2NF - Second Normal Form","p":["Eliminates partial dependencies: non-key attributes must depend on the entire primary key (not just part of it)."]},{"l":"3NF - Third Normal Form","p":["Eliminates transitive dependencies: non-key attributes must depend directly on the primary key, not through other non-key attributes."]},{"l":"BCNF - Boyce-Codd Normal Form","p":["Stricter than 3NF: every determinant must be a candidate key."]},{"l":"Denormalization Trade-offs"}],[{"l":"15 Indexing Strategies"},{"l":"Index Types in PostgreSQL","p":["Arrays, JSONB, full-text search","B-tree","BRIN","Complex data types, full-text search","Default; equality/range queries","Equality comparisons only","Fast for exact match","Fast for most queries","Flexible but slower","GIN","GiST","Hash","Index Type","Large sequential data, time-series","Performance","Smallest index size","SP-GiST","Spatial, multidimensional data","Specialized use","Use Case","Very fast for containment"]},{"l":"Creating Indexes"},{"l":"Index-Only Scans"},{"l":"Index Maintenance"},{"l":"Indexing Best Practices"}],[{"l":"16 Query Optimization"},{"l":"EXPLAIN and EXPLAIN ANALYZE","p":["Understanding query execution plans is critical for optimization."]},{"l":"Common Performance Issues and Solutions"},{"l":"Query Tuning Techniques"},{"l":"Configuration Tuning"}],[{"l":"17 Security & Permissions"},{"l":"User and Role Management"},{"l":"GRANT Permissions"},{"l":"REVOKE Permissions"},{"l":"Row-Level Security (PostgreSQL specific)"},{"l":"Auditing and Logging"}],[{"l":"18 Date & Time Functions"},{"l":"Date/Time Types and Functions"},{"l":"Common Date/Time Patterns"}],[{"l":"19 Misc"},{"l":"Advanced Techniques & Tricky Interview Questions"},{"l":"NULL Handling Edge Cases"},{"l":"Complex JOIN Scenarios"},{"l":"Aggregation Edge Cases"},{"l":"Subquery Complexity"},{"l":"Window Function Tricky Cases"},{"l":"Complex CASE Statements"},{"l":"Full-Text Search","p":["PostgreSQL provides powerful full-text search capabilities for efficient text querying."]},{"l":"Full-Text Search Components"},{"l":"Full-Text Search Queries"},{"l":"Search Index and Optimization"},{"l":"Azure PostgreSQL Integration"},{"l":"Connecting to Azure Database for PostgreSQL"},{"l":"Firewall Rules & Network Security"},{"l":"Performance Considerations for Azure PostgreSQL"},{"l":"Azure PostgreSQL Advanced Features"},{"l":"Troubleshooting Azure PostgreSQL Connections"},{"l":"Additional Optimization Techniques"},{"l":"Query Caching and Materialization"},{"l":"Partitioning for Large Tables"},{"l":"Concurrency and Locking"},{"l":"Conclusion","p":["Azure PostgreSQL Specifics:","Be ready for tricky NULL handling questions","Connection pooling is essential","CTEs & Recursive Queries- Clean code for hierarchical data","Discuss normalization trade-offs","Explain execution plans using EXPLAIN ANALYZE","Firewall and SSL configuration","Indexing- Choose right types, monitor usage, avoid premature optimization","Interview Preparation:","Joins- Understand NULL handling, duplicate keys, performance implications","Know when to use window functions vs GROUP BY","Master These Concepts:","Monitoring via Azure Portal","Practice complex JOINs with NULL values","Query Optimization- Use EXPLAIN ANALYZE, identify bottlenecks","Scaling vCore capacity as needed","Stored Procedures- Encapsulate business logic, improve security","Subqueries- Know when to use correlated vs non-correlated","This comprehensive guide covers expert-level SQL concepts for PostgreSQL interviews. Key takeaways:","This guide should serve as your complete reference for SQL interviews. The key is deep understanding, not rote memorization.","Transactions- ACID properties and isolation levels","Understand isolation levels and transaction handling","Window Functions- Powerful for ranking, running totals, comparisons"]}]]