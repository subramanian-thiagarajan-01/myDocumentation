[[{"l":"01 Overview of Databricks"},{"l":"✅ 1. What is Databricks?","p":["Databricks is a cloud-native data analytics platform built on Apache Spark that enables organizations to unify data engineering, data science, machine learning (ML), and business intelligence (BI) workloads in one place. It provides a lakehouse architecture— combining the flexibility of data lakes with the performance and management features of data warehouses."]},{"l":"Key Points","p":["It was founded by the original creators of Apache Spark, and extends it for cloud-scale analytics.","Databricks powers ETL, ad-hoc analytics, ML model building, and business dashboards on the same data platform.","Includes components such as Delta Lake (transactional storage layer) and Databricks SQL for BI."]},{"l":"✅ 2. Why do we need Databricks?","p":["The landscape of enterprise data has changed:","Data is huge(petabytes).","Data is diverse(structured, semi-structured, unstructured).","Users range from engineers and data scientists to analysts and product owners.","Databricks is needed because:"]},{"l":"\uD83C\uDF1F 1) Unified Platform","p":["Traditionally analytic systems were siloed (data warehouse for BI, data lake for ML). Databricks provides one system that supports:","ETL/ELT","Streaming & batch processing","Machine learning","SQL analytics and dashboards All on one shared metadata and governance layer."]},{"l":"\uD83C\uDF1F 2) Scalability & Cost Efficiency","p":["It decouples storage and compute, letting companies pay only for processing when needed and use cheap cloud object storage for data."]},{"l":"\uD83C\uDF1F 3) Support for Diverse Workloads","p":["From SQL reporting to Python/ML notebooks, to streaming ingestion, Databricks supports the full data lifecycle in one platform."]},{"l":"\uD83C\uDF1F 4) Open Standards & No Vendor Lock-In","p":["Uses open formats like Parquet/Delta, and open projects (Apache Spark, Delta Lake, MLflow), so your data isn’t locked into a proprietary format."]},{"l":"✅ 3. What is a Lakehouse?","p":["A lakehouse is a modern data architecture that unifies the best of data lakes and data warehouses."]},{"l":"At a high level:","p":["Like a data lake, it can store all types of data — structured, semi-structured, unstructured — in open formats.","Like a data warehouse, it supports ACID transactions, schema enforcement, performance optimizations, governance, and BI queries."]},{"l":"What a Lakehouse enables","p":["One single source of truth across analytics and AI workloads.","Remove redundant copies between systems (no separate data warehouse + data lake pipelines)."]},{"l":"✅ 4. Challenges Traditional Platforms Face (and how Databricks solves them)"},{"l":"\uD83D\uDEAB 1) Data Silos","p":["Traditional architectures often had separate:","Data lakes (cheap, flexible storage)","Data warehouses (structured analytics)","Specialized tools for ML","This leads to duplicate data and complex ETL between systems.","\uD83D\uDCCC Databricks Solution: A single lakehouse, eliminating the need to move data between systems."]},{"l":"\uD83D\uDEAB 2) Lack of Governance & Quality in Data Lakes","p":["Raw data lakes lack:","ACID transactions","Schema enforcement","Data quality checks","Unified security controls","without these, lakes easily become data swamps.","\uD83D\uDCCC Databricks Solution:","Delta Lake layer brings ACID, schema enforcement, time travel.","Unity Catalog provides centralized fine-grained governance, access controls, and lineage."]},{"l":"\uD83D\uDEAB 3) Traditional Warehouses Aren’t Flexible for AI/ML","p":["Data warehouses are optimized for structured SQL but struggle with:","Semi-structured/unstructured data (images, text, logs)","Spark/ML workloads This creates friction for data scientists.","\uD83D\uDCCC Databricks Solution: Supports rich data types and analytical engines in one platform on open storage."]},{"l":"\uD83D\uDEAB 4) Slow Data Freshness","p":["In traditional architectures, data lakes write raw data → then ETL to warehouse → analytics, which creates data staleness.","\uD83D\uDCCC Databricks Solution: Direct analytics on the same governed lakehouse data, reducing delay and making data ready for analytics and BI sooner."]},{"l":"\uD83D\uDEAB 5) Cost & Complexity of Multiple Tools","p":["Managing multiple systems increases:","Operational overhead","Cost of storage and compute","Engineering overhead on keeping data in sync","\uD83D\uDCCC Databricks Solution: Reduces operational overhead with one integrated platform and managed services on cloud."]},{"l":"✅ 5. Data Lake vs Data Warehouse vs Lakehouse","p":["✔️","❌","❌ (hard)","❌ / Limited","All (raw + structured)","All (raw)","Analytics Performance","BI Dashboards","Cost","Data Lake","Data Lakehouse","Data Type","Data Warehouse","Feature","Governance & ACID","Here’s a simple comparison:","Higher","Low","Medium / Efficient","ML Support","Structured only"]},{"l":"Quick Definitions","p":["Data Lake","A repository for all kinds of raw data.","Cheap and scalable, but lacks transactional guarantees and governance.","Data Warehouse","A structured system optimized for BI and SQL analytics.","Strong governance and performance but limited in data variety.","Data Lakehouse","Merges the best of both— open storage, governance, performance, and analytics in one place."]},{"l":"\uD83E\uDDE0 Final Summary"},{"l":"Why Databricks matters","p":["It solves data complexity, governance, and analytics fragmentation by unifying storage and compute in a managed, scalable cloud platform.","It enables BI, ML, and advanced analytics on the same data platform saving cost and reducing engineering overhead."]},{"l":"Lakehouse Advantages","p":["Single source of truth","Open formats & multi-engine access","ACID transactions","Governance & lineage","Support for real-time streaming and dashboards","These capabilities are essential for modern AI and analytics workloads that traditional systems struggle to handle efficiently."]}],[{"l":"02 Databricks Architecture & Core Roles","p":["Control Plane vs Data Plane · Account vs Workspaces · Metastores"]},{"l":"1️⃣ Concept Overview"},{"l":"What is Databricks Architecture?","p":["Databricks architecture defines how compute, storage, governance, and user access are organized and isolated in the Databricks Lakehouse Platform.","At a high level, Databricks is built on:","Cloud-native separation of control and data","Multi-workspace isolation","Centralized governance via Unity Catalog"]},{"l":"Why This Architecture Exists","p":["Traditional big data platforms struggled with:","Tight coupling of compute and storage","Weak isolation between teams","Fragmented governance","Manual security controls","Poor multi-tenant scalability","Databricks architecture addresses these via:","Control Plane vs Data Plane separation","Account-level governance","Centralized metadata and access control","Elastic compute per workload"]},{"l":"Problems It Solves","p":["Secure multi-tenant analytics","Scalable platform governance","Centralized metadata & permissions","Cloud-native cost control","Enterprise-grade isolation"]},{"l":"2️⃣ Core Architecture / How It Works"},{"l":"High-Level Architecture Diagram","p":["Databricks High-Level Architecture"]},{"l":"\uD83E\uDDE0 Control Plane vs \uD83D\uDCBE Data Plane"},{"l":"\uD83D\uDD39 Control Plane","p":["Managed entirely by Databricks (SaaS layer).","Responsible for:","Workspace metadata","Job orchestration","Cluster management","Notebooks, repos","Unity Catalog metadata","Authentication & authorization","REST APIs","Key characteristics","No customer data processed","Runs in Databricks-managed cloud","Multi-tenant","Highly secured and audited","⚠️ Interview Tip: Customer data NEVER flows through the control plane"]},{"l":"\uD83D\uDD39 Data Plane","p":["Runs inside the customer’s cloud account (AWS/Azure/GCP).","Responsible for:","Spark compute (clusters, SQL warehouses)","Data processing","Reading/writing cloud storage","Photon execution","Delta Lake operations","Key characteristics","Fully isolated per customer","Uses customer VPC/VNet","Data never leaves customer account","IAM-based access to storage"]},{"l":"Control Plane vs Data Plane Summary","p":["✅","✅ Yes","❌","❌ No","Aspect","Cloud IAM","Compute","Contains Data","Control Plane","Customer","Customer VPC/VNet","Data Plane","Databricks","Databricks SaaS","Metadata","Networking","Ownership","SaaS IAM + APIs","Security Model"]},{"l":"3️⃣ Account vs Workspace Architecture"},{"l":"\uD83C\uDFE2 Databricks Account","p":["The Account is the top-level administrative boundary.","Responsible for:","Identity federation (SCIM, SSO)","Workspace creation","Unity Catalog metastores","Account-level groups","Cross-workspace governance","Think of Account as the organization-wide control layer"]},{"l":"\uD83E\uDDEA Databricks Workspace","p":["A Workspace is an isolated environment where users:","Run notebooks","Create jobs","Use clusters and SQL warehouses","Develop pipelines","Each workspace:","Has its own notebooks, jobs, clusters","Can attach to one Unity Catalog metastore","Is isolated from other workspaces"]},{"l":"Account → Workspace Relationship Diagram","p":["Account vs Workspace"]},{"l":"Key Rules","p":["One Account→ many Workspaces","One Workspace→ one Metastore","One Metastore→ many Workspaces"]},{"l":"4️⃣ Metastores (Unity Catalog)"},{"l":"What Is a Metastore?","p":["A Metastore is a centralized metadata and governance layer in Unity Catalog.","It stores:","Catalogs","Schemas","Tables","Views","Functions","Permissions & ownership","Lineage metadata"]},{"l":"Why Metastores Exist","p":["Legacy Hive Metastore issues:","Workspace-scoped","Weak security","No lineage","Hard to govern across teams","Unity Catalog metastores solve:","Central governance","Cross-workspace access","Fine-grained permissions","Auditing and lineage"]},{"l":"Metastore Architecture Diagram","p":["Unity Catalog Metastore"]},{"l":"Metastore Scope & Binding","p":["Created at Account level","Bound to a cloud region","Attached to multiple workspaces","Uses a managed or external storage location"]},{"l":"Example: Metastore Creation (Conceptual)"},{"l":"5️⃣ Key Features & Capabilities"},{"l":"Architecture Highlights Interviewers Care About","p":["Strict Control/Data plane separation","Customer-owned data plane","Centralized governance via Unity Catalog","Multi-workspace isolation","Cloud IAM integration","Cross-workspace metadata sharing"]},{"l":"6️⃣ Common Design Patterns"},{"l":"✅ When to Use Multiple Workspaces","p":["Environment isolation (dev / test / prod)","Team-level isolation","Compliance requirements","Cost attribution"]},{"l":"❌ When NOT to Create Too Many Workspaces","p":["If governance is weak","If data duplication increases","If operational overhead outweighs benefits"]},{"l":"Recommended Enterprise Pattern"},{"l":"7️⃣ Performance, Security, and Cost Considerations"},{"l":"\uD83D\uDD10 Security","p":["Data plane uses customer IAM roles","Unity Catalog enforces row/column-level security","Control plane never sees raw data","Private Link / VPC peering supported"]},{"l":"⚡ Performance","p":["Photon runs in data plane","Control plane latency does not affect query execution","Metadata caching improves query planning"]},{"l":"\uD83D\uDCB0 Cost","p":["Compute costs driven by data plane","Control plane costs included in Databricks pricing","Workspace sprawl increases operational cost"]},{"l":"8️⃣ Common Pitfalls & Misconceptions","p":["⚠️ Misconception: Control plane processes customer data✅ Reality: Only metadata and orchestration","⚠️ Mistake: One workspace per team without shared metastore✅ Leads to governance fragmentation","⚠️ Mistake: Assuming Hive Metastore = Unity Catalog✅ Hive is deprecated for new deployments"]},{"l":"9️⃣ Interview-Focused Q&A","p":["Q1. Why does Databricks separate control and data planes? To ensure security, scalability, and cloud-native isolation.","Q2. Can Databricks access customer data? No. Data stays in the customer’s cloud account.","Q3. What is the role of an Account vs Workspace? Account manages governance; workspace runs workloads.","Q4. Can one workspace use multiple metastores? No. One workspace attaches to only one metastore.","Q5. Why is Unity Catalog account-scoped? To enable centralized, cross-workspace governance."]},{"l":"\uD83D\uDD1F Quick Revision Summary","p":["Control Plane = metadata & orchestration","Data Plane = compute & data processing","Account = org-level governance","Workspace = execution boundary","Metastore = centralized metadata & permissions","Unity Catalog is the default and recommended approach","Data never leaves customer cloud","\uD83D\uDCCC Interview Gold Line:> “Databricks achieves enterprise-grade governance by separating control and data planes, and centralizing metadata using account-scoped Unity Catalog metastores.”"]}],[{"l":"1 Delta Lake Interview Preparation Notes"},{"l":"1. Delta Lake / Delta Format Introduction","p":["What is Delta Lake?","Open-source storage framework that brings ACID transactions to data lakes","Built on top of Parquet files with a transaction log","Not a separate storage system—it's a layer on top of existing object stores (S3, ADLS, GCS, HDFS)","Provides table abstraction over data lake files","What is Delta Format?","File format specification for storing data with ACID guarantees","Consists of:","Data files: Parquet files containing actual data","Transaction log: JSON files tracking all changes (the _delta_log directory)","Delta Table = Delta Format + APIs to read/write","Key Point for Interviews:","Delta Lake is NOT a database—it's a storage layer","No separate compute engine required—works with Spark, Presto, Trino, Flink, etc."]},{"l":"2. Data Storage Evolution","p":["ACID transactions + schema enforcement on data lakes","Be clear: Delta Lake doesn't replace data lakes—it enhances them","Cheap storage (S3, ADLS)","Combines data lake flexibility with warehouse reliability","Data Lake Era (2010-2015)","Data quality issues","Delta Lake specifically introduced by Databricks in 2019, open-sourced immediately","Delta Lake, Apache Iceberg, Apache Hudi","Difficult to maintain consistency","Expensive, limited scalability","Interview Trap:","Lakehouse Era (2019-present)","No ACID transactions","No schema enforcement","Open formats (Parquet, ORC, Avro)","Problems:","Proprietary formats","Small file problem","Still uses same underlying storage (S3/ADLS/etc.)","Store raw data (structured, semi-structured, unstructured)","Strong ACID guarantees","Structured data only","Traditional Data Warehouse (Pre-2010)"]},{"l":"3. Why Delta Lake?","p":["Auto-optimize, Z-ordering","Bad data ingestion with no guarantees","Bad data quality","Data Quality Issues","Delta Lake Solutions:","Delta Solution","DESCRIBE HISTORY command","Difficult to handle late-arriving data","Failed jobs leave partial writes","Full table scans for updates/deletes","Manual compaction needed","MERGE, UPDATE, DELETE support","No ACID","No ACID Transactions in Data Lakes","No audit","No audit trail","No data clustering","No rollback mechanism","No schema enforcement","No time travel","No updates/deletes","Operational Complexity","Performance Problems","Problem","Problems Delta Lake Solves:","Reading while writing = inconsistent data","Schema evolution without validation","Schema validation, constraints, enforcement","Small file problem (millions of tiny files)","Small files","Time travel not possible","Transaction log with optimistic concurrency","Version history in transaction log"]},{"l":"4. Features of Delta Lake"},{"l":"4.1 ACID Transactions","p":["Atomicity: All or nothing writes","Consistency: Schema validation before write","Isolation: Serializable isolation level (strongest)","Durability: Once committed, guaranteed to persist","Code Example:"]},{"l":"4.2 Schema Enforcement & Evolution","p":["Schema Enforcement (default):","Schema Evolution:","Interview Point:","mergeSchema=true adds new columns, doesn't remove or change types","Type changes require explicit ALTER TABLE"]},{"l":"4.3 Time Travel (Data Versioning)","p":["Every write creates a new version:","Interview Trap:","Versions retained based on retention period (default: 30 days)","After VACUUM, old versions unreadable","Time travel uses transaction log, not data file copies"]},{"l":"4.4 MERGE (UPSERT) Operations","p":["SQL Equivalent:"]},{"l":"4.5 DELETE and UPDATE","p":["SQL:","Interview Point:","DELETE/UPDATE not in-place—creates new Parquet files","Old files marked as removed in transaction log","Requires rewriting affected data files"]},{"l":"4.6 Unified Batch and Streaming","p":["Interview Point:","Same table accessed by batch and streaming—no separate pipelines","Streaming writes are also ACID","No \"exactly-once\" issues with proper checkpointing"]},{"l":"4.7 Automatic File Management","p":["OPTIMIZE (Compaction):","Z-ORDER (Data Clustering):","VACUUM (Cleanup):","Interview Trap:","OPTIMIZE doesn't run automatically (unless Auto-Optimize enabled)","VACUUM deletes physical files—time travel stops working for vacuumed versions","Minimum retention: 7 days (safety check to prevent breaking time travel)"]},{"l":"4.8 Audit History","p":["Output Example:"]},{"l":"4.9 Data Quality Constraints","p":["NOT NULL:","CHECK Constraints:","Code Example:","Interview Point:","Constraints enforced at write time","Constraint violations fail the entire transaction","Not all SQL constraints supported (e.g., FOREIGN KEY not supported)"]},{"l":"5. Delta Lake Architecture"},{"l":"5.1 Physical Storage Layout","p":["Directory Structure:"]},{"l":"5.2 Transaction Log (_delta_log)","p":["Core Concept:","Single source of truth for table state","Ordered sequence of JSON files","Each file = one atomic transaction","Version number = sequential commit","Transaction Log Entry Structure:","Key Actions in Log:","add: New file added","remove: File logically deleted (not physically)","metaData: Schema changes","protocol: Protocol version","commitInfo: Transaction metadata"]},{"l":"5.3 Checkpoints","p":["Automatic every 10 commits by default","Build current list of active files","Checkpoint = snapshot of entire table state at version N","Checkpoint Creation:","Checkpoint interval configurable: delta.checkpointInterval","Checkpoints don't replace JSON files—both coexist","Contains all add actions (active files) at that version","Example:","How Table State is Reconstructed:","Interview Point:","Parquet format (faster to read)","Read latest checkpoint","Reading 10,000 JSON files = slow","Replay JSON commits after checkpoint","Why Checkpoints?","Without checkpoints, reads get slower as versions grow"]},{"l":"5.4 Optimistic Concurrency Control","p":["\"Optimistic\" means: assume no conflicts, check at commit time","Append-only writes rarely conflict","Checks for conflicts","Checks if anyone committed v6 already → No","Checks if anyone committed v6 already → Yes (Writer A did)","Code Example - Conflict:","Conflict Detection:","How Concurrent Writes Work:","If conflict: Retry from latest version (v6)","If no conflict: Write as v7","Interview Trap:","NOT pessimistic locking (no table locks during write)","Retries are automatic, transparent to user","Success","Two writes conflict if they modify the same data files","UPDATE/DELETE/MERGE on same partitions = conflict","Writer A attempts commit to v6:","Writer A reads current version (v5)","Writer B attempts commit to v6:","Writer B reads current version (v5)","Writes transaction log file 00000000000000000006.json"]},{"l":"5.5 Read and Write Flow","p":["\"Active files list\" = files added but not removed in log","Atomic commit by successfully writing log file","Build active files list at that version","Build final list of active files","Check for conflicts (read latest version)","Compute file statistics (min/max, count)","Find latest checkpoint","Interview Point:","List _delta_log directory","Multiple readers can read different versions simultaneously","Read checkpoint (active files at checkpoint version)","Read Flow (Current Version):","Read Flow (Historical Version):","Read Parquet files from list","Read those Parquet files","Reads don't lock table","Replay commits after checkpoint","Replay transaction log from v0 to requested version","Schema validation","Write data as Parquet files","Write Flow:","Write transaction log JSON file"]},{"l":"5.6 Metadata and Statistics","p":["File-level Statistics (in Transaction Log):","Usage:","Data skipping: Skip files not matching query predicates","Example: SELECT * FROM table WHERE id = 500","Delta reads stats, skips files where max(id) < 500 or min(id) > 500","Interview Point:","Stats collected automatically during write","Only for first 32 columns by default","Critical for query performance—enables data skipping without reading files"]},{"l":"5.7 Data Skipping","p":["How It Works:","Query: SELECT * FROM orders WHERE date = '2024-01-15'","Delta reads transaction log stats","Skips files where:","max(date) '2024-01-15' OR","min(date) '2024-01-15'","Only reads files where '2024-01-15' falls in [min, max] range","Z-Ordering Impact:","Co-locates similar values in same files","Improves data skipping effectiveness","Example: Z-order by date→ all records for same date mostly in same file","Code Example:"]},{"l":"Key Interview Points Summary","p":["ACID via Optimistic Concurrency","Automatic retries on conflict","Checkpoints for faster reads","Common Pitfall:","Conflict detection at commit time","Constraints enforced at write time","Data skipping via file statistics","Default: strict enforcement","Delta Lake = Parquet + Transaction Log","Every commit = new version","Forgetting to OPTIMIZE → small file problem","Mark old files as removed in log","MERGE/UPDATE/DELETE Not In-Place","mergeSchema=true: allow evolution","No locks during write","Not a separate storage system","Not Z-ordering frequently filtered columns → poor data skipping","OPTIMIZE for small file compaction","Performance Features","Retained for 30 days by default","Rewrite affected files","Schema Enforcement vs Evolution","Time Travel via Versions","Transaction log in _delta_log is the single source of truth","VACUUM before retention period → breaks time travel","VACUUM deletes old data files","VACUUM deletes physical files later","Z-ordering for better clustering"]}],[{"l":"2 Delta Lake Transaction Log & ACID Implementation"},{"l":"Delta Log Overview","p":["What it is: Ordered record of every transaction ever performed on a Delta table. Stored in _delta_log/ subdirectory.","Purpose:","Single source of truth for table state","Enables ACID guarantees","Powers time travel","Coordinates concurrent readers/writers"]},{"l":"Delta Log File Structure"},{"l":"Directory Layout","p":["Key components:","JSON files: One per transaction (version), named with 20-digit zero-padded version number","CRC files: Checksum for JSON integrity validation","Checkpoint files: Aggregated state snapshots every N commits (default: 10)","_last_checkpoint: Metadata file pointing to latest checkpoint"]},{"l":"Delta Log Files Explained"},{"l":"1. JSON Transaction Files (.json)","p":["Action","Action Types:","add","Application transaction ID for idempotency","cdc","Change Data Capture records","commitInfo","Content: Single transaction's actions encoded as newline-delimited JSON.","Deletion vectors, column mapping, etc.","domainMetadata","Example: Creating a table with 2 files","File added to table","File removed from table (logical deletion)","metaData","protocol","Purpose","Reader/writer version requirements","remove","Resulting 00000000000000000000.json:","Schema, partition columns, table properties, table ID","Transaction metadata (timestamp, operation, user, params)","txn"]},{"l":"2. CRC Files (.crc)","p":["Purpose: Validate JSON file integrity.","When created: After JSON commit file is written.","Content: Checksum of the JSON file.","Example: 00000000000000000002.crc","Validation: On read, Delta recomputes JSON checksum and compares with CRC. Mismatch = corrupted transaction log.","Note: Not always present in cloud storage (S3, ADLS). Delta handles gracefully."]},{"l":"3. Checkpoint Files (.checkpoint.parquet)","p":["Purpose: Avoid replaying entire log history. Aggregate all table state up to version N.","When created: Every 10 commits by default (configurable via delta.checkpointInterval).","Structure: Parquet file containing all active add actions, latest metaData, and protocol.","Example: After 10 commits, 00000000000000000010.checkpoint.parquet created.","Checkpoint content:","All add actions for current table files","Current metaData","Current protocol","No remove, commitInfo(not needed for state reconstruction)","Multi-part checkpoints: For large tables, checkpoint split into multiple files:"]},{"l":"4. _last_checkpoint File","p":["Purpose: Quick discovery of latest checkpoint.","Content: JSON with checkpoint version and file count.","Example:","Usage: Readers start here to find checkpoint, then replay subsequent JSON commits."]},{"l":"How Delta Ensures ACID Properties"},{"l":"Atomicity","p":["Mechanism: Optimistic concurrency control + atomic file operations.","Process:","Writer reads latest version from log","Performs operation (e.g., writes new Parquet files)","Creates JSON commit file for version N+1","Attempts atomic write (PUT-if-absent in S3, conditional create in ADLS/HDFS)","If write succeeds: Transaction committed","If write fails: Another writer committed first. Retry with conflict resolution.","Key: JSON file write is atomic. Either version N exists or doesn't—no partial state.","Code Example - Concurrent Writes:","Result: Both succeed (append is commutative) or one retries. No data loss, no partial commits."]},{"l":"Consistency","p":["Mechanism: Single-version snapshot isolation.","Guarantee: Readers see complete snapshot at version N. Never see partial transaction.","Example:","Implementation:","Reader lists _delta_log/, finds latest version (or checkpoint)","Constructs file list from add actions at that version","Reads only those files","Concurrent writes to version N+1 invisible until reader explicitly refreshes"]},{"l":"Isolation","p":["Behavior: Second transaction detects conflict (overlapping predicate/partition), retries.","Behind the scenes:","Both read version N","Both succeed","Both write new Parquet files","Code Example - Conflict Detection:","Concurrent appends to disjoint partitions","Concurrent appends to non-partitioned table","Concurrent appends to same partition","Conflict types:","Conflict?","Level: Serializable (strongest isolation).","Mechanism: Write conflict detection.","No","One retries","One retries, checks for logical conflicts","Resolution","Scenario","Schema evolution + Write","Transaction 1 writes version N+1 successfully","Transaction 2 attempts N+1, fails(file exists)","Transaction 2 reads N+1, detects logical conflict, retries as version N+2","Update + Delete on same records","Yes"]},{"l":"Durability","p":["Mechanism: Write-ahead log (Delta Log) committed before data visible.","Process:","Write new Parquet files to table directory","Write JSON commit file","Only after JSON commit succeeds, transaction visible","Guarantee: If JSON commit exists, data files exist. If system crashes before JSON write, data files ignored (orphaned).","Example:","Cleanup: Orphaned files removed by VACUUM command."]},{"l":"Schema Evolution, Enforcement, and Overwrite"},{"l":"Schema Enforcement (Default Behavior)","p":["Rule: Write schema must match existing table schema. Type mismatches or missing columns rejected.","Example - Schema Enforcement Failure:","Error:","Enforcement at: Write time, before Parquet files written."]},{"l":"Schema Evolution","p":["Enable with: .option(mergeSchema, true)","Behavior: Add new columns, compatible type widening.","Allowed changes:","Add new columns (nullable by default)","Type widening (e.g., int → long, float → double)","Not allowed:","Drop columns (use ALTER TABLE DROP COLUMN)","Incompatible type changes (e.g., string → int)","Change nullability (nullable → not null)","Example - Add Column:","Output:","Delta Log Change:","Version 1 JSON includes:","Old data files: Retain old schema. Delta fills missing age with null on read."]},{"l":"Schema Overwrite Modes","p":["1. overwriteSchema = true","Usage: Replace entire schema.","Output:","Use case: Complete table restructure.","2. replaceWhere (Partial Overwrite)","Usage: Overwrite specific partitions while preserving schema and other partitions.","Delta Log: Adds remove actions for old 2023-01-02 files, add actions for new file."]},{"l":"Schema Changes in Delta Log"},{"l":"Table Property: delta.columnMapping.mode","p":["Purpose: Allow column renames, drops, type changes without rewriting data.","Modes:","none(default): Physical column names must match schema","name: Use field IDs, allow renames","id: Full column mapping with IDs","Example - Column Rename with Mapping:","Output:","Delta Log: Schema updated, but Parquet files unchanged (field ID mapping used)."]},{"l":"How Delta Operations Work Internally"},{"l":"INSERT / APPEND","p":["Process:","Write new Parquet files","Create JSON commit with add actions","No remove actions (pure append)","Example:","Delta Log Version N:","Key: isBlindAppend=true means no reads required, conflict-free."]},{"l":"UPDATE","p":["Process:","Read phase: Scan files matching WHERE clause (using stats)","Rewrite phase: Rewrite affected files with updated rows","Commit phase: Add remove for old files, add for new files","Example:","Delta Log:","Optimization: If predicate matches entire file, skipped (no rewrite). If matches no rows, skipped.","Performance:","Best case: Partition pruning + Z-ordering reduce files scanned","Worst case: Full table scan if no partition/stats pruning"]},{"l":"DELETE","p":["Two modes: Copy-on-Write(default) vs. Deletion Vectors(Delta 2.0+)."]},{"l":"Copy-on-Write DELETE","p":["Process:","Scan files matching WHERE clause","Rewrite files, excluding deleted rows","Commit remove(old) + add(rewritten)","Example:","Delta Log:","Cost: Proportional to data rewritten (even if deleting 1 row)."]},{"l":"Deletion Vectors (Preferred for Small Deletes)","p":["Binary RoaringBitmap with deleted row indices (e.g., [1] for row index 1).","cardinality: Number of deleted rows","Commit add action with deletionVector metadata—no remove","Compaction: OPTIMIZE merges DVs into rewritten files.","Copy-on-Write: Large deletes, infrequent deletes, already rewriting","dataChange=false: No new data, only metadata","Deletion Vector File( deletion_vector_ab3efd67-xxxx-xxxx-xxxx-xxxxxxxxxxxx.bin):","deletionVector fields:","Delta Log:","DVs: Small deletes (<10% of file), frequent deletes","Enable:","Example:","Generate deletion vector (RoaringBitmap) marking deleted row positions","pathOrInlineDv: DV file path or inline bitmap","Process:","Read behavior: Readers load DV, skip deleted rows.","Result: DV files removed, data files rewritten without deleted rows.","Scan files matching WHERE clause","storageType: u(UUID path) or i(inline)","What: Bitmap index marking deleted rows. No file rewrite.","When to use:","Write DV file (.bin format)"]},{"l":"MERGE (Upsert)","p":["Process: Combines UPDATE, INSERT, DELETE in single transaction.","Phases:","Match phase: Join source with target using ON condition","Action phase: Execute matched/not-matched clauses","Rewrite phase: Rewrite affected files","Commit phase: Atomic commit with mixed add/ remove actions","Example:","Output:","Delta Log:"]},{"l":"Key Interview Takeaways"},{"l":"What Interviewers Test","p":["Log replay mechanism: How readers construct table state from JSON + checkpoints","Conflict resolution: Write-write conflicts, retry logic","File-level operations: No row-level deletes in Parquet—always file rewrites (or DVs)","Checkpoint usage: Why needed, when created, how reduces read latency","Schema evolution pitfalls: mergeSchema vs. overwriteSchema, backward compatibility","Deletion Vectors: Trade-offs vs. copy-on-write, when to optimize","ACID guarantees: Atomicity via file atomicity, isolation via versioning, durability via WAL"]},{"l":"Common Pitfalls","p":["Checkpoint lag: If checkpoints disabled, readers replay 1000s of commits—slow reads","Small files problem: Frequent appends create many small files—use OPTIMIZE","DV accumulation: Many deletes create many DV files—run OPTIMIZE periodically","Schema evolution without mergeSchema: Fails silently if schema mismatch not caught","Concurrent overwrites: Two mode(overwrite) writers can conflict—use replaceWhere for partitions"]},{"l":"Quick Wins for Interviews","p":["Know JSON actions: add, remove, metaData, protocol, txn, cdc, domainMetadata","Explain checkpoint necessity: \"Avoids O(N) log replay for readers\"","Articulate DV benefits: \"Efficient small deletes, deferred file rewrites, compaction required\"","Describe conflict detection: \"Optimistic concurrency—writers check overlapping predicates/partitions on retry\"","Schema evolution rules: \"Add columns, type widening allowed; drop/incompatible changes require explicit ALTER TABLE\""]},{"l":"Additional Code: Inspecting Delta Log","p":["Output Example:"]}],[{"l":"3 Delta Lake / Delta Format Code"},{"l":"1. Creating Delta Tables"},{"l":"1.1 Basic Delta Table Creation","p":["Using SQL (CREATE TABLE)","Using PySpark DataFrame API","Using DeltaTableBuilder API"]},{"l":"1.2 Identity Columns (Auto-increment)","p":["Key Concepts:","Identity columns auto-generate unique, monotonically increasing values","Introduced in Delta Lake 2.3.0 / DBR 11.3 LTS","Guarantees: uniqueness, monotonicity (not gapless)","Cannot be updated manually","Survives MERGE, INSERT operations","SQL Syntax","Using DeltaTableBuilder","Insert Examples","Interview Points:","Identity values are NOT gapless (gaps occur after failures, MERGE operations)","Identity column cannot be part of PARTITION BY","Cannot manually INSERT into GENERATED ALWAYS columns","Identity state stored in Delta table metadata","High-water mark persists across sessions"]},{"l":"1.3 Generated/Computed Columns","p":["Key Concepts:","Values computed from expressions based on other columns","Computed at write time, stored physically (not virtual)","Immutable - cannot be updated directly","Useful for partitioning, indexing, data quality","SQL Syntax","Using DeltaTableBuilder","Insert Examples","Interview Points:","Generated columns are computed at WRITE time (not read time)","Values are physically stored in data files","Cannot violate the generation expression","Expression must be deterministic","Common use case: partition pruning with derived date columns","Schema evolution compatible - can add generated columns via ALTER TABLE"]},{"l":"2. Reading Delta Format vs Delta Table"},{"l":"2.1 Delta Table (Metastore-Registered)","p":["What It Is:","Table registered in Hive metastore or Unity Catalog","Metadata includes table name, schema, location, properties","Supports SQL DDL operations (ALTER, DROP)","SQL Read","PySpark Read"]},{"l":"2.2 Delta Format (Path-Based)","p":["What It Is:","Direct read from file path (no metastore entry)","Access underlying Delta log and data files","Useful for external tables, ad-hoc analysis","SQL Read","PySpark Read"]},{"l":"2.3 Key Differences","p":["Access","ALTER, DROP supported","Aspect","DDL Operations","Delta Format","Delta Table","Discoverability","File system permissions only","Full governance support","Limited governance","Metastore entry required","Metastore-level ACLs","Must know path","No registration needed","Not supported","Only in _delta_log","Permissions","Registration","SHOW TABLES lists it","spark.read.format(delta).load(path)","spark.table(name)","Stored in metastore","Table Properties","Unity Catalog"]},{"l":"2.4 Reading Delta Transaction Log","p":["Structure:","Reading Transaction Log Directly","Interview Points:","Delta Format = raw files + _delta_log","Delta Table = Delta Format + metastore metadata","Both use the same underlying format","Path-based reads bypass metastore, useful for data recovery","Delta Format can be read by non-Databricks Spark with Delta Lake library"]},{"l":"3. Upsert/Merge Operations (SCD Type 1 & 2)"},{"l":"3.1 MERGE Syntax Basics","p":["SQL MERGE","PySpark MERGE"]},{"l":"3.2 SCD Type 1 (Overwrite Changes)","p":["Scenario: Update customer records, overwriting old values","SQL","PySpark","Result:"]},{"l":"3.3 SCD Type 2 (Maintain History)","p":["Scenario: Track historical changes with effective dates","SQL","PySpark (Better Approach)","Result:"]},{"l":"3.4 Advanced MERGE Patterns","p":["Conditional MERGE","Delete on MERGE","Interview Points:","MERGE is atomic (all-or-nothing)","MERGE creates a single transaction version","Each record in target matched at most once (no duplicates)","Order of clauses matters: WHEN MATCHED before WHEN NOT MATCHED","Can have multiple WHEN MATCHED clauses with different conditions","MERGE INTO requires target to be Delta table","Source can be any DataFrame/Table/View"]},{"l":"4. Table Utility Commands"},{"l":"4.1 DESCRIBE Commands","p":["DESCRIBE (Basic Schema)","Output:","DESCRIBE EXTENDED (Full Metadata)","DESCRIBE DETAIL (Delta-Specific Metadata)","PySpark:","DESCRIBE DETAIL Output:"]},{"l":"4.2 HISTORY","p":["SQL:","PySpark:","Output:","Interview Points:","History stored in _delta_log transaction log","Each version is immutable","History enables time travel","History does NOT show data-level changes (use CDF for that)"]},{"l":"4.3 RESTORE","p":["Restore to Specific Version","PySpark:","What Happens:","Creates new version in transaction log","Does NOT delete data files (VACUUM needed for cleanup)","Metadata points to files from target version","Restores schema, partitioning, table properties","Example:","Interview Points:","RESTORE is metadata operation (fast)","No data duplication","Old versions remain accessible until VACUUM","Cannot restore if target version VACUUMed","RESTORE creates a new version"]},{"l":"4.4 Table Properties (TBLPROPERTIES)","p":["View Properties","Set Properties","PySpark:","Common Table Properties:"]},{"l":"4.5 VACUUM","p":["Cannot time travel beyond VACUUMed versions","Checkpoint files older than retention","Data files not referenced by any version within retention","Default 7-day retention protects time travel","Files referenced by versions within retention","Interview Points:","Latest checkpoint","Long-running queries reading old versions can fail during VACUUM","Permanently delete data files not required by recent versions","Purpose:","PySpark:","Reclaim storage space","Remove uncommitted files","Retention Periods:","Set spark.databricks.delta.retentionDurationCheck.enabled = false to override (risky)","SQL:","Transaction log JSON files","Uncommitted files (from failed writes)","VACUUM is irreversible","VACUUM runs in two phases: identify files, delete files","What Gets Deleted:","What Survives:"]},{"l":"4.6 CLONE (Shallow vs Deep)","p":["Affects clone","Aspect","Backups, disaster recovery","Clones are independent tables (separate history)","Clones can have different properties, schema evolution","Complete isolation from source","Complete isolation from source changes","Copied (independent)","Copies transaction log AND data files","Copies transaction log only","Creating table variants with different properties","Data archival","Data exploration without duplication","Data Files","Deep Clone","Deep Clone (Full Copy)","Deep Clone:","Deep clones survive source VACUUM/deletion","Disaster recovery","Fast (metadata only)","Fast and space-efficient","Full copy","Full independent copy","Fully independent","Independence","Independent table metadata","Interview Points:","Key Differences:","Migrating tables across workspaces/regions","Minimal overhead","No impact","Production backups","PySpark:","Quick environment setup (dev, staging, test)","Referenced (shared)","References same data files as source","REPLACE TABLE with CLONE syntax overwrites target","Shallow Clone","Shallow Clone (Metadata Only)","Shallow Clone:","Shallow clones become \"dangling\" if source VACUUMed","Shares files with source","Slow (copies data)","Slower and space-consuming","Source VACUUM","Speed","Storage","Testing schema changes","Testing, dev/staging","Use Cases","Use Cases:","What Happens:"]},{"l":"4.7 Change Data Feed (CDF)","p":["1. Incremental ETL","2. Audit/Compliance","3. Replication to External Systems","4. Slowly Changing Dimensions","Cannot enable CDF retroactively for past versions","CDF adds ~ 5-10% storage overhead","CDF data retained per delta.deletedFileRetentionDuration","CDF Output Schema:","CDF requires Delta Lake 2.0+","CDF Use Cases:","CDF useful for CDC, incremental processing, audit logs","Change Types:","Changes tracked at row level","delete: Row deleted","Enable CDF","insert: New row added","Interview Points:","PySpark:","Reading CDF","update_postimage: New value after update","update_preimage: Old value before update"]},{"l":"4.8 OPTIMIZE","p":["1. File Selection","2. Coalesce/Repartition","3. Transaction Commit","Adds new large files to transaction log","Atomic operation","Coalesces into fewer, larger files","Compact small files into larger files","Default targetFileSize= 128 MB","Example:","Groups files by partition","How OPTIMIZE Works (Internals):","Identifies small files (< targetFileSize)","Improve read performance","Marks old small files as removed","OPTIMIZE Configuration:","Purpose:","PySpark:","Reads small files into DataFrame","Reduce metadata overhead","SQL:","Writes new files"]},{"l":"4.9 ZORDER","p":["1. Data Arrangement","2. Column Statistics","Advanced (file-level)","Arbitrary/insertion order","Aspect","Basic (partition-level)","Choosing Z-ORDER Columns:","Clustered by Z-ORDER columns","Co-locate related data in files","Co-locates rows with similar values","Collected","Collects min/max stats per file per column","Column Stats","Column Stats Example:","Data Skipping","Default (No Z-ORDER)","Enables data skipping during reads","Enhanced for Z-ORDER columns","Example:","Fast","File Organization","Frequently filtered columns (WHERE clauses)","High-cardinality columns (department, user_id)","How Z-ORDER Works:","Improve data skipping","Improved for filtered queries","Interview Points:","Limit to 3-4 columns (diminishing returns)","Low-to-medium cardinality better than very high","Multi-dimensional clustering","Order matters: most selective column first","Purpose:","PySpark:","Query Performance","Reduce data scanning for filtered queries","Slower (Z-ORDER computation)","SQL:","Standard","Stats stored in _delta_log JSON files","Stores stats in transaction log ( add action)","Uses space-filling Z-curve algorithm","With Z-ORDER","Write Performance","Z-ORDER improves read, slows write","Z-ORDER incompatible with partitioning on same columns","Z-ORDER is a one-time operation (must re-run after new writes)","Z-ORDER leverages Delta's data skipping","Z-ORDER requires full table rewrite","Z-ORDER vs Default Behavior:"]},{"l":"4.10 Liquid Clustering","p":["1. Automatic Clustering on Write","2. Adaptive Layout","3. Z-ORDER Replacement","Aspect","Automatic incremental clustering (no manual OPTIMIZE needed)","Automatic on write","Batch-optimized tables","Better for high-volume writes","Clusters data during INSERT/MERGE/UPDATE","Dynamically adjusts clustering layout","Enable Liquid Clustering","High-volume streaming","How Liquid Clustering Works:","Incremental","Incremental clustering","Key Differences: Z-ORDER vs Liquid Clustering","Learns from query patterns","Liquid Clustering","Lower maintenance overhead","Maintenance","Manual OPTIMIZE","Manual re-run needed","Minimal impact","More efficient than manual Z-ORDER","No (full rewrite)","No manual OPTIMIZE needed","Optimizes both read and write performance","Purpose:","PySpark:","Replaces manual Z-ORDER for clustering","Self-maintaining","Self-tunes over time","Slows writes","Trigger","Use Case","Write Performance","Yes (incremental)","Z-ORDER"]}]]